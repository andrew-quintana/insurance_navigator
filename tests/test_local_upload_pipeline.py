#!/usr/bin/env python3
"""
Test local upload pipeline with direct database operations.
This bypasses authentication issues and tests the core functionality.
"""

import asyncio
import aiohttp
import hashlib
import json
import time
import uuid
import logging
from datetime import datetime
import asyncpg

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_local_upload_pipeline():
    """Test the local upload pipeline with direct database operations."""
    
    base_url = "http://localhost:8000"
    
    async with aiohttp.ClientSession() as session:
        logger.info("üîç Starting local upload pipeline test...")
        
        # 1. Test signed URL generation
        logger.info("1Ô∏è‚É£ Testing signed URL generation...")
        
        # Create test document data
        test_doc_content = f"""
        INSURANCE POLICY DOCUMENT
        Policy Number: LOCAL-DB-{uuid.uuid4().hex[:8]}
        Policyholder: test@example.com
        Effective Date: 2024-01-01
        Expiration Date: 2024-12-31
        
        COVERAGE DETAILS
        ================
        
        Medical Coverage:
        - Annual Maximum: $1,000,000
        - Deductible: $2,500 per year
        - Coinsurance: 20% after deductible
        - Office Visit Copay: $25
        - Specialist Visit Copay: $50
        - Emergency Room Copay: $200
        - Urgent Care Copay: $75
        
        Dental Coverage:
        - Annual Maximum: $50,000
        - Deductible: $500 per year
        - Preventive Care: 100% covered
        - Basic Services: 80% covered after deductible
        - Major Services: 50% covered after deductible
        
        Vision Coverage:
        - Annual Maximum: $25,000
        - Eye Exam: $25 copay
        - Frames: $200 allowance every 2 years
        - Lenses: 100% covered for basic lenses
        
        This policy provides comprehensive health insurance coverage.
        """
        
        test_doc_hash = hashlib.sha256(test_doc_content.encode()).hexdigest()
        filename = f"local_db_test_{uuid.uuid4().hex[:8]}.pdf"
        
        # Test upload limits first
        try:
            async with session.get(f"{base_url}/api/upload-pipeline/upload/limits") as response:
                if response.status == 200:
                    limits = await response.json()
                    logger.info(f"‚úÖ Upload limits: {limits}")
                else:
                    logger.error(f"‚ùå Upload limits failed: {response.status}")
                    return False
        except Exception as e:
            logger.error(f"‚ùå Upload limits error: {e}")
            return False
        
        # 2. Test direct database operations
        logger.info("2Ô∏è‚É£ Testing direct database operations...")
        
        try:
            conn = await asyncpg.connect("postgresql://postgres:postgres@127.0.0.1:54322/postgres")
            
            # Create a test user
            test_user_id = str(uuid.uuid4())
            test_user_email = f"db_test_{uuid.uuid4().hex[:8]}@example.com"
            
            # First create user in auth.users
            await conn.execute("""
                INSERT INTO auth.users (id, email, encrypted_password, email_confirmed_at, created_at, updated_at)
                VALUES ($1, $2, $3, NOW(), NOW(), NOW())
                ON CONFLICT (id) DO NOTHING
            """, test_user_id, test_user_email, "hashed_password")
            
            # Then create user in public.users
            await conn.execute("""
                INSERT INTO users (id, email, name, consent_version, consent_timestamp, created_at, updated_at)
                VALUES ($1, $2, $3, $4, $5, NOW(), NOW())
                ON CONFLICT (id) DO NOTHING
            """, test_user_id, test_user_email, "DB Test User", "1.0", datetime.utcnow())
            
            logger.info(f"‚úÖ Test user created: {test_user_id}")
            
            # Create a test document
            document_id = str(uuid.uuid4())
            raw_path = f"files/user/{test_user_id}/raw/{int(time.time())}_{test_doc_hash[:8]}.pdf"
            
            await conn.execute("""
                INSERT INTO upload_pipeline.documents (
                    document_id, user_id, filename, mime, bytes_len, 
                    file_sha256, raw_path, created_at, updated_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, NOW(), NOW())
            """, document_id, test_user_id, filename, "application/pdf", 
                 len(test_doc_content), test_doc_hash, raw_path)
            
            logger.info(f"‚úÖ Test document created: {document_id}")
            
            # Create a test upload job
            job_id = str(uuid.uuid4())
            job_payload = {
                "user_id": test_user_id,
                "document_id": document_id,
                "file_sha256": test_doc_hash,
                "bytes_len": len(test_doc_content),
                "mime": "application/pdf",
                "storage_path": raw_path
            }
            
            await conn.execute("""
                INSERT INTO upload_pipeline.upload_jobs (
                    job_id, document_id, status, state, progress, 
                    created_at, updated_at
                ) VALUES ($1, $2, $3, $4, $5, NOW(), NOW())
            """, job_id, document_id, "uploaded", "queued", json.dumps(job_payload))
            
            logger.info(f"‚úÖ Test job created: {job_id}")
            
            # 3. Test signed URL generation logic
            logger.info("3Ô∏è‚É£ Testing signed URL generation logic...")
            
            # Simulate the signed URL generation
            storage_base_url = "http://127.0.0.1:54321"
            ttl_seconds = 300
            
            # Test the new path format
            if raw_path.startswith("files/user/"):
                key = raw_path
                signed_url = f"{storage_base_url}/files/{key}?signed=true&ttl={ttl_seconds}"
                logger.info(f"‚úÖ Generated signed URL: {signed_url}")
                
                if "127.0.0.1:54321" in signed_url:
                    logger.info("‚úÖ Using correct local storage URL!")
                else:
                    logger.error("‚ùå Using incorrect storage URL!")
            else:
                logger.error("‚ùå Invalid storage path format!")
            
            # 4. Test file upload to local storage
            logger.info("4Ô∏è‚É£ Testing file upload to local storage...")
            
            try:
                # Try to upload to the signed URL
                async with session.put(signed_url, data=test_doc_content.encode(), 
                                     headers={"Content-Type": "application/pdf"}) as response:
                    if response.status == 200:
                        logger.info("‚úÖ File uploaded successfully to local storage!")
                    else:
                        logger.warning(f"‚ö†Ô∏è File upload response: {response.status}")
                        response_text = await response.text()
                        logger.warning(f"Response: {response_text}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è File upload error: {e}")
            
            # 5. Test chunk generation (simulate)
            logger.info("5Ô∏è‚É£ Testing chunk generation simulation...")
            
            # Create some test chunks with proper schema
            chunk_texts = [
                "Medical Coverage: Annual Maximum: $1,000,000, Deductible: $2,500 per year",
                "Dental Coverage: Annual Maximum: $50,000, Deductible: $500 per year",
                "Vision Coverage: Annual Maximum: $25,000, Eye Exam: $25 copay"
            ]
            
            # Create a dummy embedding vector (1536 dimensions of zeros)
            dummy_embedding = [0.0] * 1536
            dummy_embedding_str = "[" + ",".join(map(str, dummy_embedding)) + "]"
            
            for i, chunk_text in enumerate(chunk_texts):
                chunk_id = str(uuid.uuid4())
                chunk_sha = hashlib.sha256(chunk_text.encode()).hexdigest()
                
                await conn.execute("""
                    INSERT INTO upload_pipeline.document_chunks (
                        chunk_id, document_id, chunker_name, chunker_version, chunk_ord, 
                        text, chunk_sha, embed_model, embed_version, vector_dim, embedding
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
                """, chunk_id, document_id, "markdown-simple", "1", i, 
                     chunk_text, chunk_sha, "text-embedding-3-small", "1", 1536, dummy_embedding_str)
            
            logger.info(f"‚úÖ Created {len(chunk_texts)} test chunks")
            
            # 6. Test RAG search simulation
            logger.info("6Ô∏è‚É£ Testing RAG search simulation...")
            
            # Query for deductible information
            query = "What is my deductible?"
            deductible_chunks = await conn.fetch("""
                SELECT chunk_id, text, chunk_ord
                FROM upload_pipeline.document_chunks
                WHERE document_id = $1 AND text ILIKE '%deductible%'
                ORDER BY chunk_ord
            """, document_id)
            
            if deductible_chunks:
                logger.info(f"‚úÖ Found {len(deductible_chunks)} chunks with deductible information:")
                for chunk in deductible_chunks:
                    logger.info(f"   - {chunk['text']}")
            else:
                logger.warning("‚ö†Ô∏è No chunks found with deductible information")
            
            await conn.close()
            
        except Exception as e:
            logger.error(f"‚ùå Database operations error: {e}")
            return False
        
        logger.info("üéâ Local upload pipeline test completed successfully!")
        return True

if __name__ == "__main__":
    success = asyncio.run(test_local_upload_pipeline())
    if success:
        logger.info("üéâ LOCAL UPLOAD PIPELINE TEST PASSED!")
    else:
        logger.info("‚ùå Local upload pipeline test failed")
