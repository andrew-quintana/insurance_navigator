#!/usr/bin/env python3

import asyncio
import os
import sys
import uuid
import hashlib
from dotenv import load_dotenv
import asyncpg

# Add project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'backend'))

from backend.workers.base_worker import BaseWorker
from backend.shared.config.worker_config import WorkerConfig

# Load environment variables
load_dotenv('.env.development')

# Test constants
TEST_SCHEMA = 'upload_pipeline'
TEST_USER_ID = uuid.UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479')

async def create_test_document(pool, file_path, user_id):
    """Creates a test document and job starting at 'parsed' stage."""
    file_name = os.path.basename(file_path)
    storage_path = f"storage://uploads/{user_id}/{file_name}"
    parsed_path = f"storage://parsed/{user_id}/{file_name}.json"
    document_id = uuid.uuid4()
    
    # Get actual file size and compute hash
    full_path = os.path.join(project_root, file_path)
    if os.path.exists(full_path):
        file_size = os.path.getsize(full_path)
        with open(full_path, 'rb') as f:
            file_sha256 = hashlib.sha256(f.read()).hexdigest()
    else:
        file_size = 2500000  # Default size if file doesn't exist
        file_sha256 = hashlib.sha256(b"test file content").hexdigest()

    async with pool.acquire() as conn:
        async with conn.transaction():
            # Create document record
            await conn.execute(
                f"""
                INSERT INTO {TEST_SCHEMA}.documents
                (document_id, user_id, filename, raw_path, processing_status, parsed_path, mime, bytes_len, file_sha256, created_at, updated_at)
                VALUES ($1, $2, $3, $4, 'parsing', $5, 'application/pdf', $6, $7, NOW(), NOW());
                """,
                document_id, user_id, file_name, storage_path, parsed_path, file_size, file_sha256
            )

            # Create job at 'parsed' stage
            job_id = uuid.uuid4()
            await conn.execute(
                f"""
                INSERT INTO {TEST_SCHEMA}.upload_jobs
                (job_id, document_id, stage, state, created_at, updated_at)
                VALUES ($1, $2, 'parsed', 'queued', NOW(), NOW());
                """,
                job_id, document_id
            )
    
    return document_id, job_id

async def verify_closeout_requirements():
    """Verify all closeout requirements are met."""
    print("üèÅ Closeout Requirements Verification")
    print("="*50)
    
    all_requirements_met = True
    
    # 1. Verify stage rename: chunks_buffered ‚Üí chunked
    print("\n1Ô∏è‚É£  STAGE RENAME VERIFICATION")
    worker_files = [
        'backend/workers/base_worker.py',
        'backend/workers/enhanced_base_worker.py'
    ]
    
    chunks_buffered_found = False
    for file_path in worker_files:
        full_path = os.path.join(project_root, file_path)
        if os.path.exists(full_path):
            with open(full_path, 'r') as f:
                content = f.read()
                if 'chunks_buffered' in content:
                    print(f"   ‚ùå Found 'chunks_buffered' in {file_path}")
                    chunks_buffered_found = True
                else:
                    print(f"   ‚úÖ No 'chunks_buffered' found in {file_path}")
    
    if not chunks_buffered_found:
        print("   ‚úÖ PASS: Stage rename completed successfully")
    else:
        print("   ‚ùå FAIL: 'chunks_buffered' references still exist")
        all_requirements_met = False
    
    # 2. Verify configurable terminal stage
    print("\n2Ô∏è‚É£  CONFIGURABLE TERMINAL STAGE")
    config = WorkerConfig(
        database_url='postgresql://postgres:postgres@127.0.0.1:54322/postgres',
        supabase_url='http://127.0.0.1:54321',
        supabase_anon_key='test',
        supabase_service_role_key='test',
        llamaparse_api_url='http://localhost:8001',
        llamaparse_api_key='test-key',
        openai_api_url='http://localhost:8002',
        openai_api_key='test-key',
        openai_model="text-embedding-3-small",
        terminal_stage="custom_terminal"  # Test custom terminal stage
    )
    
    if hasattr(config, 'terminal_stage') and config.terminal_stage == "custom_terminal":
        print("   ‚úÖ PASS: Terminal stage is configurable")
    else:
        print("   ‚ùå FAIL: Terminal stage configuration not working")
        all_requirements_met = False
    
    # 3. Test state finalization with complete pipeline
    print("\n3Ô∏è‚É£  STATE FINALIZATION TEST")
    
    # Connect to database
    pool = await asyncpg.create_pool('postgresql://postgres:postgres@127.0.0.1:54322/postgres')
    
    try:
        # Clean up existing test data
        async with pool.acquire() as conn:
            document_ids = await conn.fetch(f"SELECT document_id FROM {TEST_SCHEMA}.documents WHERE user_id = $1;", TEST_USER_ID)
            if document_ids:
                await conn.execute(f"DELETE FROM {TEST_SCHEMA}.document_chunks WHERE document_id = ANY($1);", [d['document_id'] for d in document_ids])
                await conn.execute(f"DELETE FROM {TEST_SCHEMA}.upload_jobs WHERE document_id = ANY($1);", [d['document_id'] for d in document_ids])
                await conn.execute(f"DELETE FROM {TEST_SCHEMA}.documents WHERE user_id = $1;", TEST_USER_ID)
        
        # Create test document
        document_id, job_id = await create_test_document(pool, 'examples/scan_classic_hmo.pdf', TEST_USER_ID)
        
        # Configure worker with embedded terminal stage
        config = WorkerConfig(
            database_url='postgresql://postgres:postgres@127.0.0.1:54322/postgres',
            supabase_url='http://127.0.0.1:54321',
            supabase_anon_key='test',
            supabase_service_role_key='test',
            llamaparse_api_url='http://localhost:8001',
            llamaparse_api_key='test-key',
            openai_api_url='http://localhost:8002',
            openai_api_key='test-key',
            openai_model="text-embedding-3-small",
            terminal_stage="embedded"
        )
        
        # Run pipeline to completion
        worker = BaseWorker(config)
        await worker._initialize_components()
        worker.running = True
        
        try:
            # Process through multiple stages
            for i in range(10):  # Max 10 iterations to reach terminal stage
                job = await worker._get_next_job()
                if not job:
                    break
                
                await worker._process_single_job_with_monitoring(job)
                
                # Check if we reached terminal stage with done state
                async with pool.acquire() as conn:
                    job_status = await conn.fetchrow(
                        f"SELECT stage, state FROM {TEST_SCHEMA}.upload_jobs WHERE job_id = $1;",
                        job_id
                    )
                
                if job_status and job_status['stage'] == config.terminal_stage and job_status['state'] == 'done':
                    break
        finally:
            worker.running = False
            await worker._cleanup_components()
        
        # Verify final state
        async with pool.acquire() as conn:
            final_job = await conn.fetchrow(
                f"SELECT stage, state FROM {TEST_SCHEMA}.upload_jobs WHERE job_id = $1;",
                job_id
            )
            final_doc = await conn.fetchrow(
                f"SELECT processing_status FROM {TEST_SCHEMA}.documents WHERE document_id = $1;",
                document_id
            )
            chunk_count = await conn.fetchval(
                f"SELECT COUNT(*) FROM {TEST_SCHEMA}.document_chunks WHERE document_id = $1;",
                document_id
            )
        
        if (final_job and 
            final_job['stage'] == config.terminal_stage and 
            final_job['state'] == 'done' and
            final_doc and 
            final_doc['processing_status'] == 'completed'):
            print(f"   ‚úÖ PASS: State finalization works correctly")
            print(f"      - Job: stage='{final_job['stage']}', state='{final_job['state']}'")
            print(f"      - Document: status='{final_doc['processing_status']}'")
            print(f"      - Chunks generated: {chunk_count}")
        else:
            print(f"   ‚ùå FAIL: State finalization not working")
            print(f"      - Job: stage='{final_job['stage'] if final_job else 'NONE'}', state='{final_job['state'] if final_job else 'NONE'}'")
            print(f"      - Document: status='{final_doc['processing_status'] if final_doc else 'NONE'}'")
            all_requirements_met = False
    
    except Exception as e:
        print(f"   ‚ùå FAIL: Error during state finalization test: {e}")
        all_requirements_met = False
    
    finally:
        await pool.close()
    
    # Final summary
    print("\n" + "="*50)
    print("üèÅ FINAL CLOSEOUT SUMMARY")
    print("="*50)
    
    if all_requirements_met:
        print("‚úÖ ALL CLOSEOUT REQUIREMENTS MET")
        print("\nüìã Delivered:")
        print("   ‚Ä¢ Configurable terminal stage system")
        print("   ‚Ä¢ Stage rename: chunks_buffered ‚Üí chunked") 
        print("   ‚Ä¢ State finalization: jobs reach 'done' state")
        print("   ‚Ä¢ Document completion: processing_status = 'completed'")
        print("   ‚Ä¢ Backward compatibility maintained")
        print("   ‚Ä¢ Tests validate end-to-end functionality")
        return True
    else:
        print("‚ùå SOME REQUIREMENTS NOT FULLY MET")
        print("   Review failed checks above")
        return False

if __name__ == "__main__":
    success = asyncio.run(verify_closeout_requirements())
    sys.exit(0 if success else 1)