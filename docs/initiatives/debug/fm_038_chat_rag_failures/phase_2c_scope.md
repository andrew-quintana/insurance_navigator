# Phase 2C: Step-by-Step Function Pipeline Testing - Scope

**Document Version:** 1.0  
**Date:** 2024-10-10  
**Status:** ðŸ”´ Active  
**Priority:** P0 - Critical

---

## Executive Summary

Phase 2C implements a **direct function testing approach** that calls each function in the chat RAG pipeline sequentially, capturing per-function logs and visualizing data flow. This provides superior debugging capability compared to log analysis by allowing real-time, controlled execution with complete visibility into function inputs, outputs, and transformations.

---

## Objectives

### Primary Objective
Create an instrumented integration test notebook that:
1. Calls every function in the chat pipeline in order
2. Captures isolated logs for each function
3. Displays inputs and outputs at each step
4. Identifies exact failure point automatically
5. Provides repeatable local testing

### Secondary Objectives
- Create reusable testing framework for future debugging
- Establish pattern for pipeline testing
- Generate detailed execution reports
- Enable rapid iteration on fixes

---

## Scope

### In Scope

#### 1. Function Pipeline Coverage
**Required Functions to Test:**

**Chat Entry Layer:**
- `chat_endpoint()` - Entry point
- Request validation
- Session management

**Agent Layer:**
- `select_agent()` - Agent selection logic
- `PatientNavigatorAgent.__init__()` - Agent initialization  
- `PatientNavigatorAgent.process()` - Main processing
- `PatientNavigatorAgent.select_tool()` - Tool selection

**RAG Tool Layer:**
- `retrieve_chunks_from_text()` - Main RAG function
- `_generate_embedding()` - Embedding generation
- `_validate_parameters()` - Input validation
- `_format_query()` - Query preprocessing

**External API Layer:**
- `OpenAIClient.create_embedding()` - OpenAI API call
- `OpenAIClient._handle_response()` - Response processing
- Rate limiting logic
- Error handling

**Database Layer:**
- `retrieve_chunks()` - Vector similarity search
- `_build_query()` - SQL construction
- `_execute_query()` - Query execution
- `_parse_results()` - Result parsing

**Response Generation Layer:**
- `format_chunks()` - Chunk formatting
- `generate_response()` - Final response generation
- `apply_template()` - Response templating

**Total:** ~15-20 functions in complete pipeline

#### 2. Logging Infrastructure

**Per-Function Log Capture:**
```
tests/fm_038/function_logs/
â”œâ”€â”€ 01_chat_endpoint.log
â”œâ”€â”€ 02_select_agent.log
â”œâ”€â”€ 03_patient_navigator_init.log
â”œâ”€â”€ 04_patient_navigator_process.log
â”œâ”€â”€ 05_select_tool.log
â”œâ”€â”€ 06_retrieve_chunks_from_text.log
â”œâ”€â”€ 07_validate_parameters.log
â”œâ”€â”€ 08_format_query.log
â”œâ”€â”€ 09_generate_embedding.log
â”œâ”€â”€ 10_openai_create_embedding.log
â”œâ”€â”€ 11_handle_response.log
â”œâ”€â”€ 12_retrieve_chunks.log
â”œâ”€â”€ 13_build_query.log
â”œâ”€â”€ 14_execute_query.log
â”œâ”€â”€ 15_parse_results.log
â”œâ”€â”€ 16_format_chunks.log
â”œâ”€â”€ 17_generate_response.log
â””â”€â”€ 18_apply_template.log
```

**Log Management Features:**
- Automatic log file creation
- Per-function isolation
- Timestamp tracking
- Log level filtering
- Memory and file capture
- Log rotation/cleanup

#### 3. Output Visualization

**For Each Function Display:**
- **Function Header:**
  - Step number
  - Function name
  - Duration (ms)
  - Success/failure status

- **Inputs Section:**
  - Parameter names and values
  - Data types
  - Data sizes (if applicable)
  - JSON-formatted display

- **Outputs Section:**
  - Return value
  - Data type
  - Data structure
  - JSON-formatted display

- **Logs Section:**
  - All logs generated by this function
  - Color-coded by level (ERROR=red, WARNING=yellow, INFO=blue)
  - Truncated if too long (with line count)

- **Validation Section:**
  - Expected vs actual output
  - Data integrity checks
  - Business logic validation

#### 4. Data Flow Tracking

**Pipeline State Management:**
```python
pipeline_state = {
    'step_1_output': {...},
    'step_2_output': {...},
    # ... for each step
    'current_step': N,
    'failed_step': None,
    'execution_time': {...}
}
```

**Variable Flow Visualization:**
```
user_query (str)
    â†“
agent_name (str) 
    â†“
agent_instance (PatientNavigatorAgent)
    â†“
tool_name (str)
    â†“
embedding_vector (List[float], len=1536)
    â†“
chunks (List[Dict], len=N)
    â†“
formatted_context (str)
    â†“
final_response (str)
```

#### 5. Test Configuration

**Configurable Parameters:**
```python
TEST_CONFIG = {
    # User context
    'user_id': str,
    'session_id': str,
    
    # Query
    'query_text': str,
    'query_type': str,
    
    # RAG settings
    'similarity_threshold': float (0.0-1.0),
    'max_chunks': int,
    'embedding_model': str,
    
    # Database
    'db_timeout': int (ms),
    'vector_index': str,
    
    # API settings
    'openai_timeout': int (ms),
    'retry_count': int,
    
    # Debug flags
    'verbose_logging': bool,
    'stop_on_error': bool,
    'save_intermediates': bool
}
```

#### 6. Failure Detection

**Automatic Failure Point Identification:**
- Check each step output for None
- Validate data types
- Validate data structure
- Check for exceptions
- Verify expected transformations
- Identify first failing step
- Highlight in red

**Failure Reporting:**
```python
{
    'failed_step': {
        'number': int,
        'name': str,
        'function': str,
        'error': str,
        'inputs': dict,
        'expected_output': str,
        'actual_output': any,
        'logs': str,
        'stack_trace': str
    }
}
```

#### 7. Export Functionality

**Export Formats:**

**JSON Report:**
```json
{
    "timestamp": "2024-10-10T15:30:00Z",
    "test_config": {...},
    "pipeline_results": {
        "step_1": {"status": "success", "duration_ms": 12.3, ...},
        "step_2": {"status": "success", "duration_ms": 45.6, ...},
        "step_N": {"status": "failed", "error": "...", ...}
    },
    "failure_point": {
        "step": 7,
        "name": "openai_create_embedding",
        "error": "..."
    },
    "logs_directory": "tests/fm_038/function_logs",
    "total_duration_ms": 234.5
}
```

**HTML Report:**
- Interactive visualization
- Expandable sections
- Color-coded status
- Embedded logs
- Copy-able data

### Out of Scope

âŒ **Not Included in Phase 2C:**

1. **Production Deployment**
   - This is a testing notebook only
   - Not for production use

2. **Multiple Test Cases**
   - Single test run per execution
   - Batch testing in future phase

3. **Performance Benchmarking**
   - Focus is on correctness, not performance
   - Performance testing in separate phase

4. **Mocking**
   - Uses real functions and APIs
   - No mock objects (want to test real behavior)

5. **Automated Fixes**
   - Identifies issues only
   - Does not implement fixes

6. **Load Testing**
   - Single request testing only
   - Concurrent requests not tested

---

## Technical Requirements

### Environment Setup

**Required Dependencies:**
```python
# Core
python >= 3.8
jupyter >= 1.0.0
ipython >= 7.0.0

# Data
pandas >= 1.3.0
numpy >= 1.20.0

# Visualization  
matplotlib >= 3.3.0
seaborn >= 0.11.0

# Project
[All project dependencies from requirements.txt]
```

**Environment Variables:**
```bash
OPENAI_API_KEY=<key>
SUPABASE_URL=<url>
SUPABASE_KEY=<key>
DATABASE_URL=<url>
```

### File Structure

```
tests/fm_038/
â”œâ”€â”€ FM_038_Function_Pipeline_Test.ipynb     # Main notebook
â”œâ”€â”€ function_logs/                          # Log outputs
â”‚   â”œâ”€â”€ 01_*.log
â”‚   â”œâ”€â”€ 02_*.log
â”‚   â””â”€â”€ ...
â”œâ”€â”€ pipeline_test_YYYYMMDD_HHMMSS.json     # Test results
â”œâ”€â”€ PHASE_2C_USAGE_GUIDE.md                 # Usage documentation
â””â”€â”€ PHASE_2C_QUICK_REFERENCE.md             # Quick reference
```

### Notebook Structure

**Total Cells:** ~25-30

**Part 1: Setup (Cells 1-5)**
- Overview
- Imports
- Log management
- Display functions
- Configuration

**Part 2: Pipeline Execution (Cells 6-20)**
- One cell per function
- ~15 functions = 15 cells
- Each follows same template

**Part 3: Analysis (Cells 21-25)**
- Summary
- Failure analysis
- Log analysis
- Visualization
- Export

### Function Call Template

Each function test cell follows this pattern:

```python
# ============================================================
# STEP N: function_name()
# ============================================================

step_num = N
function_name = "function_name"

# Display header
display(Markdown(f"## Step {step_num}: {function_name}()"))

# Prepare inputs
inputs = {
    'param1': previous_step_output,
    'param2': config_value,
}

# Execute with timing and logging
start_time = time.time()

with FunctionLogger(function_name, step_num) as logger:
    try:
        output = target_function(**inputs)
        success = True
    except Exception as e:
        output = None
        success = False
        logging.error(f"Function failed: {e}", exc_info=True)

duration_ms = (time.time() - start_time) * 1000

# Get logs
logs = logger.get_logs()

# Display results
display_function_step(step_num, function_name, inputs, output, logs, duration_ms)

# Validation
if success and output is not None:
    # Run validation checks
    validation_passed = validate_output(output)
    display_validation(step_num, "Output validation", validation_passed, "...")
else:
    display_validation(step_num, "Function execution", False, "Function failed")

# Store for next step
globals()[f'step_{step_num}_output'] = output
```

---

## Implementation Plan

### Phase 1: Setup Infrastructure (2 hours)

**Tasks:**
1. Create notebook skeleton
2. Implement `FunctionLogger` class
3. Implement display functions
4. Create log directory structure
5. Setup test configuration

**Deliverables:**
- Working log capture
- Display functions ready
- Configuration loaded

### Phase 2: Implement Function Pipeline (3-4 hours)

**Tasks:**
1. Implement each function test cell
2. Wire up data flow between steps
3. Add validation checks
4. Test each step individually

**Deliverables:**
- All 15+ function cells working
- Data flows correctly
- Validations in place

### Phase 3: Analysis and Reporting (1-2 hours)

**Tasks:**
1. Implement summary analysis
2. Implement failure detection
3. Create visualizations
4. Implement export functionality

**Deliverables:**
- Complete analysis
- Failure identification
- Export working

### Phase 4: Documentation (1 hour)

**Tasks:**
1. Create usage guide
2. Create quick reference
3. Add examples
4. Document common issues

**Deliverables:**
- Complete documentation
- Usage examples
- Troubleshooting guide

**Total Estimated Time:** 6-8 hours

---

## Success Metrics

### Functional Metrics

âœ… **Pipeline Coverage:** All 15+ functions testable  
âœ… **Log Capture:** 100% of logs captured per function  
âœ… **Output Display:** All inputs/outputs visible  
âœ… **Failure Detection:** Automatic failure point identification  
âœ… **Repeatability:** Can run multiple times consistently  
âœ… **Export:** Results exportable to JSON/HTML  

### Quality Metrics

âœ… **Accuracy:** Identifies correct failure point 100% of time  
âœ… **Completeness:** Captures all relevant data  
âœ… **Usability:** Can be run by any developer  
âœ… **Documentation:** Complete usage instructions  
âœ… **Maintainability:** Easy to add new functions  

### Performance Metrics

- **Execution Time:** < 30 seconds for complete pipeline
- **Log Size:** < 10MB total logs
- **Report Size:** < 5MB JSON report

---

## Validation Criteria

Before considering Phase 2C complete:

1. âœ… All functions in pipeline are testable
2. âœ… Logs captured in separate files per function
3. âœ… Inputs and outputs displayed for each step
4. âœ… Failure point automatically identified
5. âœ… Data flow visible throughout pipeline
6. âœ… Validation checks at each step
7. âœ… Results exportable
8. âœ… Complete documentation
9. âœ… Successfully identifies FM-038 root cause
10. âœ… Can be used by other developers

---

## Risks and Mitigation

### Risk 1: Function Signatures Change
**Impact:** High  
**Probability:** Medium  
**Mitigation:**
- Use `**kwargs` where possible
- Document expected signatures
- Add version checks
- Use try/except for compatibility

### Risk 2: API Rate Limits
**Impact:** Medium  
**Probability:** High (OpenAI)  
**Mitigation:**
- Add rate limiting
- Cache embeddings
- Use test mode if available
- Add retry logic

### Risk 3: Database State
**Impact:** High  
**Probability:** Medium  
**Mitigation:**
- Use test user account
- Verify test data exists
- Add data setup cell
- Document required data

### Risk 4: Log File Size
**Impact:** Low  
**Probability:** Medium  
**Mitigation:**
- Implement log rotation
- Truncate display
- Add cleanup cell
- Set max file size

---

## Dependencies

### Code Dependencies
- `agents/patient_navigator/agent.py`
- `agents/tooling/rag/core.py`
- `backend/shared/external/openai_real.py`
- `db/services/vector_service.py`
- `db/services/supabase_auth_service.py`
- `core/agent_integration.py`

### Data Dependencies
- Test user account with ID
- Uploaded documents for test user
- Stored chunks with embeddings
- Valid session ID

### External Dependencies
- OpenAI API access
- Supabase database access
- Internet connectivity
- Valid API keys

---

## Future Enhancements

**Phase 2C.1 - Batch Testing:**
- Multiple test cases
- Comparison analysis
- Success rate tracking

**Phase 2C.2 - Performance Profiling:**
- Memory usage tracking
- CPU profiling
- Bottleneck identification

**Phase 2C.3 - Mock Mode:**
- Option to mock external APIs
- Faster testing
- Offline capability

**Phase 2C.4 - Continuous Testing:**
- Git hooks integration
- Pre-commit testing
- CI/CD integration

---

## References

- **Prompt:** `docs/initiatives/debug/fm_038_chat_rag_failures/phase_2c_prompt.md`
- **FM-038 Investigation:** `tests/fm_038/`
- **RAG Core:** `agents/tooling/rag/core.py`
- **Agent:** `agents/patient_navigator/agent.py`

---

## Approval

**Status:** ðŸ”´ Awaiting Implementation  
**Expected Completion:** TBD  
**Assigned To:** Development Team  
**Priority:** P0 - Critical Path

---

**Document Version:** 1.0  
**Last Updated:** 2024-10-10  
**Next Review:** After Phase 2C completion

