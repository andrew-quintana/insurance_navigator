# Phase 2C Prompt: Step-by-Step Function Pipeline Testing

**Phase:** 2C - Direct Function Call Testing  
**Objective:** Unit test each function in the chat pipeline with instrumented logging and output visualization  
**Status:** üî¥ **Ready for Implementation**  
**Priority:** P0 - Critical for root cause identification

---

## Context

Phase 2B analyzed production logs after-the-fact. We need a more **controlled, repeatable approach** that:

1. **Directly calls each function** in the chat pipeline
2. **Captures logs per function** (isolated logging)
3. **Displays outputs at each step** before passing to next function
4. **Visualizes data transformation** through the pipeline
5. **Provides step-by-step debugging** capability

This is essentially an **instrumented integration test** that lets us:
- See exactly what each function receives
- See exactly what each function returns
- Capture logs generated by each specific function
- Identify where data gets corrupted/lost
- Reproduce issues locally

---

## Your Task

Create a Jupyter notebook that implements a **step-by-step function pipeline tester** for the chat RAG flow.

---

## Core Requirements

### 1. Function Call Chain

The notebook should call functions in **exact order** as they would be called in production:

```python
# Example flow:
1. chat_endpoint()
2. select_agent()
3. patient_navigator_agent.process()
4. tool_selection()
5. retrieve_chunks_from_text()
6. _generate_embedding()
7. openai_client.create_embedding()
8. retrieve_chunks() [database]
9. format_chunks()
10. generate_response()
```

### 2. Per-Function Logging

For **each function**:
- Create isolated log capture
- Save logs to function-specific file
- Display logs in notebook
- Organize logs by function name

**Directory Structure:**
```
tests/fm_038/function_logs/
‚îú‚îÄ‚îÄ 01_chat_endpoint.log
‚îú‚îÄ‚îÄ 02_select_agent.log
‚îú‚îÄ‚îÄ 03_patient_navigator_process.log
‚îú‚îÄ‚îÄ 04_tool_selection.log
‚îú‚îÄ‚îÄ 05_retrieve_chunks_from_text.log
‚îú‚îÄ‚îÄ 06_generate_embedding.log
‚îú‚îÄ‚îÄ 07_openai_create_embedding.log
‚îú‚îÄ‚îÄ 08_retrieve_chunks.log
‚îú‚îÄ‚îÄ 09_format_chunks.log
‚îî‚îÄ‚îÄ 10_generate_response.log
```

### 3. Output Visualization

After **each function**:
- Display function inputs (what it received)
- Display function outputs (what it returned)
- Display data type and structure
- Show transformation that occurred
- Validate outputs before passing to next function

### 4. Variable Flow Tracking

Track the **data pipeline**:
```python
Input: user_query
  ‚Üì
Function 1 output: agent_selection
  ‚Üì
Function 2 output: tool_name
  ‚Üì
Function 3 output: embedding_vector
  ‚Üì
Function 4 output: chunks_list
  ‚Üì
Final output: response_text
```

---

## Notebook Structure

### Part 1: Setup and Configuration (Cells 1-5)

#### Cell 1: Overview (Markdown)
```markdown
# FM-038: Step-by-Step Function Pipeline Tester

This notebook tests each function in the chat RAG pipeline individually,
capturing logs and outputs at each step.

**Advantages:**
- Direct function calls (not log analysis)
- Isolated per-function logging
- Output visualization between steps
- Repeatable local testing
- Easy debugging
```

#### Cell 2: Imports (Python)
```python
import sys
import os
import logging
from pathlib import Path
from datetime import datetime
import json
from typing import Any, Dict, List
from io import StringIO
import contextlib

# Add project to path
sys.path.insert(0, '/path/to/insurance_navigator')

# Import system functions
from agents.patient_navigator.agent import PatientNavigatorAgent
from agents.tooling.rag.core import retrieve_chunks_from_text, _generate_embedding
from backend.shared.external.openai_real import OpenAIClient
from db.services.vector_service import retrieve_chunks
# ... other imports

print("‚úÖ All imports loaded")
```

#### Cell 3: Log Management Setup (Python)
```python
# Create logs directory
LOGS_DIR = Path("tests/fm_038/function_logs")
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# Clear previous logs
for log_file in LOGS_DIR.glob("*.log"):
    log_file.unlink()

class FunctionLogger:
    """Captures logs for a specific function call"""
    
    def __init__(self, function_name: str, step_number: int):
        self.function_name = function_name
        self.step_number = step_number
        self.log_file = LOGS_DIR / f"{step_number:02d}_{function_name}.log"
        self.logs = []
        
    def __enter__(self):
        # Create handler for this function
        self.handler = logging.FileHandler(self.log_file, mode='w')
        self.handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        )
        self.handler.setFormatter(formatter)
        
        # Add to root logger
        logging.getLogger().addHandler(self.handler)
        logging.getLogger().setLevel(logging.DEBUG)
        
        # Also capture to memory
        self.string_handler = logging.StreamHandler(StringIO())
        self.string_handler.setFormatter(formatter)
        logging.getLogger().addHandler(self.string_handler)
        
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        # Remove handlers
        logging.getLogger().removeHandler(self.handler)
        logging.getLogger().removeHandler(self.string_handler)
        self.handler.close()
        
    def get_logs(self) -> str:
        """Read logs from file"""
        if self.log_file.exists():
            return self.log_file.read_text()
        return ""

print("‚úÖ Log management setup complete")
print(f"üìÅ Logs directory: {LOGS_DIR}")
```

#### Cell 4: Output Display Functions (Python)
```python
def display_function_step(step_num: int, function_name: str, 
                         inputs: Dict[str, Any], outputs: Any,
                         logs: str, duration_ms: float):
    """Display a function execution step with inputs, outputs, and logs"""
    
    from IPython.display import display, HTML, Markdown
    
    # Header
    display(HTML(f"""
    <div style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                color: white; border-radius: 10px; margin: 20px 0;">
        <h2 style="margin: 0;">Step {step_num}: {function_name}()</h2>
        <p style="margin: 5px 0 0 0;">Duration: {duration_ms:.2f}ms</p>
    </div>
    """))
    
    # Inputs
    display(Markdown("### üì• Inputs"))
    display(HTML(f"""
    <div style="padding: 15px; background: #f8f9fa; border-left: 4px solid #007bff; 
                border-radius: 5px; margin: 10px 0; font-family: monospace; max-height: 300px; overflow-y: auto;">
        <pre>{json.dumps(inputs, indent=2, default=str)}</pre>
    </div>
    """))
    
    # Outputs
    display(Markdown("### üì§ Outputs"))
    output_color = "#d4edda" if outputs is not None else "#f8d7da"
    display(HTML(f"""
    <div style="padding: 15px; background: {output_color}; border-left: 4px solid #28a745; 
                border-radius: 5px; margin: 10px 0; font-family: monospace; max-height: 300px; overflow-y: auto;">
        <pre>{json.dumps(outputs, indent=2, default=str) if outputs is not None else "None"}</pre>
    </div>
    """))
    
    # Logs
    if logs:
        display(Markdown("### üìã Function Logs"))
        log_lines = logs.strip().split('\n')
        log_html = "<br>".join(f"<code>{line}</code>" for line in log_lines[:50])
        if len(log_lines) > 50:
            log_html += f"<br><em>... and {len(log_lines) - 50} more lines</em>"
        
        display(HTML(f"""
        <div style="padding: 15px; background: #fff3cd; border-left: 4px solid #ffc107; 
                    border-radius: 5px; margin: 10px 0; font-family: monospace; font-size: 11px;
                    max-height: 200px; overflow-y: auto;">
            {log_html}
        </div>
        """))
    
    # Data flow arrow
    display(HTML("""
    <div style="text-align: center; margin: 20px 0;">
        <span style="font-size: 24px; color: #667eea;">‚¨áÔ∏è</span>
    </div>
    """))

def display_validation(step_num: int, validation_name: str, 
                       passed: bool, details: str):
    """Display validation checkpoint"""
    
    from IPython.display import display, HTML, Markdown
    
    color = "#d4edda" if passed else "#f8d7da"
    icon = "‚úÖ" if passed else "‚ùå"
    
    display(HTML(f"""
    <div style="padding: 15px; background: {color}; border-radius: 5px; margin: 10px 0;">
        <h4 style="margin: 0;">{icon} Validation {step_num}: {validation_name}</h4>
        <p style="margin: 5px 0 0 0;">{details}</p>
    </div>
    """))

print("‚úÖ Display functions ready")
```

#### Cell 5: Test Configuration (Python)
```python
# Test parameters
TEST_CONFIG = {
    'user_id': 'test-user-123',  # Replace with real user ID
    'query_text': 'What does my insurance cover?',
    'similarity_threshold': 0.5,
    'max_chunks': 5,
    'session_id': 'test-session-456'
}

# Environment setup
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', '')
os.environ['SUPABASE_URL'] = os.getenv('SUPABASE_URL', '')
os.environ['SUPABASE_KEY'] = os.getenv('SUPABASE_KEY', '')

display(Markdown("### üîß Test Configuration"))
display(HTML(f"<pre>{json.dumps(TEST_CONFIG, indent=2)}</pre>"))

print("‚úÖ Configuration loaded")
```

---

### Part 2: Function Pipeline Execution (Cells 6-20)

Each cell follows this pattern:

#### Cell Template
```python
# ============================================================
# STEP N: function_name()
# ============================================================

import time

step_num = N
function_name = "function_name"

display(Markdown(f"## Step {step_num}: {function_name}()"))

# Prepare inputs from previous step
inputs = {
    'param1': previous_output_1,
    'param2': previous_output_2,
    # ... from previous step
}

# Execute with logging
start_time = time.time()

with FunctionLogger(function_name, step_num) as logger:
    try:
        output = actual_function_call(**inputs)
        success = True
    except Exception as e:
        output = None
        success = False
        logging.error(f"Function failed: {e}", exc_info=True)

duration_ms = (time.time() - start_time) * 1000

# Get logs
logs = logger.get_logs()

# Display results
display_function_step(step_num, function_name, inputs, output, logs, duration_ms)

# Validation
if success and output is not None:
    display_validation(step_num, "Output exists", True, f"Function returned: {type(output)}")
else:
    display_validation(step_num, "Output exists", False, "Function returned None or failed")
    
# Store for next step
step_N_output = output
```

#### Specific Steps

**Cell 6: Step 1 - Chat Endpoint Entry**
```python
# Entry point simulation
step_1_output = {
    'user_id': TEST_CONFIG['user_id'],
    'query': TEST_CONFIG['query_text'],
    'session_id': TEST_CONFIG['session_id']
}
```

**Cell 7: Step 2 - Agent Selection**
```python
# Call agent selector
from core.agent_integration import select_agent

inputs = {'query': step_1_output['query']}

with FunctionLogger("select_agent", 2) as logger:
    agent_name = select_agent(**inputs)

# Display...
step_2_output = agent_name
```

**Cell 8: Step 3 - Patient Navigator Agent Init**
```python
from agents.patient_navigator.agent import PatientNavigatorAgent

inputs = {
    'user_id': step_1_output['user_id'],
    'session_id': step_1_output['session_id']
}

with FunctionLogger("patient_navigator_init", 3) as logger:
    agent = PatientNavigatorAgent(**inputs)

step_3_output = agent
```

**Cell 9: Step 4 - Tool Selection**
```python
inputs = {
    'query': step_1_output['query'],
    'agent': step_3_output
}

with FunctionLogger("tool_selection", 4) as logger:
    tool_name = agent.select_tool(step_1_output['query'])

step_4_output = tool_name
```

**Cell 10: Step 5 - RAG Tool: retrieve_chunks_from_text**
```python
from agents.tooling.rag.core import retrieve_chunks_from_text

inputs = {
    'user_id': step_1_output['user_id'],
    'query_text': step_1_output['query'],
    'similarity_threshold': TEST_CONFIG['similarity_threshold']
}

with FunctionLogger("retrieve_chunks_from_text", 5) as logger:
    chunks = retrieve_chunks_from_text(**inputs)

step_5_output = chunks
```

**Cell 11: Step 6 - Generate Embedding**
```python
from agents.tooling.rag.core import _generate_embedding

inputs = {
    'text': step_1_output['query']
}

with FunctionLogger("_generate_embedding", 6) as logger:
    embedding = _generate_embedding(**inputs)

# Validation
if embedding:
    display_validation(6, "Embedding dimensions", 
                      len(embedding) == 1536,
                      f"Embedding has {len(embedding)} dimensions (expected 1536)")

step_6_output = embedding
```

**Cell 12: Step 7 - OpenAI API Call**
```python
from backend.shared.external.openai_real import OpenAIClient

openai_client = OpenAIClient()

inputs = {
    'text': step_1_output['query'],
    'model': 'text-embedding-3-small'
}

with FunctionLogger("openai_create_embedding", 7) as logger:
    response = openai_client.create_embedding(**inputs)
    embedding_from_api = response.get('embedding') if response else None

# Validation
if embedding_from_api:
    display_validation(7, "OpenAI API success", True,
                      f"API returned embedding with {len(embedding_from_api)} dimensions")
else:
    display_validation(7, "OpenAI API success", False,
                      "API did not return embedding")

step_7_output = embedding_from_api
```

**Cell 13: Step 8 - Database Query (retrieve_chunks)**
```python
from db.services.vector_service import retrieve_chunks

inputs = {
    'user_id': step_1_output['user_id'],
    'embedding': step_7_output,
    'similarity_threshold': TEST_CONFIG['similarity_threshold'],
    'limit': TEST_CONFIG['max_chunks']
}

with FunctionLogger("retrieve_chunks", 8) as logger:
    db_chunks = retrieve_chunks(**inputs)

# Validation
if db_chunks is not None:
    display_validation(8, "Chunks retrieved", 
                      len(db_chunks) > 0,
                      f"Retrieved {len(db_chunks)} chunks from database")
else:
    display_validation(8, "Chunks retrieved", False,
                      "No chunks returned from database")

step_8_output = db_chunks
```

**Cell 14: Step 9 - Format Chunks**
```python
def format_chunks(chunks):
    """Format chunks for context"""
    if not chunks:
        return ""
    return "\n\n".join([f"[{i+1}] {chunk['content']}" for i, chunk in enumerate(chunks)])

inputs = {'chunks': step_8_output}

with FunctionLogger("format_chunks", 9) as logger:
    formatted_context = format_chunks(**inputs)

step_9_output = formatted_context
```

**Cell 15: Step 10 - Generate Response**
```python
from agents.patient_navigator.response_generator import generate_response

inputs = {
    'query': step_1_output['query'],
    'context': step_9_output,
    'user_id': step_1_output['user_id']
}

with FunctionLogger("generate_response", 10) as logger:
    final_response = generate_response(**inputs)

step_10_output = final_response
```

---

### Part 3: Analysis and Validation (Cells 16-20)

#### Cell 16: Complete Pipeline Summary
```python
display(Markdown("## üìä Complete Pipeline Summary"))

pipeline_results = {
    'Step 1: Chat Entry': 'Success',
    'Step 2: Agent Selection': step_2_output,
    'Step 3: Agent Init': 'Success' if step_3_output else 'Failed',
    'Step 4: Tool Selection': step_4_output,
    'Step 5: RAG Tool': f"{len(step_5_output)} chunks" if step_5_output else "0 chunks",
    'Step 6: Generate Embedding': f"{len(step_6_output)} dimensions" if step_6_output else "Failed",
    'Step 7: OpenAI API': 'Success' if step_7_output else 'Failed',
    'Step 8: DB Query': f"{len(step_8_output)} chunks" if step_8_output else "0 chunks",
    'Step 9: Format': f"{len(step_9_output)} chars" if step_9_output else "Empty",
    'Step 10: Response': 'Generated' if step_10_output else 'Failed'
}

# Display as table
import pandas as pd
df = pd.DataFrame(list(pipeline_results.items()), columns=['Step', 'Result'])
display(df)
```

#### Cell 17: Identify Failure Point
```python
display(Markdown("## üéØ Failure Point Analysis"))

# Check each step
failure_point = None

checks = [
    (2, "Agent Selection", step_2_output is not None),
    (3, "Agent Init", step_3_output is not None),
    (4, "Tool Selection", step_4_output is not None),
    (5, "RAG Tool", step_5_output is not None and len(step_5_output) > 0),
    (6, "Generate Embedding", step_6_output is not None and len(step_6_output) == 1536),
    (7, "OpenAI API", step_7_output is not None),
    (8, "DB Query", step_8_output is not None and len(step_8_output) > 0),
    (9, "Format Chunks", step_9_output is not None and len(step_9_output) > 0),
    (10, "Generate Response", step_10_output is not None)
]

for step_num, step_name, passed in checks:
    icon = "‚úÖ" if passed else "‚ùå"
    display(Markdown(f"{icon} **Step {step_num}: {step_name}**"))
    
    if not passed and failure_point is None:
        failure_point = (step_num, step_name)
        display(HTML(f"""
        <div style="padding: 20px; background: #f8d7da; border: 2px solid #dc3545; 
                    border-radius: 10px; margin: 20px 0;">
            <h3 style="color: #dc3545; margin-top: 0;">üö® FAILURE POINT IDENTIFIED</h3>
            <p><strong>Step {step_num}: {step_name}</strong></p>
            <p>This is where the pipeline failed. Review the logs and outputs above for this step.</p>
        </div>
        """))

if failure_point is None:
    display(HTML("""
    <div style="padding: 20px; background: #d4edda; border: 2px solid #28a745; 
                border-radius: 10px; margin: 20px 0;">
        <h3 style="color: #28a745; margin-top: 0;">‚úÖ ALL STEPS PASSED</h3>
        <p>The complete pipeline executed successfully!</p>
    </div>
    """))
```

#### Cell 18: Log Analysis
```python
display(Markdown("## üìã Log Analysis"))

# Read all log files
all_logs = []
for log_file in sorted(LOGS_DIR.glob("*.log")):
    content = log_file.read_text()
    all_logs.append({
        'file': log_file.name,
        'size': len(content),
        'lines': len(content.split('\n')),
        'errors': content.count('[ERROR]'),
        'warnings': content.count('[WARNING]')
    })

df_logs = pd.DataFrame(all_logs)
display(df_logs)

# Highlight problematic logs
problems = df_logs[(df_logs['errors'] > 0) | (df_logs['warnings'] > 0)]
if not problems.empty:
    display(Markdown("### ‚ö†Ô∏è Logs with Issues:"))
    display(problems)
```

#### Cell 19: Data Flow Visualization
```python
display(Markdown("## üîÑ Data Flow Visualization"))

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

fig, ax = plt.subplots(figsize=(12, 10))

# Define pipeline steps
steps = [
    "Chat Entry",
    "Agent Selection", 
    "Agent Init",
    "Tool Selection",
    "RAG Tool",
    "Generate Embedding",
    "OpenAI API",
    "DB Query",
    "Format Chunks",
    "Generate Response"
]

# Create flow diagram
for i, step in enumerate(steps):
    # Check if step passed
    passed = checks[i][2] if i < len(checks) else True
    color = 'lightgreen' if passed else 'lightcoral'
    
    # Draw box
    box = mpatches.FancyBboxPatch((1, 10-i), 8, 0.8, 
                                   boxstyle="round,pad=0.1",
                                   facecolor=color, edgecolor='black', linewidth=2)
    ax.add_patch(box)
    
    # Add text
    ax.text(5, 10-i+0.4, f"{i+1}. {step}", 
           ha='center', va='center', fontsize=12, weight='bold')
    
    # Draw arrow to next step
    if i < len(steps) - 1:
        ax.arrow(5, 10-i-0.1, 0, -0.7, head_width=0.3, head_length=0.1, 
                fc='gray', ec='gray')

ax.set_xlim(0, 10)
ax.set_ylim(0, 11)
ax.axis('off')
plt.title('Pipeline Execution Flow', fontsize=16, weight='bold')
plt.tight_layout()
plt.show()
```

#### Cell 20: Export Results
```python
display(Markdown("## üíæ Export Results"))

# Create summary report
report = {
    'timestamp': datetime.now().isoformat(),
    'config': TEST_CONFIG,
    'results': pipeline_results,
    'failure_point': {
        'step': failure_point[0],
        'name': failure_point[1]
    } if failure_point else None,
    'logs_directory': str(LOGS_DIR),
    'final_output': step_10_output
}

report_file = Path(f"tests/fm_038/pipeline_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
report_file.write_text(json.dumps(report, indent=2, default=str))

display(Markdown(f"‚úÖ **Report saved:** `{report_file}`"))
display(Markdown(f"‚úÖ **Logs saved:** `{LOGS_DIR}/`"))

print(f"\n{'='*60}")
print("PIPELINE TEST COMPLETE")
print(f"{'='*60}")
print(f"Failure Point: Step {failure_point[0]} - {failure_point[1]}" if failure_point else "All steps passed")
print(f"Logs: {LOGS_DIR}")
print(f"Report: {report_file}")
print(f"{'='*60}")
```

---

## Success Criteria

Your implementation is successful if:

‚úÖ Each function in the pipeline can be called directly  
‚úÖ Logs are captured per function in separate files  
‚úÖ Inputs and outputs are displayed for each step  
‚úÖ Data transformations are visible  
‚úÖ Failure point is automatically identified  
‚úÖ All logs are organized and accessible  
‚úÖ Notebook can be run locally (not dependent on production)  
‚úÖ Results can be exported for documentation  

---

## Key Advantages Over Phase 2B

| Aspect | Phase 2B (Log Analysis) | Phase 2C (Function Testing) |
|--------|------------------------|----------------------------|
| **Execution** | Analyzes past logs | Calls functions directly |
| **Control** | Read-only analysis | Full control of inputs |
| **Repeatability** | Depends on production | Fully repeatable |
| **Debugging** | After-the-fact | Real-time |
| **Isolation** | Limited | Per-function isolation |
| **Local Testing** | No | Yes |
| **Data Visibility** | Log parsing | Direct output inspection |

---

## Expected Outcome

After running this notebook, you will know:

1. **Exact failure step** - Which function failed
2. **Failure cause** - Why it failed (from logs and outputs)
3. **Data state** - What data looked like before failure
4. **Function behavior** - How each function transformed data
5. **Reproducible test** - Can run locally anytime

---

## References

- **Scope:** `docs/initiatives/debug/fm_038_chat_rag_failures/phase_2c_scope.md`
- **RAG Core:** `agents/tooling/rag/core.py`
- **Agent:** `agents/patient_navigator/agent.py`
- **Database:** `db/services/vector_service.py`

---

**Next Steps After Completion:**
1. Run notebook with test user data
2. Identify failing function
3. Fix the specific function
4. Re-run to verify fix
5. Add regression test

---

**Phase:** 2C - Step-by-Step Function Testing  
**Status:** Ready for Implementation  
**Expected Time:** 4-6 hours  
**Deliverable:** `tests/fm_038/FM_038_Function_Pipeline_Test.ipynb`

