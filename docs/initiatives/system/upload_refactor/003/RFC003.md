# RFC003: Worker Refactor - Local-First Development Architecture

## Context & Overview

This RFC defines the technical architecture for the 003 Worker Refactor iteration, implementing a local-first development approach with Docker-based testing environments. The design builds upon the 002 BaseWorker architecture while addressing critical infrastructure validation, testing strategy, and deployment verification gaps identified in the 002 post-mortem.

**Migration Context:** This architecture maintains the unified BaseWorker design from 002 while prioritizing local development environment setup, comprehensive testing validation, and infrastructure configuration management to prevent the deployment and testing failures experienced in 002.

**Reference Documents:**
- `@docs/initiatives/system/upload_refactor/003/PRD003.md` - Product requirements and success criteria
- `@docs/initiatives/system/upload_refactor/002/POSTMORTEM002.md` - Lessons learned from 002 failures
- `@docs/initiatives/system/upload_refactor/002/CONTEXT002.md` - Architectural foundation to build upon

## Architecture Overview

### High-Level Development Flow

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Local Dev     │    │   Infrastructure│    │   Production    │
│   Environment   │ => │   Validation    │ => │   Deployment    │
│   (Docker)      │    │   (Automated)   │    │   (Verified)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
    Local Testing          Config Validation        Real Processing
    Mock Services          Health Checks            Monitoring/Alerts
    Complete Pipeline      Service Verification     Production Data
```

### Development Environment Architecture

```
Local Docker Environment:
┌─────────────────────────────────────────────────────────────────┐
│                         Docker Compose Stack                    │
├─────────────────┬─────────────────┬─────────────────────────────┤
│   API Server    │   BaseWorker    │        Dependencies         │
│   (FastAPI)     │   (Unified)     │                             │
│   - Webhooks    │   - State Mgmt  │   - Postgres + Vector Ext   │
│   - Job Status  │   - Buffer Ops  │   - Supabase Local          │
│   - Health      │   - External    │   - Mock LlamaParse         │
│                 │     Services    │   - Mock OpenAI             │
└─────────────────┴─────────────────┴─────────────────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌─────────────────────┐
                    │   Local Testing     │
                    │   & Validation      │
                    │   - E2E Pipeline    │
                    │   - State Machine   │
                    │   - Buffer Ops      │
                    │   - External APIs   │
                    └─────────────────────┘
```

## Local Development Environment

### Docker Compose Architecture

**Core Services:**
```yaml
# docker-compose.yml
version: '3.8'
services:
  postgres:
    image: pgvector/pgvector:pg15
    environment:
      POSTGRES_DB: accessa_dev
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d

  supabase-storage:
    image: supabase/storage-api:latest
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/accessa_dev
      PGRST_JWT_SECRET: super-secret-jwt-token
    ports:
      - "5000:5000"
    depends_on:
      - postgres

  api-server:
    build:
      context: .
      dockerfile: backend/api/Dockerfile
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/accessa_dev
      SUPABASE_URL: http://supabase-storage:5000
      LLAMAPARSE_API_URL: http://mock-llamaparse:8001
      OPENAI_API_URL: http://mock-openai:8002
    depends_on:
      - postgres
      - supabase-storage

  base-worker:
    build:
      context: .
      dockerfile: backend/workers/Dockerfile
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/accessa_dev
      SUPABASE_URL: http://supabase-storage:5000
      LLAMAPARSE_API_URL: http://mock-llamaparse:8001
      OPENAI_API_URL: http://mock-openai:8002
    depends_on:
      - postgres
      - api-server

  mock-llamaparse:
    build:
      context: ./testing/mocks
      dockerfile: llamaparse.Dockerfile
    ports:
      - "8001:8001"

  mock-openai:
    build:
      context: ./testing/mocks
      dockerfile: openai.Dockerfile
    ports:
      - "8002:8002"

  monitoring:
    build:
      context: ./monitoring
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      POSTGRES_URL: postgresql://postgres:postgres@postgres:5432/accessa_dev
    depends_on:
      - postgres

volumes:
  postgres_data:
```

### Project Directory Structure

The 003 implementation maintains the same directory structure established in 002 with additional infrastructure and testing components:

```
backend/
├── api/
│   ├── main.py                 # FastAPI application
│   ├── webhooks/
│   │   ├── __init__.py
│   │   └── llamaparse.py       # Webhook handlers
│   ├── jobs/
│   │   ├── __init__.py
│   │   └── status.py           # Job status endpoints
│   └── middleware/
│       └── auth.py             # Authentication middleware
├── workers/
│   ├── __init__.py
│   ├── base_worker.py          # Enhanced BaseWorker class
│   ├── runner.py               # Worker process runner
│   └── processors/
│       ├── __init__.py
│       ├── parse_validator.py  # Parse stage logic
│       ├── chunk_processor.py  # Chunking logic
│       └── embed_batcher.py    # Embedding logic
├── shared/
│   ├── db/
│   │   ├── __init__.py
│   │   ├── connection.py       # Database connection management
│   │   └── models.py           # SQLAlchemy models
│   ├── storage/
│   │   ├── __init__.py
│   │   └── supabase.py         # Storage client
│   ├── external/
│   │   ├── __init__.py
│   │   ├── llamaparse.py       # LlamaParse client
│   │   └── openai_client.py    # OpenAI client
│   ├── logging/
│   │   ├── __init__.py
│   │   └── structured.py       # Enhanced structured logging
│   ├── rate_limit/
│   │   ├── __init__.py
│   │   └── token_bucket.py     # Rate limiting
│   └── schemas/
│       ├── __init__.py
│       ├── jobs.py             # Job-related schemas
│       └── webhooks.py         # Webhook schemas
├── scripts/
│   ├── migrate.py              # Database migrations
│   ├── worker_runner.py        # Worker process launcher
│   ├── cleanup.py              # Buffer cleanup utilities
│   ├── setup-local-env.sh      # Local environment setup
│   ├── run-local-tests.sh      # Local testing script
│   └── validate-local-environment.sh  # Environment validation
└── tests/                      # Comprehensive testing framework
    ├── unit/
    │   ├── test_state_machine.py
    │   ├── test_buffer_operations.py
    │   └── test_external_services.py
    ├── integration/
    │   ├── test_mock_services.py
    │   └── test_real_apis.py
    ├── e2e/
    │   └── test_complete_pipeline.py
    └── performance/
        ├── test_large_documents.py
        ├── test_concurrent_workers.py
        └── test_database_load.py

infrastructure/                 # New in 003: Infrastructure validation
├── validation/
│   └── deployment_validator.py # Automated deployment validation
├── config/
│   ├── production.yaml         # Production configuration
│   ├── staging.yaml            # Staging configuration
│   └── render.yaml             # Render deployment config
├── monitoring/
│   ├── dashboard.py            # Real-time monitoring dashboard
│   └── alerts.py               # Alerting configuration
└── scripts/
    ├── deploy-and-verify.sh    # Deployment with validation
    ├── rollback-complete.sh    # Automated rollback
    └── validate-production-deployment.py  # Production validation

testing/                        # New in 003: Mock services and testing
├── mocks/
│   ├── llamaparse.Dockerfile
│   ├── llamaparse_server.py    # Mock LlamaParse service
│   ├── openai.Dockerfile
│   └── openai_server.py        # Mock OpenAI service
└── fixtures/
    ├── sample_documents/       # Test documents
    └── test_data.json          # Test data fixtures

monitoring/                     # New in 003: Local monitoring
├── Dockerfile
├── dashboard.py                # Real-time processing dashboard
└── requirements.txt

scripts/                        # Root-level scripts
├── setup-local-env.sh          # Complete environment setup
├── run-local-tests.sh          # Comprehensive testing
├── validate-local-environment.sh  # Environment validation
└── deploy-and-verify.sh        # Production deployment
```

### Local Environment Setup Scripts

**Environment Initialization:**
```bash
#!/bin/bash
# scripts/setup-local-env.sh

set -e

echo "Setting up local development environment..."

# Check prerequisites
command -v docker >/dev/null 2>&1 || { echo "Docker required but not installed." >&2; exit 1; }
command -v docker-compose >/dev/null 2>&1 || { echo "Docker Compose required but not installed." >&2; exit 1; }

# Create necessary directories
mkdir -p local-storage/raw local-storage/parsed
mkdir -p logs/api logs/worker logs/monitoring

# Copy environment configuration
cp .env.example .env.local

# Build and start services
echo "Building Docker images..."
docker-compose build

echo "Starting services..."
docker-compose up -d postgres supabase-storage

# Wait for database to be ready
echo "Waiting for database to be ready..."
until docker-compose exec postgres pg_isready -U postgres; do
  sleep 2
done

# Run database migrations
echo "Running database migrations..."
docker-compose exec postgres psql -U postgres -d accessa_dev -f /docker-entrypoint-initdb.d/001_create_schemas.sql

# Start remaining services
echo "Starting application services..."
docker-compose up -d

# Verify all services are healthy
echo "Verifying service health..."
sleep 10

# Health check API server
curl -f http://localhost:8000/health || { echo "API server health check failed" >&2; exit 1; }

# Health check worker process
docker-compose exec base-worker python -c "from backend.workers.base_worker import BaseWorker; print('Worker import successful')"

echo "Local environment setup complete!"
echo "Access API server at: http://localhost:8000"
echo "Access monitoring at: http://localhost:3000"
echo "View logs with: docker-compose logs -f [service-name]"
```

**Local Testing Script:**
```bash
#!/bin/bash
# scripts/run-local-tests.sh

set -e

echo "Running comprehensive local tests..."

# Ensure environment is running
docker-compose ps | grep "Up" || { echo "Local environment not running. Run ./scripts/setup-local-env.sh first." >&2; exit 1; }

# Run unit tests
echo "Running unit tests..."
docker-compose exec api-server python -m pytest backend/tests/unit/ -v

# Run integration tests
echo "Running integration tests..."
docker-compose exec api-server python -m pytest backend/tests/integration/ -v

# Run end-to-end pipeline test
echo "Running end-to-end pipeline test..."
docker-compose exec api-server python backend/tests/e2e/test_complete_pipeline.py

# Validate state machine transitions
echo "Testing state machine transitions..."
docker-compose exec base-worker python backend/tests/state_machine/test_all_transitions.py

# Test buffer operations
echo "Testing buffer operations..."
docker-compose exec api-server python backend/tests/buffers/test_idempotent_operations.py

# Test external service integration
echo "Testing external service integration..."
docker-compose exec base-worker python backend/tests/external/test_llamaparse_integration.py
docker-compose exec base-worker python backend/tests/external/test_openai_integration.py

# Performance test with sample documents
echo "Running performance tests..."
docker-compose exec api-server python backend/tests/performance/test_document_processing.py

echo "All local tests completed successfully!"
```

## Mock Service Implementation

### LlamaParse Mock Service

```python
# testing/mocks/llamaparse_server.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
import asyncio
import httpx
import hashlib
import hmac
from datetime import datetime

app = FastAPI()

@app.post("/parse")
async def submit_parse_job(
    background_tasks: BackgroundTasks,
    request: dict
):
    """Mock LlamaParse job submission with webhook callback"""
    job_id = request.get("job_id")
    source_url = request.get("source_url")
    webhook_url = request.get("webhook_url")
    
    if not all([job_id, source_url, webhook_url]):
        raise HTTPException(status_code=400, detail="Missing required fields")
    
    # Simulate async processing with callback
    background_tasks.add_task(
        simulate_parse_and_callback,
        job_id, source_url, webhook_url
    )
    
    return {"parse_job_id": f"mock-parse-{job_id}", "status": "queued"}

async def simulate_parse_and_callback(job_id: str, source_url: str, webhook_url: str):
    """Simulate parsing delay and webhook callback"""
    # Simulate processing time
    await asyncio.sleep(2)
    
    # Generate mock parsed content
    parsed_content = f"""# Document {job_id}

This is a mock parsed document for testing purposes.

## Section 1
Sample content for chunking and embedding tests.

## Section 2  
Additional content to ensure multiple chunks are generated.

Processing timestamp: {datetime.utcnow().isoformat()}
"""
    
    # Calculate content hash
    content_hash = hashlib.sha256(parsed_content.encode()).hexdigest()
    
    # Prepare webhook callback payload
    payload = {
        "job_id": job_id,
        "status": "parsed",
        "artifacts": [{
            "type": "markdown",
            "content": parsed_content,
            "sha256": content_hash,
            "bytes": len(parsed_content.encode())
        }],
        "meta": {
            "parser_name": "mock-llamaparse",
            "parser_version": "1.0.0"
        }
    }
    
    # Send webhook callback
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(webhook_url, json=payload)
            print(f"Webhook callback sent for {job_id}: {response.status_code}")
        except Exception as e:
            print(f"Webhook callback failed for {job_id}: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

### OpenAI Mock Service

```python
# testing/mocks/openai_server.py
from fastapi import FastAPI, HTTPException
import numpy as np
from typing import List
import hashlib

app = FastAPI()

@app.post("/v1/embeddings")
async def create_embeddings(request: dict):
    """Mock OpenAI embeddings API"""
    input_texts = request.get("input", [])
    model = request.get("model", "text-embedding-3-small")
    
    if not input_texts:
        raise HTTPException(status_code=400, detail="No input provided")
    
    if isinstance(input_texts, str):
        input_texts = [input_texts]
    
    # Generate deterministic mock embeddings
    embeddings_data = []
    for i, text in enumerate(input_texts):
        # Generate deterministic embedding based on text content
        text_hash = hashlib.md5(text.encode()).hexdigest()
        np.random.seed(int(text_hash[:8], 16))
        embedding = np.random.normal(0, 1, 1536).tolist()
        
        embeddings_data.append({
            "object": "embedding",
            "embedding": embedding,
            "index": i
        })
    
    return {
        "object": "list",
        "data": embeddings_data,
        "model": model,
        "usage": {
            "prompt_tokens": sum(len(text.split()) for text in input_texts),
            "total_tokens": sum(len(text.split()) for text in input_texts)
        }
    }

@app.get("/v1/models")
async def list_models():
    """Mock models endpoint"""
    return {
        "object": "list",
        "data": [{
            "id": "text-embedding-3-small",
            "object": "model",
            "created": 1677610602,
            "owned_by": "openai"
        }]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
```

## Infrastructure Validation Framework

### Deployment Configuration Validation

```python
# infrastructure/validation/deployment_validator.py
import asyncio
import httpx
import psycopg2
from typing import Dict, List, Optional
import yaml
import os

class DeploymentValidator:
    """Validate deployment infrastructure against local environment"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
    
    async def validate_complete_deployment(self) -> Dict[str, bool]:
        """Run complete deployment validation suite"""
        results = {}
        
        # Database connectivity and schema
        results["database"] = await self._validate_database()
        
        # API server health and endpoints
        results["api_server"] = await self._validate_api_server()
        
        # Worker process health
        results["worker_process"] = await self._validate_worker_process()
        
        # External service connectivity
        results["external_services"] = await self._validate_external_services()
        
        # Storage configuration
        results["storage"] = await self._validate_storage()
        
        # Environment configuration
        results["environment"] = await self._validate_environment()
        
        return results
    
    async def _validate_database(self) -> bool:
        """Validate database connectivity and schema"""
        try:
            conn = psycopg2.connect(self.config["database"]["url"])
            cursor = conn.cursor()
            
            # Check required tables exist
            required_tables = [
                "upload_jobs",
                "document_chunk_buffer", 
                "document_vector_buffer"
            ]
            
            for table in required_tables:
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = %s
                    );
                """, (table,))
                
                if not cursor.fetchone()[0]:
                    print(f"Missing required table: {table}")
                    return False
            
            # Check vector extension
            cursor.execute("SELECT EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'vector');")
            if not cursor.fetchone()[0]:
                print("Missing vector extension")
                return False
            
            conn.close()
            return True
            
        except Exception as e:
            print(f"Database validation failed: {e}")
            return False
    
    async def _validate_api_server(self) -> bool:
        """Validate API server health and endpoints"""
        try:
            api_url = self.config["api"]["url"]
            
            async with httpx.AsyncClient() as client:
                # Health check
                response = await client.get(f"{api_url}/health")
                if response.status_code != 200:
                    print(f"API health check failed: {response.status_code}")
                    return False
                
                # Webhook endpoint check
                response = await client.post(
                    f"{api_url}/webhooks/llamaparse",
                    json={"test": "validation"},
                    headers={"X-Test-Validation": "true"}
                )
                # Should return 401 for missing auth, not 404
                if response.status_code == 404:
                    print("Webhook endpoint not found")
                    return False
            
            return True
            
        except Exception as e:
            print(f"API server validation failed: {e}")
            return False
    
    async def _validate_worker_process(self) -> bool:
        """Validate worker process is running and functional"""
        try:
            # Check if worker process is actually running
            # This would need to be adapted based on deployment platform
            worker_health_url = self.config.get("worker", {}).get("health_url")
            
            if worker_health_url:
                async with httpx.AsyncClient() as client:
                    response = await client.get(worker_health_url)
                    if response.status_code != 200:
                        print(f"Worker health check failed: {response.status_code}")
                        return False
            
            return True
            
        except Exception as e:
            print(f"Worker validation failed: {e}")
            return False
    
    async def _validate_external_services(self) -> bool:
        """Validate external service connectivity"""
        try:
            # Test LlamaParse connectivity
            llamaparse_url = self.config["external"]["llamaparse"]["url"]
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{llamaparse_url}/health")
                # Real service might not have health endpoint, so any response is good
            
            # Test OpenAI connectivity
            openai_url = self.config["external"]["openai"]["url"]
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{openai_url}/v1/models")
                if response.status_code not in [200, 401]:  # 401 is OK, means API is up
                    print(f"OpenAI connectivity failed: {response.status_code}")
                    return False
            
            return True
            
        except Exception as e:
            print(f"External services validation failed: {e}")
            return False
    
    async def _validate_storage(self) -> bool:
        """Validate storage configuration and accessibility"""
        try:
            # Test Supabase storage connectivity
            storage_url = self.config["storage"]["url"]
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{storage_url}/health")
                # Storage service health validation
            
            return True
            
        except Exception as e:
            print(f"Storage validation failed: {e}")
            return False
    
    async def _validate_environment(self) -> bool:
        """Validate environment configuration"""
        try:
            required_env_vars = [
                "DATABASE_URL",
                "SUPABASE_URL", 
                "SUPABASE_SERVICE_ROLE_KEY",
                "LLAMAPARSE_API_KEY",
                "OPENAI_API_KEY"
            ]
            
            missing_vars = []
            for var in required_env_vars:
                if not os.getenv(var):
                    missing_vars.append(var)
            
            if missing_vars:
                print(f"Missing environment variables: {missing_vars}")
                return False
            
            return True
            
        except Exception as e:
            print(f"Environment validation failed: {e}")
            return False

# CLI interface for deployment validation
async def main():
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python deployment_validator.py <config.yaml>")
        sys.exit(1)
    
    validator = DeploymentValidator(sys.argv[1])
    results = await validator.validate_complete_deployment()
    
    print("\nDeployment Validation Results:")
    print("=" * 40)
    
    all_passed = True
    for component, passed in results.items():
        status = "✅ PASS" if passed else "❌ FAIL"
        print(f"{component:20} {status}")
        if not passed:
            all_passed = False
    
    print("=" * 40)
    
    if all_passed:
        print("✅ All validation checks passed!")
        sys.exit(0)
    else:
        print("❌ Some validation checks failed!")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
```

## Enhanced BaseWorker Implementation

### State Machine with Comprehensive Logging

```python
# backend/workers/base_worker.py
import asyncio
import logging
from typing import Optional, Dict, Any
from uuid import UUID
import json
from datetime import datetime, timedelta

from backend.shared.db.connection import DatabaseManager
from backend.shared.storage.supabase import StorageManager
from backend.shared.external.llamaparse import LlamaParseClient
from backend.shared.external.openai_client import OpenAIClient
from backend.shared.logging.structured import StructuredLogger

class BaseWorker:
    """Enhanced BaseWorker with comprehensive logging and validation"""
    
    def __init__(self, config):
        self.db = DatabaseManager(config.database_url)
        self.storage = StorageManager(config.storage_config)
        self.llamaparse = LlamaParseClient(config.llamaparse_config)
        self.openai = OpenAIClient(config.openai_config)
        self.logger = StructuredLogger("base_worker")
        
        # Performance and monitoring
        self.processing_metrics = {}
        self.error_counts = {}
    
    async def process_jobs_continuously(self):
        """Main worker loop with health monitoring"""
        self.logger.info("Starting BaseWorker continuous processing")
        
        while True:
            try:
                # Get next available job
                job = await self._get_next_job()
                
                if job:
                    await self._process_single_job(job)
                else:
                    # No jobs available, wait before polling again
                    await asyncio.sleep(5)
                    
            except Exception as e:
                self.logger.error("Unexpected error in worker loop", error=str(e))
                await asyncio.sleep(10)  # Longer wait on errors
    
    async def _get_next_job(self) -> Optional[Dict[str, Any]]:
        """Get next job with proper locking and error handling"""
        try:
            async with self.db.transaction() as tx:
                # Optimized job polling query
                job = await tx.fetchrow("""
                    WITH next_job AS (
                        SELECT job_id, user_id, document_id, status, raw_path, parsed_path,
                               parsed_sha256, chunks_version, embed_model, embed_version,
                               progress, retry_count, last_error, created_at, updated_at
                        FROM upload_jobs
                        WHERE status IN (
                            'parsed', 'parse_validated', 'chunks_stored', 
                            'embedding_queued', 'embedding_in_progress'
                        )
                        AND (
                            last_error IS NULL 
                            OR (last_error->>'retry_at')::timestamp <= now()
                        )
                        ORDER BY created_at
                        FOR UPDATE SKIP LOCKED
                        LIMIT 1
                    )
                    SELECT * FROM next_job
                """)
                
                if job:
                    job_dict = dict(job)
                    self.logger.info(
                        "Retrieved job for processing",
                        job_id=str(job_dict["job_id"]),
                        status=job_dict["status"],
                        document_id=str(job_dict["document_id"])
                    )
                    return job_dict
                
                return None
                
        except Exception as e:
            self.logger.error("Failed to retrieve next job", error=str(e))
            return None
    
    async def _process_single_job(self, job: Dict[str, Any]):
        """Process a single job through the state machine"""
        job_id = job["job_id"]
        status = job["status"]
        
        start_time = datetime.utcnow()
        
        try:
            self.logger.info(
                "Starting job processing",
                job_id=str(job_id),
                status=status,
                document_id=str(job["document_id"])
            )
            
            # Route to appropriate processor based on status
            if status == "parsed":
                await self._validate_parsed(job)
            elif status == "parse_validated":
                await self._process_chunks(job)
            elif status == "chunks_stored":
                await self._queue_embeddings(job)
            elif status in ["embedding_queued", "embedding_in_progress"]:
                await self._process_embeddings(job)
            elif status == "embeddings_stored":
                await self._finalize_job(job)
            else:
                raise ValueError(f"Unexpected job status: {status}")
            
            # Record successful processing metrics
            duration = (datetime.utcnow() - start_time).total_seconds()
            self._record_processing_success(status, duration)
            
            self.logger.info(
                "Job processing completed successfully",
                job_id=str(job_id),
                status=status,
                duration_seconds=duration
            )
            
        except Exception as e:
            duration = (datetime.utcnow() - start_time).total_seconds()
            self._record_processing_error(status, str(e))
            
            self.logger.error(
                "Job processing failed",
                job_id=str(job_id),
                status=status,
                error=str(e),
                duration_seconds=duration
            )
            
            # Handle error - retry or mark as failed
            await self._handle_processing_error(job, e)
    
    async def _validate_parsed(self, job: Dict[str, Any]):
        """Validate parsed content with comprehensive error checking"""
        job_id = job["job_id"]
        parsed_path = job["parsed_path"]
        
        if not parsed_path:
            raise ValueError("No parsed_path found for parsed job")
        
        try:
            # Read parsed content from storage
            parsed_content = await self.storage.read_blob(parsed_path)
            
            if not parsed_content or len(parsed_content.strip()) == 0:
                raise ValueError("Parsed content is empty")
            
            # Normalize and hash content
            normalized_content = self._normalize_markdown(parsed_content)
            content_sha = self._compute_sha256(normalized_content)
            
            # Check for duplicate parsed content
            async with self.db.transaction() as tx:
                existing = await tx.fetchrow("""
                    SELECT job_id, parsed_path 
                    FROM upload_jobs 
                    WHERE parsed_sha256 = $1 AND job_id != $2
                    LIMIT 1
                """, content_sha, job_id)
                
                if existing:
                    # Use canonical path for duplicate
                    canonical_path = existing["parsed_path"]
                    self.logger.info(
                        "Using canonical path for duplicate content",
                        job_id=str(job_id),
                        canonical_path=canonical_path,
                        original_path=parsed_path
                    )
                    parsed_path = canonical_path
                
                # Update job with validation results
                await tx.execute("""
                    UPDATE upload_jobs 
                    SET parsed_path = $1, parsed_sha256 = $2, status = 'parse_validated',
                        updated_at = now()
                    WHERE job_id = $3
                """, parsed_path, content_sha, job_id)
                
                self.logger.info(
                    "Parse validation completed",
                    job_id=str(job_id),
                    content_sha=content_sha,
                    content_length=len(parsed_content)
                )
        
        except Exception as e:
            self.logger.error(
                "Parse validation failed",
                job_id=str(job_id),
                parsed_path=parsed_path,
                error=str(e)
            )
            raise
    
    async def _process_chunks(self, job: Dict[str, Any]):
        """Generate chunks with comprehensive validation"""
        job_id = job["job_id"]
        document_id = job["document_id"]
        parsed_path = job["parsed_path"]
        chunks_version = job["chunks_version"]
        
        try:
            # Read parsed content
            parsed_content = await self.storage.read_blob(parsed_path)
            
            # Generate chunks using specified chunker
            chunks = await self._generate_chunks(parsed_content, chunks_version)
            
            if not chunks:
                raise ValueError("No chunks generated from parsed content")
            
            # Write chunks to buffer with idempotent operations
            async with self.db.transaction() as tx:
                chunks_written = 0
                
                for chunk in chunks:
                    chunk_id = self._generate_chunk_id(
                        document_id, chunk["chunker_name"], 
                        chunk["chunker_version"], chunk["ord"]
                    )
                    
                    chunk_sha = self._compute_sha256(chunk["text"])
                    
                    # Idempotent chunk write
                    result = await tx.execute("""
                        INSERT INTO document_chunk_buffer 
                        (chunk_id, document_id, chunk_ord, chunker_name, chunker_version,
                         chunk_sha, text, meta, created_at)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, now())
                        ON CONFLICT (chunk_id) DO NOTHING
                    """, chunk_id, document_id, chunk["ord"], chunk["chunker_name"],
                        chunk["chunker_version"], chunk_sha, chunk["text"], 
                        json.dumps(chunk["meta"]))
                    
                    # Count actual writes (not conflicts)
                    if result.split()[-1] == "1":  # INSERT 0 1
                        chunks_written += 1
                
                # Update job progress and status
                progress = job.get("progress", {})
                progress.update({
                    "chunks_total": len(chunks),
                    "chunks_done": len(chunks),
                    "chunks_written": chunks_written
                })
                
                await tx.execute("""
                    UPDATE upload_jobs
                    SET progress = $1, status = 'chunks_stored', updated_at = now()
                    WHERE job_id = $2
                """, json.dumps(progress), job_id)
                
                self.logger.info(
                    "Chunking completed",
                    job_id=str(job_id),
                    document_id=str(document_id),
                    chunks_generated=len(chunks),
                    chunks_written=chunks_written,
                    chunks_version=chunks_version
                )
        
        except Exception as e:
            self.logger.error(
                "Chunking failed",
                job_id=str(job_id),
                document_id=str(document_id),
                error=str(e)
            )
            raise
    
    async def _process_embeddings(self, job: Dict[str, Any]):
        """Process embeddings in micro-batches with comprehensive monitoring"""
        job_id = job["job_id"]
        document_id = job["document_id"]
        embed_model = job["embed_model"]
        embed_version = job["embed_version"]
        
        try:
            # Get pending chunks for embedding
            async with self.db.transaction() as tx:
                pending_chunks = await tx.fetch("""
                    SELECT cb.chunk_id, cb.text, cb.chunk_ord
                    FROM document_chunk_buffer cb
                    LEFT JOIN document_vector_buffer vb ON (
                        cb.chunk_id = vb.chunk_id 
                        AND vb.embed_model = $2 
                        AND vb.embed_version = $3
                    )
                    WHERE cb.document_id = $1 AND vb.chunk_id IS NULL
                    ORDER BY cb.chunk_ord
                """, document_id, embed_model, embed_version)
                
                if not pending_chunks:
                    # All chunks already embedded
                    await tx.execute("""
                        UPDATE upload_jobs
                        SET status = 'embeddings_stored', updated_at = now()
                        WHERE job_id = $1
                    """, job_id)
                    
                    self.logger.info(
                        "All embeddings already complete",
                        job_id=str(job_id),
                        document_id=str(document_id)
                    )
                    return
                
                # Update status to in_progress if not already
                if job["status"] == "embedding_queued":
                    await tx.execute("""
                        UPDATE upload_jobs
                        SET status = 'embedding_in_progress', updated_at = now()
                        WHERE job_id = $1
                    """, job_id)
            
            # Process in micro-batches
            batch_size = min(256, len(pending_chunks))  # OpenAI limit
            total_batches = (len(pending_chunks) + batch_size - 1) // batch_size
            
            self.logger.info(
                "Starting embedding processing",
                job_id=str(job_id),
                total_chunks=len(pending_chunks),
                batch_size=batch_size,
                total_batches=total_batches
            )
            
            for batch_num in range(total_batches):
                start_idx = batch_num * batch_size
                end_idx = min(start_idx + batch_size, len(pending_chunks))
                batch = pending_chunks[start_idx:end_idx]
                
                await self._process_embedding_batch(
                    job, batch, batch_num + 1, total_batches
                )
            
            # Mark embedding as complete
            async with self.db.transaction() as tx:
                await tx.execute("""
                    UPDATE upload_jobs
                    SET status = 'embeddings_stored', updated_at = now()
                    WHERE job_id = $1
                """, job_id)
                
                self.logger.info(
                    "Embedding processing completed",
                    job_id=str(job_id),
                    document_id=str(document_id),
                    total_chunks=len(pending_chunks)
                )
        
        except Exception as e:
            self.logger.error(
                "Embedding processing failed",
                job_id=str(job_id),
                document_id=str(document_id),
                error=str(e)
            )
            raise
    
    async def _process_embedding_batch(self, job: Dict[str, Any], batch: list, 
                                     batch_num: int, total_batches: int):
        """Process a single embedding batch with error handling"""
        job_id = job["job_id"]
        document_id = job["document_id"]
        embed_model = job["embed_model"]
        embed_version = job["embed_version"]
        
        batch_texts = [chunk["text"] for chunk in batch]
        batch_ids = [chunk["chunk_id"] for chunk in batch]
        
        try:
            # Generate embeddings via OpenAI
            vectors = await self.openai.create_embeddings(batch_texts)
            
            if len(vectors) != len(batch):
                raise ValueError(f"Expected {len(batch)} vectors, got {len(vectors)}")
            
            # Persist batch to vector buffer
            async with self.db.transaction() as tx:
                for chunk, vector in zip(batch, vectors):
                    vector_sha = self._compute_vector_sha(vector)
                    
                    await tx.execute("""
                        INSERT INTO document_vector_buffer
                        (document_id, chunk_id, embed_model, embed_version, 
                         vector, vector_sha, created_at)
                        VALUES ($1, $2, $3, $4, $5, $6, now())
                        ON CONFLICT (chunk_id, embed_model, embed_version)
                        DO UPDATE SET vector = EXCLUDED.vector, vector_sha = EXCLUDED.vector_sha
                        WHERE document_vector_buffer.vector_sha != EXCLUDED.vector_sha
                    """, document_id, chunk["chunk_id"], embed_model, embed_version,
                        vector, vector_sha)
                
                # Update progress
                progress = job.get("progress", {})
                embeds_done = progress.get("embeds_done", 0) + len(batch)
                progress["embeds_done"] = embeds_done
                
                await tx.execute("""
                    UPDATE upload_jobs
                    SET progress = $1, updated_at = now()
                    WHERE job_id = $2
                """, json.dumps(progress), job_id)
                
                self.logger.info(
                    "Embedding batch processed",
                    job_id=str(job_id),
                    batch_num=batch_num,
                    total_batches=total_batches,
                    batch_size=len(batch),
                    embeds_done=embeds_done
                )
        
        except Exception as e:
            self.logger.error(
                "Embedding batch failed",
                job_id=str(job_id),
                batch_num=batch_num,
                batch_size=len(batch),
                error=str(e)
            )
            raise
    
    # Helper methods for content processing, hashing, etc.
    def _normalize_markdown(self, content: str) -> str:
        """Normalize markdown content for consistent hashing"""
        # Implementation details for markdown normalization
        return content.strip().replace('\r\n', '\n')
    
    def _compute_sha256(self, content: str) -> str:
        """Compute SHA256 hash of content"""
        import hashlib
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    def _compute_vector_sha(self, vector: list) -> str:
        """Compute SHA256 hash of vector for integrity checking"""
        import hashlib
        import base64
        vector_bytes = base64.b64encode(str(vector).encode())
        return hashlib.sha256(vector_bytes).hexdigest()
    
    def _generate_chunk_id(self, document_id: UUID, chunker_name: str, 
                          chunker_version: str, chunk_ord: int) -> UUID:
        """Generate deterministic chunk ID using UUIDv5"""
        from uuid import uuid5, UUID
        namespace = UUID("6c8a1e6e-1f0b-4aa8-9f0a-1a7c2e6f2b42")
        canonical_string = f"{document_id}:{chunker_name}:{chunker_version}:{chunk_ord}"
        return uuid5(namespace, canonical_string.lower())
    
    def _record_processing_success(self, stage: str, duration: float):
        """Record successful processing metrics"""
        if stage not in self.processing_metrics:
            self.processing_metrics[stage] = {"count": 0, "total_duration": 0}
        
        self.processing_metrics[stage]["count"] += 1
        self.processing_metrics[stage]["total_duration"] += duration
    
    def _record_processing_error(self, stage: str, error: str):
        """Record processing error metrics"""
        if stage not in self.error_counts:
            self.error_counts[stage] = {}
        
        error_type = type(error).__name__
        if error_type not in self.error_counts[stage]:
            self.error_counts[stage][error_type] = 0
        
        self.error_counts[stage][error_type] += 1
```

## Comprehensive Testing Framework

### End-to-End Pipeline Test

```python
# backend/tests/e2e/test_complete_pipeline.py
import asyncio
import pytest
import tempfile
import os
from uuid import uuid4

from backend.shared.db.connection import DatabaseManager
from backend.shared.storage.supabase import StorageManager
from backend.workers.base_worker import BaseWorker

class TestCompletePipeline:
    """Comprehensive end-to-end pipeline testing"""
    
    @pytest.fixture
    async def test_environment(self):
        """Set up test environment with clean database state"""
        # Use test database configuration
        config = TestConfig()
        
        db = DatabaseManager(config.database_url)
        storage = StorageManager(config.storage_config)
        worker = BaseWorker(config)
        
        # Clean up any existing test data
        async with db.transaction() as tx:
            await tx.execute("DELETE FROM document_vector_buffer WHERE document_id LIKE 'test-%'")
            await tx.execute("DELETE FROM document_chunk_buffer WHERE document_id LIKE 'test-%'")
            await tx.execute("DELETE FROM upload_jobs WHERE document_id LIKE 'test-%'")
        
        yield {
            "db": db,
            "storage": storage,
            "worker": worker,
            "config": config
        }
        
        # Cleanup after test
        async with db.transaction() as tx:
            await tx.execute("DELETE FROM document_vector_buffer WHERE document_id LIKE 'test-%'")
            await tx.execute("DELETE FROM document_chunk_buffer WHERE document_id LIKE 'test-%'") 
            await tx.execute("DELETE FROM upload_jobs WHERE document_id LIKE 'test-%'")
    
    async def test_complete_document_processing(self, test_environment):
        """Test complete document processing pipeline"""
        env = test_environment
        db = env["db"]
        storage = env["storage"]
        worker = env["worker"]
        
        # Generate test document
        document_id = f"test-{uuid4()}"
        user_id = f"test-user-{uuid4()}"
        job_id = uuid4()
        
        # Create sample PDF content (mock)
        test_pdf_content = b"Mock PDF content for testing"
        raw_path = f"storage://raw/{user_id}/{document_id}.pdf"
        
        # Upload raw document
        await storage.write_blob(raw_path, test_pdf_content)
        
        # Create initial job entry
        async with db.transaction() as tx:
            await tx.execute("""
                INSERT INTO upload_jobs 
                (job_id, user_id, document_id, status, raw_path, chunks_version, 
                 embed_model, embed_version, created_at, updated_at)
                VALUES ($1, $2, $3, 'uploaded', $4, 'markdown-simple@1', 
                        'text-embedding-3-small', '1', now(), now())
            """, job_id, user_id, document_id, raw_path)
        
        # Simulate LlamaParse processing by directly creating parsed content
        parsed_content = f"""# Test Document {document_id}

This is a test document for validating the complete processing pipeline.

## Section 1: Introduction
This section contains introductory content that should be chunked appropriately.

## Section 2: Details  
Additional content to ensure multiple chunks are generated for embedding testing.

## Section 3: Conclusion
Final section to complete the document structure.
"""
        
        parsed_path = f"storage://parsed/{user_id}/{document_id}.md"
        await storage.write_blob(parsed_path, parsed_content.encode())
        
        # Update job to parsed status
        async with db.transaction() as tx:
            await tx.execute("""
                UPDATE upload_jobs
                SET status = 'parsed', parsed_path = $1, updated_at = now()
                WHERE job_id = $2
            """, parsed_path, job_id)
        
        # Process through all stages
        max_iterations = 10
        iteration = 0
        
        while iteration < max_iterations:
            # Get current job status
            async with db.transaction() as tx:
                job = await tx.fetchrow("""
                    SELECT status, progress FROM upload_jobs WHERE job_id = $1
                """, job_id)
            
            if not job:
                raise AssertionError("Job not found")
            
            status = job["status"]
            print(f"Iteration {iteration}: Job status = {status}")
            
            if status == "complete":
                break
            elif status in ["failed_parse", "failed_chunking", "failed_embedding"]:
                raise AssertionError(f"Job failed with status: {status}")
            
            # Process one step
            job_data = await self._get_job_data(db, job_id)
            await worker._process_single_job(job_data)
            
            iteration += 1
            
            # Small delay to allow processing
            await asyncio.sleep(0.1)
        
        if iteration >= max_iterations:
            current_status = await self._get_job_status(db, job_id)
            raise AssertionError(f"Pipeline did not complete within {max_iterations} iterations. Final status: {current_status}")
        
        # Validate final state
        await self._validate_pipeline_completion(db, document_id, job_id)
        
        print(f"✅ Complete pipeline test passed! Document {document_id} processed successfully.")
    
    async def _get_job_data(self, db, job_id):
        """Get complete job data for processing"""
        async with db.transaction() as tx:
            job = await tx.fetchrow("""
                SELECT job_id, user_id, document_id, status, raw_path, parsed_path,
                       parsed_sha256, chunks_version, embed_model, embed_version,
                       progress, retry_count, last_error, created_at, updated_at
                FROM upload_jobs 
                WHERE job_id = $1
            """, job_id)
            
            return dict(job) if job else None
    
    async def _get_job_status(self, db, job_id):
        """Get current job status"""
        async with db.transaction() as tx:
            status = await tx.fetchval("""
                SELECT status FROM upload_jobs WHERE job_id = $1
            """, job_id)
            return status
    
    async def _validate_pipeline_completion(self, db, document_id, job_id):
        """Validate complete pipeline processing"""
        async with db.transaction() as tx:
            # Check job status
            job = await tx.fetchrow("""
                SELECT status, progress FROM upload_jobs WHERE job_id = $1
            """, job_id)
            
            assert job["status"] == "complete", f"Expected complete status, got {job['status']}"
            
            # Check chunks were created
            chunk_count = await tx.fetchval("""
                SELECT COUNT(*) FROM document_chunk_buffer WHERE document_id = $1
            """, document_id)
            
            assert chunk_count > 0, f"No chunks found for document {document_id}"
            
            # Check embeddings were created
            vector_count = await tx.fetchval("""
                SELECT COUNT(*) FROM document_vector_buffer WHERE document_id = $1
            """, document_id)
            
            assert vector_count > 0, f"No vectors found for document {document_id}"
            assert vector_count == chunk_count, f"Vector count ({vector_count}) doesn't match chunk count ({chunk_count})"
            
            # Check progress tracking
            progress = job["progress"]
            assert progress.get("chunks_done", 0) > 0, "No chunks marked as done"
            assert progress.get("embeds_done", 0) > 0, "No embeddings marked as done"
            
            print(f"✅ Pipeline validation passed:")
            print(f"   - Chunks created: {chunk_count}")
            print(f"   - Embeddings created: {vector_count}")
            print(f"   - Progress: {progress}")

class TestConfig:
    """Test configuration for local environment"""
    
    def __init__(self):
        self.database_url = "postgresql://postgres:postgres@localhost:5432/accessa_dev"
        self.storage_config = {
            "url": "http://localhost:5000",
            "service_role_key": "test-service-role-key"
        }
        # Add other config as needed

if __name__ == "__main__":
    # Run standalone for development testing
    async def run_test():
        test = TestCompletePipeline()
        async for env in test.test_environment():
            await test.test_complete_document_processing(env)
            break
    
    asyncio.run(run_test())
```

## Monitoring and Observability

### Real-time Processing Dashboard

```python
# monitoring/dashboard.py
from fastapi import FastAPI, WebSocket
import asyncio
import json
from datetime import datetime, timedelta
import psycopg2
from typing import Dict, List

app = FastAPI()

class ProcessingMonitor:
    """Real-time processing pipeline monitoring"""
    
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.active_connections: List[WebSocket] = []
    
    async def start_monitoring(self):
        """Start continuous monitoring loop"""
        while True:
            try:
                metrics = await self._collect_metrics()
                await self._broadcast_metrics(metrics)
                await asyncio.sleep(5)  # Update every 5 seconds
            except Exception as e:
                print(f"Monitoring error: {e}")
                await asyncio.sleep(10)
    
    async def _collect_metrics(self) -> Dict:
        """Collect comprehensive processing metrics"""
        conn = psycopg2.connect(self.database_url)
        cursor = conn.cursor()
        
        # Job status counts
        cursor.execute("""
            SELECT status, COUNT(*) 
            FROM upload_jobs 
            WHERE created_at > now() - interval '24 hours'
            GROUP BY status
        """)
        status_counts = dict(cursor.fetchall())
        
        # Processing progress
        cursor.execute("""
            SELECT 
                COUNT(*) as total_jobs,
                AVG(EXTRACT(EPOCH FROM (updated_at - created_at))/60) as avg_processing_minutes,
                COUNT(*) FILTER (WHERE status = 'complete') as completed_jobs,
                COUNT(*) FILTER (WHERE status LIKE 'failed_%') as failed_jobs
            FROM upload_jobs 
            WHERE created_at > now() - interval '24 hours'
        """)
        processing_stats = dict(zip(
            ["total_jobs", "avg_processing_minutes", "completed_jobs", "failed_jobs"],
            cursor.fetchone()
        ))
        
        # Buffer table sizes
        cursor.execute("SELECT COUNT(*) FROM document_chunk_buffer")
        chunk_buffer_size = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM document_vector_buffer")  
        vector_buffer_size = cursor.fetchone()[0]
        
        # Recent activity
        cursor.execute("""
            SELECT job_id, document_id, status, updated_at
            FROM upload_jobs
            WHERE updated_at > now() - interval '1 hour'
            ORDER BY updated_at DESC
            LIMIT 10
        """)
        recent_activity = [
            {
                "job_id": str(row[0]),
                "document_id": str(row[1]), 
                "status": row[2],
                "updated_at": row[3].isoformat()
            }
            for row in cursor.fetchall()
        ]
        
        conn.close()
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "status_counts": status_counts,
            "processing_stats": processing_stats,
            "buffer_sizes": {
                "chunks": chunk_buffer_size,
                "vectors": vector_buffer_size
            },
            "recent_activity": recent_activity
        }
    
    async def _broadcast_metrics(self, metrics: Dict):
        """Broadcast metrics to all connected clients"""
        if self.active_connections:
            message = json.dumps(metrics)
            disconnected = []
            
            for connection in self.active_connections:
                try:
                    await connection.send_text(message)
                except:
                    disconnected.append(connection)
            
            # Remove disconnected clients
            for connection in disconnected:
                self.active_connections.remove(connection)

monitor = ProcessingMonitor("postgresql://postgres:postgres@localhost:5432/accessa_dev")

@app.websocket("/ws/metrics")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time metrics"""
    await websocket.accept()
    monitor.active_connections.append(websocket)
    
    try:
        # Send initial metrics
        metrics = await monitor._collect_metrics()
        await websocket.send_text(json.dumps(metrics))
        
        # Keep connection alive
        while True:
            await websocket.receive_text()
    except:
        pass
    finally:
        if websocket in monitor.active_connections:
            monitor.active_connections.remove(websocket)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

@app.on_event("startup")
async def startup_event():
    """Start monitoring on application startup"""
    asyncio.create_task(monitor.start_monitoring())

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3000)
```

## Deployment Verification and Rollback

### Production Deployment Validation

```bash
#!/bin/bash
# scripts/deploy-and-verify.sh

set -e

echo "Starting deployment and verification process..."

# Step 1: Validate local environment passes all tests
echo "Step 1: Validating local environment..."
./scripts/run-local-tests.sh || {
    echo "❌ Local tests failed. Deployment aborted."
    exit 1
}

# Step 2: Deploy infrastructure components
echo "Step 2: Deploying infrastructure..."
./scripts/deploy-infrastructure.sh || {
    echo "❌ Infrastructure deployment failed."
    exit 1
}

# Step 3: Validate infrastructure health
echo "Step 3: Validating infrastructure..."
python infrastructure/validation/deployment_validator.py config/production.yaml || {
    echo "❌ Infrastructure validation failed. Rolling back..."
    ./scripts/rollback-infrastructure.sh
    exit 1
}

# Step 4: Deploy application services
echo "Step 4: Deploying application services..."
./scripts/deploy-application.sh || {
    echo "❌ Application deployment failed. Rolling back..."
    ./scripts/rollback-complete.sh
    exit 1
}

# Step 5: Validate application functionality
echo "Step 5: Validating application functionality..."
python scripts/validate-production-deployment.py || {
    echo "❌ Application validation failed. Rolling back..."
    ./scripts/rollback-complete.sh
    exit 1
}

# Step 6: Run smoke tests
echo "Step 6: Running production smoke tests..."
python scripts/production-smoke-tests.py || {
    echo "❌ Smoke tests failed. Rolling back..."
    ./scripts/rollback-complete.sh
    exit 1
}

echo "✅ Deployment completed successfully!"
echo "🎯 Production system verified and operational."
```

## Risk Mitigation and Rollback Procedures

### Automated Rollback System

```python
# scripts/automated_rollback.py
import asyncio
import subprocess
import sys
from datetime import datetime

class DeploymentRollback:
    """Automated rollback system for failed deployments"""
    
    def __init__(self, deployment_config: dict):
        self.config = deployment_config
        self.rollback_steps = []
    
    async def execute_rollback(self, failure_stage: str):
        """Execute rollback based on failure stage"""
        print(f"🔄 Starting rollback from failure at stage: {failure_stage}")
        
        try:
            if failure_stage in ["infrastructure", "application"]:
                await self._rollback_infrastructure()
            
            if failure_stage == "application":
                await self._rollback_application()
            
            await self._verify_rollback_success()
            
            print("✅ Rollback completed successfully")
            
        except Exception as e:
            print(f"❌ Rollback failed: {e}")
            print("🚨 Manual intervention required!")
            sys.exit(1)
    
    async def _rollback_infrastructure(self):
        """Rollback infrastructure to previous stable state"""
        print("Rolling back infrastructure...")
        
        # Implementation would depend on deployment platform
        # For Render, this might involve API calls to revert services
        
    async def _rollback_application(self):
        """Rollback application to previous version"""
        print("Rolling back application...")
        
        # Revert to previous code version
        # Restore database to backup if needed
        
    async def _verify_rollback_success(self):
        """Verify rollback restored system to working state"""
        print("Verifying rollback success...")
        
        # Run health checks
        # Validate basic functionality
        # Ensure no data corruption
```

## Future Migration Considerations

### Cloud Platform Readiness

The local-first architecture is designed to facilitate future migration to managed cloud platforms:

**Kubernetes Migration Path:**
- Docker compose can be translated directly to Kubernetes manifests
- Local persistent volumes map to cloud storage solutions
- Service discovery patterns remain consistent

**Managed Queue Services:**
- Current database polling can be replaced with SQS/Pub-Sub
- Worker scaling logic remains unchanged
- Message-driven processing replaces polling

**Serverless Migration:**
- BaseWorker stages can become separate cloud functions
- Buffer tables provide durable handoffs between functions
- Local testing validates function logic before deployment

---

## Comprehensive Testing Strategy

### Multi-Environment Testing Framework

The 003 iteration implements a comprehensive testing strategy covering local development, deployed environments, and hybrid scenarios to ensure complete validation before production deployment.

#### Testing Environment Matrix

**Local Development Testing:**
```bash
# Mock services testing (fastest, most reliable)
docker-compose exec api-server python -m pytest backend/tests/unit/ -v --mock-services

# Real API integration testing (slower, requires API keys)
docker-compose exec api-server python -m pytest backend/tests/integration/ -v --real-apis

# Hybrid testing (mock for LlamaParse, real for OpenAI)
docker-compose exec api-server python -m pytest backend/tests/integration/ -v --hybrid-mode
```

**Deployed Environment Testing:**
```bash
# Health check validation on deployed infrastructure
./scripts/validate-deployed-environment.sh https://api.yourdomain.com

# End-to-end processing in deployed environment
python scripts/deployed-e2e-test.py --environment=staging --document-samples=./test-docs/

# Performance benchmarking comparison
python scripts/performance-comparison.py --local-baseline=./benchmarks/local.json --deployed-url=https://api.yourdomain.com
```

**Cross-Environment Validation:**
```bash
# Validate consistency between environments
python scripts/cross-environment-validation.py --local=docker-compose --deployed=https://api.yourdomain.com

# Configuration drift detection
python scripts/config-drift-detection.py --baseline=./config/local.json --target=production
```

#### Mock Service Testing Strategy

**Deterministic Testing:**
```python
# testing/mocks/config.py
MOCK_TESTING_CONFIG = {
    "llamaparse": {
        "deterministic_responses": True,
        "processing_delay": 2.0,  # seconds
        "failure_rate": 0.0,      # 0% for reliable testing
        "response_size_variation": False
    },
    "openai": {
        "deterministic_embeddings": True,
        "rate_limit_simulation": True,
        "cost_tracking": True,
        "batch_optimization": True
    }
}
```

**Failure Injection Testing:**
```python
# Configurable failure injection for resilience testing
FAILURE_INJECTION_CONFIG = {
    "llamaparse": {
        "failure_rate": 0.1,      # 10% failure rate
        "timeout_simulation": True,
        "partial_response_errors": True,
        "webhook_delivery_failures": True
    },
    "openai": {
        "rate_limit_errors": True,
        "service_unavailable": True,
        "embedding_corruption": False,
        "cost_limit_exceeded": True
    }
}
```

#### Real API Integration Testing

**Controlled Real API Testing:**
```python
# backend/tests/integration/test_real_apis.py
class RealAPIIntegrationTest:
    """Carefully controlled real API testing with cost management"""
    
    @pytest.fixture
    def cost_limiter(self):
        """Prevent runaway API costs during testing"""
        return CostLimiter(
            max_llamaparse_calls=5,
            max_openai_tokens=10000,
            max_cost_cents=100  # $1.00 limit
        )
    
    async def test_real_llamaparse_integration(self, cost_limiter):
        """Test with real LlamaParse API using small test documents"""
        test_doc = TestDocuments.get_small_pdf()  # <100KB
        # Limited, controlled real API testing
        
    async def test_real_openai_embedding(self, cost_limiter):
        """Test with real OpenAI API using minimal token usage"""
        test_text = "Short test content for embedding validation"
        # Controlled embedding generation with cost tracking
```

#### Deployed Environment Testing

**Infrastructure Validation:**
```python
# scripts/validate-deployed-environment.py
class DeployedEnvironmentValidator:
    """Validate deployed infrastructure matches local baseline"""
    
    async def validate_deployed_infrastructure(self, deployed_url: str):
        results = {}
        
        # Health check validation
        results["health"] = await self._validate_health_endpoints(deployed_url)
        
        # Database schema validation
        results["database"] = await self._validate_database_schema(deployed_url)
        
        # Configuration consistency validation
        results["config"] = await self._validate_configuration_consistency(deployed_url)
        
        # Performance baseline validation
        results["performance"] = await self._validate_performance_baseline(deployed_url)
        
        return results
```

**End-to-End Deployed Testing:**
```python
# scripts/deployed-e2e-test.py
class DeployedE2ETest:
    """End-to-end testing in deployed environment"""
    
    async def test_complete_pipeline_deployed(self, deployed_url: str):
        """Test complete document processing in deployed environment"""
        
        # Upload test document to deployed environment
        test_doc_path = await self._upload_test_document(deployed_url)
        
        # Monitor processing through all stages
        job_id = await self._submit_processing_job(deployed_url, test_doc_path)
        
        # Validate processing completion within expected timeframes
        result = await self._monitor_job_completion(deployed_url, job_id, timeout=300)
        
        # Verify processed results match local baseline
        await self._validate_processing_results(deployed_url, job_id)
        
        # Clean up test data
        await self._cleanup_test_data(deployed_url, job_id)
```

#### Hybrid Testing Framework

**Environment Transition Testing:**
```python
# backend/tests/hybrid/test_environment_transitions.py
class EnvironmentTransitionTest:
    """Test transitions between mock, real, and deployed environments"""
    
    async def test_mock_to_real_transition(self):
        """Validate consistent behavior when transitioning from mock to real APIs"""
        
        # Process document with mock services
        mock_result = await self._process_with_mock_services(test_document)
        
        # Process same document with real services
        real_result = await self._process_with_real_services(test_document)
        
        # Validate processing consistency
        await self._validate_result_consistency(mock_result, real_result)
    
    async def test_local_to_deployed_consistency(self):
        """Validate consistent behavior between local and deployed environments"""
        
        # Process document in local environment
        local_result = await self._process_locally(test_document)
        
        # Process same document in deployed environment
        deployed_result = await self._process_deployed(test_document)
        
        # Validate deployment consistency
        await self._validate_deployment_consistency(local_result, deployed_result)
```

**A/B Testing Framework:**
```python
# scripts/ab-testing-framework.py
class ABTestingFramework:
    """Compare performance and behavior across environments"""
    
    async def compare_environments(self, test_suite: List[TestDocument]):
        """Run comparative testing across all environments"""
        
        results = {
            "local_mock": [],
            "local_real": [],
            "deployed_real": []
        }
        
        for test_doc in test_suite:
            # Run in all environments
            results["local_mock"].append(await self._test_local_mock(test_doc))
            results["local_real"].append(await self._test_local_real(test_doc))
            results["deployed_real"].append(await self._test_deployed_real(test_doc))
        
        # Generate comparative analysis
        return self._generate_comparison_report(results)
```

#### Test Coverage Requirements

**Comprehensive Coverage Matrix:**
- **Local Mock Services**: 100% state machine coverage, all error scenarios
- **Local Real APIs**: Core functionality validation, cost-controlled testing
- **Deployed Environment**: Infrastructure validation, performance benchmarking
- **Cross-Environment**: Consistency validation, configuration drift detection

**Success Criteria:**
- Local mock tests: >99% success rate, <5 minute execution time
- Local real API tests: >95% success rate, <$5 cost per full test suite
- Deployed environment tests: 100% infrastructure validation, performance within 20% of local baseline
- Cross-environment validation: 100% consistency between environments

---

## Conclusion

This RFC provides a comprehensive technical foundation for implementing the 003 Worker Refactor with a local-first development approach. The architecture prioritizes validation and testing at every stage, ensuring that deployment failures like those experienced in 002 are prevented through comprehensive local validation.

**Key Success Factors:**
1. Complete local environment replicates production architecture
2. Infrastructure validation occurs before application deployment
3. Comprehensive testing covers all state machine transitions
4. Monitoring and observability provide immediate failure detection
5. Automated rollback procedures ensure safe deployment practices

**Implementation Timeline:** 4 weeks with emphasis on local validation
**Success Criteria:** 100% local validation before deployment, >99% production reliability
**Review Required:** Development team, infrastructure team, security team