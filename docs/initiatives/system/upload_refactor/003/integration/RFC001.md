# RFC001: Upload Pipeline + Agent Workflow Integration - Technical Architecture

## Document Context
This RFC defines the technical architecture for integrating the completed 003 Worker Refactor upload pipeline infrastructure with patient navigator agent workflows, building upon the requirements established in PRD001.md.

**Reference Documents**: 
- `PRD001.md` - Product requirements and success criteria
- `CONTEXT001.md` - Integration context and current state analysis

## Overview

The integration architecture eliminates the need for schema bridges by configuring agent workflows to query `upload_pipeline` vectorized chunks directly. This approach leverages the production-ready upload infrastructure as the single source of truth for processed document vectors, enabling immediate RAG semantic search capabilities for agent conversations.

### Core Technical Approach
1. **Direct Vector Access**: Agents query upload_pipeline tables using pgvector for semantic search
2. **Unified Development Environment**: Single Docker stack supporting both upload processing and agent conversations  
3. **Mock Service Coordination**: Shared deterministic mock services for initial development and testing only
4. **Automated Integration Testing**: End-to-end validation from document upload through agent conversations

## Architecture

### System Integration Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                    Integrated System Architecture               │
├─────────────────┬─────────────────┬─────────────────────────────┤
│   003 Upload    │   Integration   │      Agent Workflows        │
│   Pipeline      │   Layer         │                             │
│                 │                 │                             │
│  ┌───────────┐  │  ┌───────────┐  │  ┌─────────────────────────┤
│  │Documents  │  │  │RAG Query  │  │  │Information Retrieval    │
│  │Upload     │──┼─▶│Validation │──┼─▶│Agent                    │
│  │           │  │  │           │  │  │                         │
│  ├───────────┤  │  ├───────────┤  │  ├─────────────────────────┤
│  │LlamaParse │  │  │Health     │  │  │Strategy Workflow        │
│  │Mock       │◄─┼──┤Monitoring │◄─┼──┤Agent                    │
│  │           │  │  │           │  │  │                         │
│  ├───────────┤  │  ├───────────┤  │  ├─────────────────────────┤
│  │OpenAI     │  │  │Performance│  │  │Supervisor Workflow      │
│  │Mock       │◄─┼──┤Tracking   │◄─┼──┤Agent                    │
│  │           │  │  │           │  │  │                         │
│  ├───────────┤  │  └───────────┘  │  └─────────────────────────┤
│  │Vector     │  │                 │                             │
│  │Storage    │──┼─────────────────┼─────────────────────────────▶
│  │(pgvector) │  │                 │         Direct RAG Access   │
│  └───────────┘  │                 │                             │
└─────────────────┴─────────────────┴─────────────────────────────┘
```

### Data Flow Architecture

```
Upload Request → 003 Processing → Vector Storage → Agent RAG Queries → Conversations
     │                │              │                │                │
     ▼                ▼              ▼                ▼                ▼
  Web/API       State Machine    pgvector DB    Semantic Search   User Response
  Local File    Buffer Ops       Final Tables   Similarity Match  Reference Accuracy
  Status Track  Mock Services    Embeddings     Query Execution   Confidence Score
```

### Database Schema Integration

**Upload Pipeline Schema (Single Source of Truth)**:
```sql
-- Core upload processing tables
upload_pipeline.documents (
  document_id UUID PRIMARY KEY,
  user_id UUID NOT NULL,
  filename TEXT NOT NULL,
  document_type TEXT DEFAULT 'policy', -- Added for document availability checking
  file_path TEXT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

upload_pipeline.upload_jobs (
  job_id UUID PRIMARY KEY,
  document_id UUID REFERENCES upload_pipeline.documents(document_id),
  user_id UUID NOT NULL,
  status TEXT NOT NULL CHECK (status IN (
    'uploaded', 'parse_queued', 'parsed', 'parse_validated',
    'chunking', 'chunks_stored', 'embedding_queued', 
    'embedding_in_progress', 'embeddings_stored', 'complete',
    'failed_parse', 'failed_chunking', 'failed_embedding'
  )),
  processing_started_at TIMESTAMPTZ,
  processing_completed_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Final vector storage (Agent RAG Target)
upload_pipeline.document_chunks (
  chunk_id UUID PRIMARY KEY,
  document_id UUID REFERENCES upload_pipeline.documents(document_id),
  user_id UUID NOT NULL,
  chunk_text TEXT NOT NULL,
  chunk_metadata JSONB,
  embedding_vector vector(1536), -- OpenAI text-embedding-3-small
  chunk_index INTEGER,
  created_at TIMESTAMPTZ DEFAULT now()
);

-- Indexes for RAG performance
CREATE INDEX idx_document_chunks_user_id ON upload_pipeline.document_chunks (user_id);
CREATE INDEX idx_document_chunks_document_id ON upload_pipeline.document_chunks (document_id);
CREATE INDEX idx_document_chunks_embedding USING ivfflat (embedding_vector vector_cosine_ops);
```

**Agent RAG Access Pattern**:
```sql
-- Information Retrieval Agent semantic search query
SELECT 
  chunk_id,
  document_id,
  chunk_text,
  chunk_metadata,
  1 - (embedding_vector <=> $1) AS similarity
FROM upload_pipeline.document_chunks 
WHERE user_id = $2
  AND 1 - (embedding_vector <=> $1) > $3  -- similarity_threshold
ORDER BY embedding_vector <=> $1
LIMIT $4;

-- Document availability check for Supervisor Workflow
SELECT DISTINCT document_type, COUNT(*) as document_count
FROM upload_pipeline.documents 
WHERE user_id = $1
  AND document_id IN (
    SELECT document_id 
    FROM upload_pipeline.upload_jobs 
    WHERE status = 'complete'
  )
GROUP BY document_type;
```

## Technical Decisions

### Decision 1: Direct Vector Access (No Schema Bridge)
**Rationale**: The upload pipeline already produces vectorized chunks in the exact format needed for RAG semantic search. Creating bridge views or separate schemas adds unnecessary complexity and potential performance overhead.

**Implementation**:
- Configure agent RAG systems to connect directly to upload_pipeline schema
- Use existing `RAGTool` class with modified database connection pointing to upload_pipeline tables
- Maintain user access control through upload_pipeline schema RLS policies

**Benefits**:
- Eliminates data synchronization complexity
- Reduces latency (no bridge query overhead)
- Single source of truth for processed document vectors
- Simplified development and maintenance

### Decision 2: Unified Mock Service Infrastructure (Development/Testing Only)
**Rationale**: Both upload pipeline and agent systems need consistent, deterministic responses during initial implementation and testing phases. Production will use real LlamaParse and OpenAI APIs.

**Implementation**:
```python
class UnifiedMockServices:
    """Coordinated mock services for development and testing phases only"""
    
    def __init__(self, config: MockServiceConfig):
        self.llamaparse = DeterministicLlamaParseService(config.llamaparse)
        self.openai = DeterministicOpenAIService(config.openai)
        self.shared_state = MockServiceState()
        # NOTE: Production uses real API services
    
    def generate_consistent_content(self, document_id: str) -> str:
        """Ensure same document_id produces same content for testing"""
        seed = hashlib.md5(document_id.encode()).hexdigest()
        return self.llamaparse.generate_content(seed)
    
    def generate_consistent_embeddings(self, text: str) -> List[float]:
        """Ensure same text produces same embeddings for testing"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        np.random.seed(int(text_hash[:8], 16))
        return np.random.normal(0, 1, 1536).tolist()
```

### Decision 3: Docker-Based Integrated Development Environment
**Rationale**: Both systems require complex dependencies (databases, vector extensions, mock services). A unified Docker environment ensures consistent development experience and eliminates environment setup complexity.

**Implementation**:
```yaml
# docker-compose.integration.yml
version: '3.8'
services:
  postgres:
    image: pgvector/pgvector:pg15
    environment:
      POSTGRES_DB: postgres
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - ./sql/upload_pipeline_schema.sql:/docker-entrypoint-initdb.d/01_upload_pipeline.sql
      - ./sql/rls_policies.sql:/docker-entrypoint-initdb.d/02_rls_policies.sql
    ports:
      - "5432:5432"

  upload_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    depends_on:
      - postgres
      - mock_llamaparse
      - mock_openai
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/postgres
      LLAMAPARSE_API_URL: http://mock_llamaparse:8001  # Development/testing only
      OPENAI_API_URL: http://mock_openai:8002          # Development/testing only

  mock_llamaparse:
    build:
      context: ./backend/mocks
      dockerfile: Dockerfile.llamaparse
    ports:
      - "8001:8001"
    # NOTE: Mock service for development/testing only - production uses real LlamaParse API

  mock_openai:
    build:
      context: ./backend/mocks  
      dockerfile: Dockerfile.openai
    ports:
      - "8002:8002"
    # NOTE: Mock service for development/testing only - production uses real OpenAI API

  agent_api:
    build:
      context: ./agents
      dockerfile: Dockerfile.api
    depends_on:
      - postgres
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/postgres
      OPENAI_API_URL: http://mock_openai:8002  # Development/testing only
    ports:
      - "8003:8003"
```

### Decision 4: RAG Integration Service Pattern
**Rationale**: Need a dedicated service to validate upload completion translates to agent RAG readiness, providing integration health monitoring and troubleshooting capabilities.

**Implementation**:
```python
class UploadRAGIntegration:
    """Validates upload pipeline vectors are ready for agent RAG queries"""
    
    def __init__(self, db_config: DatabaseConfig):
        self.upload_db = AsyncDatabase(db_config.upload_schema_url)
        self.logger = StructuredLogger("rag_integration")
        self.health_monitor = IntegrationHealthMonitor()
    
    async def validate_documents_rag_ready(self) -> List[DocumentRAGStatus]:
        """Verify completed documents have vectors ready for semantic search"""
        async with self.upload_db.acquire() as conn:
            # Check for completed jobs with corresponding vectors
            result = await conn.fetch("""
                SELECT 
                  d.document_id,
                  d.filename,
                  d.user_id,
                  uj.status,
                  COUNT(dc.chunk_id) as chunk_count,
                  AVG(vector_norm(dc.embedding_vector)) as avg_vector_norm
                FROM upload_pipeline.documents d
                JOIN upload_pipeline.upload_jobs uj ON d.document_id = uj.document_id
                LEFT JOIN upload_pipeline.document_chunks dc ON d.document_id = dc.document_id
                WHERE uj.status = 'complete'
                GROUP BY d.document_id, d.filename, d.user_id, uj.status
                HAVING COUNT(dc.chunk_id) > 0
            """)
            
            return [DocumentRAGStatus(
                document_id=row['document_id'],
                filename=row['filename'], 
                user_id=row['user_id'],
                is_rag_ready=row['chunk_count'] > 0 and row['avg_vector_norm'] > 0,
                chunk_count=row['chunk_count'],
                vector_quality_score=row['avg_vector_norm']
            ) for row in result]
    
    async def test_sample_rag_query(self, document_id: str, user_id: str) -> RAGQueryTestResult:
        """Execute sample semantic search to validate RAG functionality"""
        test_query_vector = await self._generate_test_query_vector(document_id)
        
        async with self.upload_db.acquire() as conn:
            result = await conn.fetch("""
                SELECT 
                  chunk_id,
                  chunk_text,
                  1 - (embedding_vector <=> $1) AS similarity
                FROM upload_pipeline.document_chunks 
                WHERE document_id = $2 AND user_id = $3
                ORDER BY embedding_vector <=> $1
                LIMIT 3
            """, test_query_vector, document_id, user_id)
            
            return RAGQueryTestResult(
                document_id=document_id,
                query_successful=len(result) > 0,
                top_similarity=result[0]['similarity'] if result else 0,
                chunks_found=len(result),
                response_time=time.time() - start_time
            )
```

## Alternative Approaches

### Alternative 1: Schema Bridge with Views (Rejected)
**Approach**: Create database views mapping upload_pipeline tables to agent-expected schema format.
**Why Rejected**: 
- Adds query overhead and complexity
- Creates maintenance burden for view synchronization  
- Upload pipeline already produces correct vector format
- Introduces potential data consistency issues

### Alternative 2: Message Queue Integration (Rejected)
**Approach**: Use message queues to notify agents when documents complete processing.
**Why Rejected**:
- Over-engineering for integration of two existing systems
- Adds infrastructure complexity for development environment
- Direct database polling simpler and more reliable for current scale
- Upload completion already tracked in database status

### Alternative 3: Separate Agent Vector Store (Rejected)  
**Approach**: Copy vectors from upload pipeline to separate agent-specific database.
**Why Rejected**:
- Creates data duplication and synchronization complexity
- Increases storage requirements and operational overhead
- Upload pipeline vectors already optimized for RAG queries
- Violates single source of truth principle

## Implementation Plan

### Phase 1: Mock Integration Setup & Testing (Week 1)

**1.1 Mock Environment Setup**
```bash
# Mock environment setup
./scripts/setup-mock-integration-environment.sh:
  - Launch Docker Compose stack with mock services
  - Initialize upload_pipeline schema with vector indexes
  - Configure mock LlamaParse and OpenAI services
  - Configure agent RAG connections to upload_pipeline
  - Seed test data (users, documents)
```

**1.2 Agent RAG Configuration for Mock Testing**
```python
# RAG configuration pointing to upload_pipeline with mock services
class UploadPipelineRAGConfig:
    def __init__(self):
        self.database_url = "postgresql://postgres:postgres@postgres:5432/postgres"
        self.schema_name = "upload_pipeline"
        self.chunks_table = "document_chunks"
        self.similarity_threshold = 0.7
        self.max_chunks = 10

# RAGTool with mock OpenAI service
rag_tool = RAGTool(
    user_id=user_id,
    config=UploadPipelineRAGConfig(),
    embedding_service=mock_openai_service  # Mock for testing
)
```

**1.3 Cyclical Mock Testing Development**
- Upload test document through 003 pipeline (using mock LlamaParse)
- Debug issues, fix problems, repeat until processing completes
- Verify document vectors exist in upload_pipeline.document_chunks (mock embeddings)
- Execute agent RAG query, debug issues, fix, repeat until working
- Continue debug → fix → test cycle until complete end-to-end flow passes with mocks

### Phase 2: Development Environment Testing with Real APIs (Week 2)

**2.1 Real API Environment Setup**
```bash
# Development environment with real APIs
./scripts/setup-dev-integration-environment.sh:
  - Launch Docker Compose stack configured for real APIs
  - Configure real LlamaParse API credentials
  - Configure real OpenAI API credentials  
  - Update agent configurations for real API endpoints
  - Validate API connectivity and authentication
```

**2.2 Cyclical Development Testing with Real APIs**
```python
class RealAPIIntegrationTesting:
    """Integration testing with real external services"""
    
    async def test_upload_to_conversation_with_real_apis(self):
        """Test complete pipeline with real LlamaParse and OpenAI"""
        # Upload document using real LlamaParse
        document_id = await self.upload_test_document("policy_sample.pdf")
        
        # Debug and fix issues as they arise with real API responses
        # Wait for processing completion (real parsing takes longer)
        await self.wait_for_processing_completion(document_id, timeout=300)
        
        # Execute agent conversation with real OpenAI embeddings
        conversation = await self.information_retrieval_agent.process({
            "user_id": self.test_user_id,
            "query": "What is the deductible for this policy?"
        })
        
        # Validate real API results
        assert conversation.document_references
        assert len(conversation.document_references) > 0
    
    async def cyclical_debug_fix_test(self):
        """Debug → Fix → Test cycle until all tests pass"""
        while not await self.all_integration_tests_pass():
            # Run tests, identify failures
            failures = await self.run_integration_tests()
            
            # Debug issues (API rate limits, response formats, timing)
            await self.debug_failures(failures)
            
            # Fix identified issues
            await self.apply_fixes()
            
            # Repeat until all tests pass
```

**2.3 Real API Integration Challenges**
- Handle real API rate limits and timing differences
- Debug actual LlamaParse response formats and webhook timing
- Address real OpenAI embedding generation and costs
- Test error scenarios with real external service failures
- Continue debug → fix → test cycle until stable

### Phase 3: Documentation & Handoff (Week 3)

**3.1 Integration Documentation**
```markdown
# Integration Setup Guide
## Mock Environment Setup (for development/testing)
- Docker Compose stack with mock services
- Agent configuration for mock testing
- Troubleshooting mock service issues

## Development Environment Setup (with real APIs)
- Real API credential configuration
- Agent configuration for real APIs
- Common issues with real API integration
```

**3.2 Operational Procedures**
```markdown
# Development Workflow
## Mock Testing Phase
1. Set up mock environment
2. Debug → Fix → Test cycle with mock services
3. Ensure end-to-end flow works with mocks

## Real API Testing Phase  
1. Configure real API credentials
2. Debug → Fix → Test cycle with real services
3. Handle real API timing, rate limits, costs
4. Ensure integration works with actual external services
```

**3.3 Team Handoff**
- Complete integration setup procedures
- Troubleshooting guide for common integration issues
- Knowledge transfer for ongoing development work
- Clear documentation of mock vs real API configurations

## Testing Strategy

### Unit Testing
```python
# Upload RAG Integration service tests
class TestUploadRAGIntegration:
    async def test_validate_documents_rag_ready(self):
        """Test document vector validation"""
        # Setup: Create completed upload job with vectors
        # Execute: Validate RAG readiness
        # Assert: Correct document identified as RAG-ready
    
    async def test_sample_rag_query_performance(self):
        """Test RAG query execution and performance"""
        # Setup: Upload pipeline with test vectors
        # Execute: Sample semantic search query
        # Assert: Results returned within performance targets
```

### Integration Testing  
```python
# End-to-end integration test scenarios
class TestEndToEndIntegration:
    async def test_information_retrieval_workflow(self):
        """Test information retrieval agent with processed documents"""
        # Upload → Process → RAG Query → Agent Response → Validation
    
    async def test_supervisor_workflow_integration(self):
        """Test supervisor workflow orchestration"""
        # Document availability check → Workflow prescription → Agent execution
    
    async def test_strategy_workflow_integration(self):
        """Test strategy workflow with RAG + web search"""
        # RAG query for context → Web search → Strategy generation → RAG validation
```

### Performance Testing
- Load testing with concurrent upload processing and agent queries
- RAG query performance benchmarking on upload_pipeline vectors
- Memory and CPU utilization monitoring under integrated load
- Database connection pool efficiency testing

## Performance Considerations

### Database Optimization
```sql
-- pgvector index tuning for semantic search workload
SET ivfflat.probes = 10;  -- Adjust based on recall requirements

-- Connection pooling configuration for integrated load
-- Recommended: 20 connections for upload workers + 10 for agent queries
```

### Future Performance Considerations
```python
# NOTE: Caching removed for initial implementation
# Future consideration: RAG query result caching for frequently accessed documents
# class RAGQueryCache:
#     def __init__(self, redis_client):
#         self.cache = redis_client
#         self.cache_ttl = 3600  # 1 hour
#     
#     async def get_cached_results(self, user_id: str, query_hash: str):
#         cache_key = f"rag:{user_id}:{query_hash}"
#         return await self.cache.get(cache_key)
#     
#     async def cache_results(self, user_id: str, query_hash: str, results: List[RAGResult]):
#         cache_key = f"rag:{user_id}:{query_hash}"
#         await self.cache.setex(cache_key, self.cache_ttl, json.dumps(results))
```

### Resource Monitoring
- Database connection usage tracking
- Vector index performance monitoring  
- Memory utilization for concurrent operations
- Integration-specific error rate tracking

## Security Considerations

### Access Control
```sql
-- Row Level Security for upload_pipeline schema
ALTER TABLE upload_pipeline.documents ENABLE ROW LEVEL SECURITY;
ALTER TABLE upload_pipeline.document_chunks ENABLE ROW LEVEL SECURITY;

-- Policy ensuring users only access their own documents
CREATE POLICY user_document_access ON upload_pipeline.documents
  FOR ALL TO authenticated
  USING (user_id = auth.uid());

CREATE POLICY user_chunk_access ON upload_pipeline.document_chunks  
  FOR ALL TO authenticated
  USING (user_id = auth.uid());
```

### Data Protection
- Ensure mock services don't log or persist sensitive test data
- Validate user access controls work correctly for RAG queries
- Audit logging for integration-specific operations
- Secure handling of embedding vectors and document content

## Deployment Strategy

### Development Environment Deployment
1. **Infrastructure Setup**: Deploy unified Docker Compose stack
2. **Database Migration**: Apply upload_pipeline schema with vector indexes
3. **Service Configuration**: Configure agents for upload_pipeline access
4. **Integration Validation**: Execute comprehensive integration test suite
5. **Performance Baseline**: Establish performance metrics and monitoring

### Rollout Approach
- **Week 1**: Development environment setup and basic integration validation
- **Week 2**: Comprehensive testing and performance optimization  
- **Week 3**: Production readiness and operational procedure establishment
- **Future**: Production deployment and customer-facing feature development

## Risk Mitigation

### Technical Risk Mitigation
- **Vector Access Issues**: Comprehensive RAG query testing against upload_pipeline
- **Performance Degradation**: Benchmarking at each phase with rollback procedures
- **Development Complexity**: Automated environment setup with health validation
- **Data Synchronization**: Event-driven monitoring with retry mechanisms

### Operational Risk Mitigation  
- **Integration Monitoring**: Health checks across system boundaries
- **Incident Response**: Clear escalation procedures for upload vs agent issues
- **Recovery Procedures**: Documented rollback for both systems
- **Performance Monitoring**: Resource utilization tracking and alerting

## Next Steps

This RFC provides the technical foundation for integration implementation. The following document will break down specific implementation tasks:

1. **TODO001.md** - Detailed implementation tasks organized by phase with specific deliverables, validation criteria, and handoff procedures
2. **Technical Review** - Engineering team review of direct vector access approach and performance expectations
3. **Integration Environment Setup** - Initial Docker Compose stack deployment and validation
4. **Baseline Performance Testing** - Establish performance benchmarks for optimization efforts

**Critical Technical Decisions Requiring Validation**:
- pgvector index configuration for expected RAG query patterns
- Database connection pooling strategy for concurrent upload + agent load
- Mock service response consistency across integration scenarios

---

**Document Version**: RFC001  
**Created**: 2025-01-18  
**Status**: Draft - Pending Technical Review  
**Next Document**: TODO001.md - Implementation Task Breakdown