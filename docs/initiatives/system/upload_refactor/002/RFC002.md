# RFC002: Worker Refactor - Single BaseWorker Technical Architecture

## Context & Overview

This RFC defines the technical architecture for the 002 Worker Refactor iteration, implementing a unified BaseWorker design that replaces specialized workers with buffer-driven pipeline orchestration. The design prioritizes idempotency, resilience, and horizontal scalability while maintaining clear separation between API and worker concerns.

**Migration Context:** This architecture builds upon learnings from the 001 implementation, addressing complexity issues in specialized worker coordination and providing a foundation for future cloud platform migration.

## Architecture Overview

### High-Level System Design

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Frontend      │    │   FastAPI       │    │   BaseWorker    │
│   (Next.js)     │◄──►│   (API + Hook)  │◄──►│   (Unified)     │
│                 │    │                 │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         │                       │                       │
         ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Supabase      │    │   Buffer Tables │    │   External      │
│   Storage       │    │   (Postgres)    │    │   Services      │
│   (Blobs)       │    │                 │    │   (Parse+Embed) │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### Component Architecture Changes

**From 001 (Specialized Workers):**
```
API Server ─┐
            ├─ ParseWorker
            ├─ ChunkWorker  
            ├─ EmbedWorker
            └─ JobQueue
```

**To 002 (Unified Worker):**
```
API Server ──┬─ Webhook Handler
             └─ Job Status API
             
BaseWorker ───┬─ State Machine
              ├─ Parse Handler
              ├─ Chunk Processor
              ├─ Embed Batcher
              └─ Buffer Manager
```

## Data Architecture

### Enhanced Schema Design

**upload_jobs (Enhanced State Management)**
```sql
CREATE TABLE upload_jobs (
    job_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL,
    document_id UUID NOT NULL,
    status TEXT NOT NULL CHECK (status IN (
        'uploaded', 'parse_queued', 'parsed', 'parse_validated',
        'chunking', 'chunks_stored', 'embedding_queued', 
        'embedding_in_progress', 'embeddings_stored', 'complete',
        'failed_parse', 'failed_chunking', 'failed_embedding'
    )),
    raw_path TEXT NOT NULL,
    parsed_path TEXT,
    parsed_sha256 TEXT,
    chunks_version TEXT NOT NULL DEFAULT 'markdown-simple@1',
    embed_model TEXT DEFAULT 'text-embedding-3-small',
    embed_version TEXT DEFAULT '1',
    progress JSONB DEFAULT '{}',
    retry_count INT DEFAULT 0,
    last_error JSONB,
    webhook_secret TEXT,
    created_at TIMESTAMPTZ DEFAULT now(),
    updated_at TIMESTAMPTZ DEFAULT now()
);

-- Efficient worker polling
CREATE INDEX idx_upload_jobs_status_created ON upload_jobs (status, created_at) 
WHERE status NOT IN ('complete', 'failed_parse', 'failed_chunking', 'failed_embedding');

-- Document lookups
CREATE INDEX idx_upload_jobs_document ON upload_jobs (document_id);
CREATE INDEX idx_upload_jobs_user ON upload_jobs (user_id);
```

**document_chunk_buffer (Staging Layer)**
```sql
CREATE TABLE document_chunk_buffer (
    chunk_id UUID PRIMARY KEY,  -- UUIDv5 deterministic
    document_id UUID NOT NULL,
    chunk_ord INT NOT NULL,
    chunker_name TEXT NOT NULL,
    chunker_version TEXT NOT NULL,
    chunk_sha TEXT NOT NULL,     -- Content integrity
    text TEXT NOT NULL,
    meta JSONB,                  -- {page, section, headings, offsets}
    created_at TIMESTAMPTZ DEFAULT now(),
    
    UNIQUE (document_id, chunker_name, chunker_version, chunk_ord)
);

CREATE INDEX idx_chunk_buffer_document ON document_chunk_buffer (document_id);
CREATE INDEX idx_chunk_buffer_for_embedding ON document_chunk_buffer (document_id, chunker_name, chunker_version);
```

**document_vector_buffer (Embedding Staging)**
```sql
CREATE TABLE document_vector_buffer (
    buffer_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL,
    chunk_id UUID NOT NULL REFERENCES document_chunk_buffer(chunk_id),
    embed_model TEXT NOT NULL,
    embed_version TEXT NOT NULL,
    vector VECTOR(1536) NOT NULL,
    vector_sha TEXT NOT NULL,
    batch_id UUID,               -- Track micro-batch operations
    created_at TIMESTAMPTZ DEFAULT now(),
    
    UNIQUE (chunk_id, embed_model, embed_version)
);

CREATE INDEX idx_vector_buffer_document ON document_vector_buffer (document_id);
CREATE INDEX idx_vector_buffer_batch ON document_vector_buffer (batch_id);
CREATE INDEX idx_vector_buffer_pending ON document_vector_buffer (document_id, embed_model, embed_version);
```

## BaseWorker State Machine

### Core Processing Logic

```python
class BaseWorker:
    """Unified worker for all document processing stages"""
    
    def __init__(self, config):
        self.db = DatabaseManager(config.database_url)
        self.storage = StorageManager(config.storage_config)
        self.llamaparse = LlamaParseClient(config.llamaparse_config)
        self.openai = OpenAIClient(config.openai_config)
        self.rate_limiter = RateLimiter(config.rate_limits)
    
    async def process_job(self, job_id: UUID):
        """Main processing entry point"""
        async with self.db.transaction() as tx:
            job = await self.get_job_for_update(tx, job_id)
            
            try:
                if job.status == "parsed":
                    await self._validate_parsed(tx, job)
                elif job.status == "parse_validated":
                    await self._process_chunks(tx, job)
                elif job.status == "chunks_stored":
                    await self._queue_embeddings(tx, job)
                elif job.status in ["embedding_queued", "embedding_in_progress"]:
                    await self._process_embeddings(tx, job)
                elif job.status == "embeddings_stored":
                    await self._finalize_job(tx, job)
                    
            except RetryableError as e:
                await self._schedule_retry(tx, job, e)
            except PermanentError as e:
                await self._mark_failed(tx, job, e)
```

### Stage Processing Implementations

**Parse Validation:**
```python
async def _validate_parsed(self, tx, job):
    """Validate parsed content and handle deduplication"""
    parsed_content = await self.storage.read_blob(job.parsed_path)
    content_sha = sha256(self._normalize_markdown(parsed_content))
    
    # Check for duplicate parsed content
    existing = await self._find_duplicate_parsed(tx, content_sha)
    if existing:
        job.parsed_path = existing.parsed_path
        job.parsed_sha256 = content_sha
    else:
        job.parsed_sha256 = content_sha
    
    await self._update_job_status(tx, job, "parse_validated")
```

**Chunking Process:**
```python
async def _process_chunks(self, tx, job):
    """Generate deterministic chunks with buffer persistence"""
    parsed_content = await self.storage.read_blob(job.parsed_path)
    chunks = self._generate_chunks(parsed_content, job.chunks_version)
    
    # Idempotent buffer writes
    for chunk in chunks:
        chunk_id = self._generate_chunk_id(job.document_id, chunk)
        await tx.execute("""
            INSERT INTO document_chunk_buffer 
            (chunk_id, document_id, chunk_ord, chunker_name, chunker_version, 
             chunk_sha, text, meta)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (chunk_id) DO NOTHING
        """, chunk_id, job.document_id, chunk.ord, chunk.chunker_name,
            chunk.chunker_version, chunk.sha, chunk.text, chunk.meta)
    
    # Update progress and status
    job.progress['chunks_total'] = len(chunks)
    job.progress['chunks_done'] = len(chunks)
    await self._update_job_status(tx, job, "chunks_stored")
```

**Micro-Batch Embedding:**
```python
async def _process_embeddings(self, tx, job):
    """Process embeddings in micro-batches with immediate persistence"""
    pending_chunks = await self._get_pending_chunks(tx, job)
    
    if not pending_chunks:
        await self._update_job_status(tx, job, "embeddings_stored")
        return
    
    # Update to in_progress if not already
    if job.status == "embedding_queued":
        await self._update_job_status(tx, job, "embedding_in_progress")
    
    # Process in micro-batches
    batch_size = min(256, len(pending_chunks))
    batch = pending_chunks[:batch_size]
    batch_id = uuid4()
    
    # Rate-limited OpenAI call
    await self.rate_limiter.acquire()
    vectors = await self.openai.create_embeddings([chunk.text for chunk in batch])
    
    # Immediate persistence per batch
    for chunk, vector in zip(batch, vectors):
        vector_sha = sha256(base64.b64encode(vector.bytes)).hexdigest()
        await tx.execute("""
            INSERT INTO document_vector_buffer 
            (document_id, chunk_id, embed_model, embed_version, vector, vector_sha, batch_id)
            VALUES ($1, $2, $3, $4, $5, $6, $7)
            ON CONFLICT (chunk_id, embed_model, embed_version) 
            DO UPDATE SET vector = EXCLUDED.vector, vector_sha = EXCLUDED.vector_sha
            WHERE document_vector_buffer.vector_sha != EXCLUDED.vector_sha
        """, job.document_id, chunk.chunk_id, job.embed_model, 
            job.embed_version, vector, vector_sha, batch_id)
    
    # Update progress
    job.progress['embeds_done'] = job.progress.get('embeds_done', 0) + len(batch)
    await self._update_job_progress(tx, job)
```

## Webhook Implementation

### LlamaParse Webhook Handler

```python
@app.post("/webhooks/llamaparse")
async def handle_llamaparse_webhook(request: LlamaParseWebhookRequest):
    """Secure webhook handler for LlamaParse callbacks"""
    
    # Verify HMAC signature
    if not verify_webhook_signature(request, settings.llamaparse_webhook_secret):
        raise HTTPException(status_code=401, detail="Invalid signature")
    
    async with database.transaction() as tx:
        job = await get_job(tx, request.job_id)
        
        if job.status != "parse_queued":
            # Idempotent - already processed
            return {"status": "ok", "message": "Already processed"}
        
        # Process artifacts
        for artifact in request.artifacts:
            if artifact.type == "markdown":
                # Write to blob storage
                parsed_path = f"storage://{job.user_id}/parsed/{job.document_id}.md"
                await storage.write_blob(parsed_path, artifact.content)
                
                # Update job atomically
                await tx.execute("""
                    UPDATE upload_jobs 
                    SET parsed_path = $1, parsed_sha256 = $2, status = 'parsed', 
                        updated_at = now()
                    WHERE job_id = $3 AND status = 'parse_queued'
                """, parsed_path, artifact.sha256, request.job_id)
                
                # Log success event
                await log_event(tx, job.job_id, "PARSE_WEBHOOK_SUCCESS", 
                              {"artifact_size": artifact.bytes})
                
                return {"status": "ok", "message": "Processed successfully"}
    
    raise HTTPException(status_code=400, detail="No processable artifacts")
```

### Webhook Security

**HMAC Verification:**
```python
def verify_webhook_signature(request: LlamaParseWebhookRequest, secret: str) -> bool:
    """Verify HMAC-SHA256 signature from LlamaParse"""
    payload = json.dumps(request.dict(), sort_keys=True, separators=(',', ':'))
    expected_signature = hmac.new(
        secret.encode(), 
        payload.encode(), 
        hashlib.sha256
    ).hexdigest()
    
    received_signature = request.headers.get('X-LlamaParse-Signature', '')
    return hmac.compare_digest(expected_signature, received_signature)
```

**Replay Protection:**
```python
async def check_webhook_replay(tx, webhook_id: str, timestamp: int) -> bool:
    """Prevent replay attacks using timestamp and nonce tracking"""
    current_time = int(time.time())
    if abs(current_time - timestamp) > 300:  # 5 minute window
        return False
    
    # Check if webhook_id already processed
    existing = await tx.fetchval(
        "SELECT 1 FROM webhook_log WHERE webhook_id = $1", webhook_id
    )
    if existing:
        return False
    
    # Record webhook processing
    await tx.execute(
        "INSERT INTO webhook_log (webhook_id, processed_at) VALUES ($1, now())",
        webhook_id
    )
    return True
```

## External Service Integration

### LlamaParse Integration Pattern

```python
class LlamaParseClient:
    """LlamaParse API client with webhook support"""
    
    async def submit_parse_job(self, job_id: UUID, document_path: str) -> str:
        """Submit document for parsing with webhook callback"""
        webhook_url = f"{settings.api_base_url}/webhooks/llamaparse"
        
        # Generate signed upload URLs for LlamaParse
        signed_url = await self.storage.generate_signed_url(document_path, ttl=3600)
        
        response = await self.client.post("/parse", json={
            "job_id": str(job_id),
            "source_url": signed_url,
            "webhook_url": webhook_url,
            "output_format": "markdown",
            "webhook_secret": self._generate_webhook_secret(job_id)
        })
        
        return response.json()["parse_job_id"]
    
    def _generate_webhook_secret(self, job_id: UUID) -> str:
        """Generate per-job webhook secret for verification"""
        return hmac.new(
            settings.master_webhook_secret.encode(),
            str(job_id).encode(),
            hashlib.sha256
        ).hexdigest()
```

### OpenAI Batch Processing

```python
class OpenAIEmbeddingClient:
    """OpenAI client optimized for micro-batch processing"""
    
    def __init__(self, config):
        self.client = openai.AsyncOpenAI(api_key=config.api_key)
        self.rate_limiter = TokenBucketRateLimiter(
            requests_per_minute=config.rpm_limit,
            tokens_per_minute=config.tpm_limit
        )
    
    async def create_embeddings(self, texts: List[str]) -> List[Vector]:
        """Create embeddings with rate limiting and error handling"""
        if len(texts) > 256:
            raise ValueError("Batch size exceeds OpenAI limit of 256")
        
        # Estimate token usage for rate limiting
        estimated_tokens = sum(len(text.split()) * 1.3 for text in texts)
        await self.rate_limiter.acquire(requests=1, tokens=estimated_tokens)
        
        try:
            response = await self.client.embeddings.create(
                model="text-embedding-3-small",
                input=texts,
                encoding_format="float"
            )
            
            return [Vector(data.embedding) for data in response.data]
            
        except openai.RateLimitError as e:
            # Exponential backoff for rate limits
            backoff_time = self._calculate_backoff(e)
            await asyncio.sleep(backoff_time)
            raise RetryableError(f"Rate limited, retry after {backoff_time}s")
            
        except openai.APIError as e:
            if e.status_code >= 500:
                raise RetryableError(f"OpenAI server error: {e}")
            else:
                raise PermanentError(f"OpenAI client error: {e}")
```

## Buffer Management Strategy

### Idempotency Implementation

**Deterministic Key Generation:**
```python
class DeterministicKeyGenerator:
    """Generate deterministic UUIDs for idempotent operations"""
    
    NAMESPACE_UUID = UUID("6c8a1e6e-1f0b-4aa8-9f0a-1a7c2e6f2b42")
    
    @classmethod
    def chunk_id(cls, document_id: UUID, chunker_name: str, 
                 chunker_version: str, chunk_ord: int) -> UUID:
        """Generate deterministic chunk ID"""
        canonical_string = f"{document_id}:{chunker_name}:{chunker_version}:{chunk_ord}"
        return uuid5(cls.NAMESPACE_UUID, canonical_string.lower())
    
    @classmethod  
    def parse_id(cls, document_id: UUID, parser_name: str, 
                 parser_version: str) -> UUID:
        """Generate deterministic parse ID"""
        canonical_string = f"{document_id}:{parser_name}:{parser_version}"
        return uuid5(cls.NAMESPACE_UUID, canonical_string.lower())
```

**Buffer Cleanup Strategy:**
```python
async def cleanup_completed_buffers(self, document_id: UUID):
    """Clean up buffer tables after successful processing"""
    async with self.db.transaction() as tx:
        # Verify job is complete
        job_status = await tx.fetchval(
            "SELECT status FROM upload_jobs WHERE document_id = $1", 
            document_id
        )
        
        if job_status == "complete":
            # Archive to cold storage if needed
            await self._archive_buffers(tx, document_id)
            
            # Clean up staging tables
            await tx.execute(
                "DELETE FROM document_vector_buffer WHERE document_id = $1", 
                document_id
            )
            await tx.execute(
                "DELETE FROM document_chunk_buffer WHERE document_id = $1", 
                document_id
            )
```

### Buffer-to-Production Promotion

**Optional Materialization:**
```python
async def promote_buffers_to_production(self, document_id: UUID):
    """Promote buffer contents to production tables"""
    async with self.db.transaction() as tx:
        # Promote chunks
        await tx.execute("""
            INSERT INTO document_chunks 
            (chunk_id, document_id, chunk_ord, chunker_name, chunker_version, 
             chunk_sha, text, meta)
            SELECT chunk_id, document_id, chunk_ord, chunker_name, chunker_version,
                   chunk_sha, text, meta
            FROM document_chunk_buffer 
            WHERE document_id = $1
            ON CONFLICT (chunk_id) DO NOTHING
        """, document_id)
        
        # Promote embeddings
        await tx.execute("""
            UPDATE document_chunks dc
            SET embedding = dvb.vector, 
                embed_model = dvb.embed_model,
                embed_version = dvb.embed_version,
                embed_updated_at = now()
            FROM document_vector_buffer dvb
            WHERE dc.chunk_id = dvb.chunk_id 
            AND dvb.document_id = $1
        """, document_id)
```

## Error Handling & Recovery

### Error Classification System

```python
class ProcessingError(Exception):
    """Base exception for processing errors"""
    pass

class RetryableError(ProcessingError):
    """Errors that should be retried with backoff"""
    def __init__(self, message: str, retry_after: Optional[int] = None):
        super().__init__(message)
        self.retry_after = retry_after

class PermanentError(ProcessingError):
    """Errors that should not be retried"""
    pass

class RateLimitError(RetryableError):
    """Rate limiting specific error with calculated backoff"""
    pass
```

### Retry Logic Implementation

```python
async def _schedule_retry(self, tx, job, error: RetryableError):
    """Schedule job retry with exponential backoff"""
    job.retry_count += 1
    
    if job.retry_count > settings.max_retries:
        await self._mark_failed(tx, job, PermanentError("Max retries exceeded"))
        return
    
    # Exponential backoff: 2^n * 3 seconds
    backoff_seconds = min(300, (2 ** job.retry_count) * 3)
    retry_at = datetime.utcnow() + timedelta(seconds=backoff_seconds)
    
    job.last_error = {
        "error": str(error),
        "retry_count": job.retry_count,
        "retry_at": retry_at.isoformat(),
        "error_type": error.__class__.__name__
    }
    
    await tx.execute("""
        UPDATE upload_jobs 
        SET retry_count = $1, last_error = $2, updated_at = now()
        WHERE job_id = $3
    """, job.retry_count, job.last_error, job.job_id)
    
    # Schedule retry (implementation depends on job scheduler)
    await self.scheduler.schedule_job(job.job_id, retry_at)
```

## Performance Optimization

### Database Query Optimization

**Efficient Worker Polling:**
```sql
-- Optimized job dequeue query
WITH next_job AS (
    SELECT job_id
    FROM upload_jobs
    WHERE status IN ('parsed', 'parse_validated', 'chunks_stored', 
                     'embedding_queued', 'embedding_in_progress')
    AND (last_error->>'retry_at')::timestamp IS NULL 
        OR (last_error->>'retry_at')::timestamp <= now()
    ORDER BY created_at
    FOR UPDATE SKIP LOCKED
    LIMIT 1
)
UPDATE upload_jobs u
SET status = 'processing', updated_at = now()
FROM next_job nj
WHERE u.job_id = nj.job_id
RETURNING u.*;
```

**Progress Tracking Queries:**
```sql
-- Efficient progress calculation
SELECT 
    uj.job_id,
    uj.status,
    uj.progress,
    COALESCE(chunk_counts.total, 0) as chunks_in_buffer,
    COALESCE(vector_counts.total, 0) as vectors_in_buffer
FROM upload_jobs uj
LEFT JOIN (
    SELECT document_id, COUNT(*) as total
    FROM document_chunk_buffer
    GROUP BY document_id
) chunk_counts ON uj.document_id = chunk_counts.document_id
LEFT JOIN (
    SELECT document_id, COUNT(*) as total  
    FROM document_vector_buffer
    GROUP BY document_id
) vector_counts ON uj.document_id = vector_counts.document_id
WHERE uj.job_id = $1;
```

### Memory Management

**Streaming Large Documents:**
```python
async def _process_large_document(self, document_path: str):
    """Process large documents without loading entirely into memory"""
    async with self.storage.stream_blob(document_path) as stream:
        async for chunk in self._chunk_stream(stream, chunk_size=1024*1024):
            # Process chunk immediately, don't accumulate
            processed_chunk = await self._process_chunk(chunk)
            await self._persist_chunk_immediately(processed_chunk)
```

**Batch Size Adaptation:**
```python
def _calculate_optimal_batch_size(self, available_chunks: int) -> int:
    """Calculate optimal batch size based on system constraints"""
    # Consider OpenAI limits, memory constraints, and rate limits
    openai_limit = 256
    memory_based_limit = self._estimate_memory_limit()
    rate_limit_based = self.rate_limiter.get_optimal_batch_size()
    
    return min(openai_limit, memory_based_limit, rate_limit_based, available_chunks)
```

## Directory Structure

### Refactored Organization

```
backend/
├── api/
│   ├── main.py                 # FastAPI application
│   ├── webhooks/
│   │   ├── __init__.py
│   │   └── llamaparse.py       # Webhook handlers
│   ├── jobs/
│   │   ├── __init__.py
│   │   └── status.py           # Job status endpoints
│   └── middleware/
│       └── auth.py             # Authentication middleware
├── workers/
│   ├── __init__.py
│   ├── base_worker.py          # Main BaseWorker class
│   ├── runner.py               # Worker process runner
│   └── processors/
│       ├── __init__.py
│       ├── parse_validator.py  # Parse stage logic
│       ├── chunk_processor.py  # Chunking logic
│       └── embed_batcher.py    # Embedding logic
├── shared/
│   ├── db/
│   │   ├── __init__.py
│   │   ├── connection.py       # Database connection management
│   │   └── models.py           # SQLAlchemy models
│   ├── storage/
│   │   ├── __init__.py
│   │   └── supabase.py         # Storage client
│   ├── external/
│   │   ├── __init__.py
│   │   ├── llamaparse.py       # LlamaParse client
│   │   └── openai_client.py    # OpenAI client
│   ├── logging/
│   │   ├── __init__.py
│   │   └── structured.py       # Structured logging
│   ├── rate_limit/
│   │   ├── __init__.py
│   │   └── token_bucket.py     # Rate limiting
│   └── schemas/
│       ├── __init__.py
│       ├── jobs.py             # Job-related schemas
│       └── webhooks.py         # Webhook schemas
└── scripts/
    ├── migrate.py              # Database migrations
    ├── worker_runner.py        # Worker process launcher
    └── cleanup.py              # Buffer cleanup utilities
```

## Security Implementation

### Webhook Security

**Multi-layer Security:**
```python
class WebhookSecurity:
    """Comprehensive webhook security implementation"""
    
    @staticmethod
    def verify_signature(payload: bytes, signature: str, secret: str) -> bool:
        """Verify HMAC-SHA256 signature"""
        expected = hmac.new(secret.encode(), payload, hashlib.sha256).hexdigest()
        return hmac.compare_digest(signature, expected)
    
    @staticmethod
    def verify_timestamp(timestamp: int, tolerance: int = 300) -> bool:
        """Verify timestamp within tolerance window"""
        return abs(int(time.time()) - timestamp) <= tolerance
    
    async def verify_webhook_origin(self, request_ip: str) -> bool:
        """Verify webhook originates from allowed IPs"""
        allowed_ips = await self._get_llamaparse_ip_ranges()
        return ipaddress.ip_address(request_ip) in allowed_ips
```

### Data Protection

**Sensitive Data Handling:**
```python
class SensitiveDataFilter:
    """Filter sensitive data from logs and errors"""
    
    SENSITIVE_FIELDS = ['text', 'embedding', 'vector', 'content']
    
    def sanitize_log_data(self, data: dict) -> dict:
        """Remove sensitive fields from log data"""
        sanitized = data.copy()
        for field in self.SENSITIVE_FIELDS:
            if field in sanitized:
                sanitized[field] = f"[REDACTED - {len(str(data[field]))} chars]"
        return sanitized
```

## Monitoring & Observability

### Comprehensive Metrics

```python
class ProcessingMetrics:
    """Processing pipeline metrics collection"""
    
    def __init__(self):
        self.stage_duration = Counter('processing_stage_duration_seconds')
        self.stage_errors = Counter('processing_stage_errors_total')
        self.buffer_sizes = Gauge('buffer_table_sizes')
        self.external_api_calls = Counter('external_api_calls_total')
        self.cost_tracking = Counter('processing_costs_total')
    
    def record_stage_completion(self, stage: str, duration: float):
        """Record successful stage completion"""
        self.stage_duration.labels(stage=stage, status='success').inc(duration)
    
    def record_stage_error(self, stage: str, error_type: str):
        """Record stage processing error"""
        self.stage_errors.labels(stage=stage, error_type=error_type).inc()
    
    def record_api_cost(self, service: str, cost_cents: int):
        """Record external API costs"""
        self.cost_tracking.labels(service=service).inc(cost_cents)
```

### Health Checks

```python
@app.get("/health")
async def health_check():
    """Comprehensive health check endpoint"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }
    
    # Database connectivity
    try:
        await database.execute("SELECT 1")
        health_status["checks"]["database"] = "healthy"
    except Exception as e:
        health_status["checks"]["database"] = f"unhealthy: {e}"
        health_status["status"] = "degraded"
    
    # Storage connectivity
    try:
        await storage.health_check()
        health_status["checks"]["storage"] = "healthy"  
    except Exception as e:
        health_status["checks"]["storage"] = f"unhealthy: {e}"
        health_status["status"] = "degraded"
    
    # Worker queue depth
    queue_depth = await get_queue_depth()
    health_status["checks"]["queue_depth"] = queue_depth
    if queue_depth > 1000:
        health_status["status"] = "degraded"
    
    return health_status
```

## Migration & Deployment

### Migration Strategy

**Phase 1: Infrastructure Setup**
- Deploy new buffer tables alongside existing schema
- Implement BaseWorker without disrupting current workers
- Deploy webhook endpoint with feature flag disabled

**Phase 2: Parallel Operation**
- Enable webhook endpoint for new jobs only
- Run BaseWorker alongside existing workers
- Validate processing consistency and performance

**Phase 3: Full Migration**
- Switch all new jobs to BaseWorker processing
- Complete existing jobs with legacy workers
- Remove specialized worker components

**Phase 4: Cleanup**
- Remove legacy worker code and infrastructure
- Optimize buffer table indexes and maintenance
- Update monitoring and alerting for new architecture

### Deployment Configuration

```yaml
# render.yaml for BaseWorker deployment
services:
- type: worker
  name: base-worker
  env: python
  buildCommand: pip install -r requirements.txt
  startCommand: python backend/scripts/worker_runner.py
  envVars:
  - key: DATABASE_URL
    fromDatabase:
      name: accessa-postgres
      property: connectionString
  - key: SUPABASE_SERVICE_ROLE_KEY
    sync: false
  - key: OPENAI_API_KEY
    sync: false
  - key: LLAMAPARSE_API_KEY
    sync: false
  scaling:
    minInstances: 1
    maxInstances: 5
    targetCPU: 70
    targetMemory: 80
```

## Risk Mitigation

### Technical Risks

**Risk: Buffer Table Growth**
- Mitigation: Automated cleanup processes, monitoring, archival strategies
- Monitoring: Buffer table size alerts, cleanup job success rates

**Risk: External Service Dependencies**
- Mitigation: Circuit breakers, comprehensive retry logic, fallback strategies
- Monitoring: External API success rates, response time percentiles

**Risk: Memory Pressure from Large Documents**
- Mitigation: Streaming processing, adaptive batch sizes, resource monitoring
- Monitoring: Memory usage alerts, processing time distributions

### Operational Risks

**Risk: Webhook Security Vulnerabilities**
- Mitigation: Multi-layer security, IP allowlisting, signature verification
- Monitoring: Failed authentication attempts, suspicious traffic patterns

**Risk: Processing Queue Backlog**
- Mitigation: Horizontal scaling, priority queues, capacity planning
- Monitoring: Queue depth alerts, processing throughput trends

## Future Considerations

### Cloud Platform Migration

The BaseWorker architecture is designed to facilitate future migration to managed cloud platforms:

- **Kubernetes**: Stateless design enables easy containerization and orchestration
- **AWS ECS/Fargate**: Buffer-based handoffs work well with managed container services
- **Google Cloud Run**: Webhook-driven processing fits serverless execution model
- **Message Queues**: Current job polling can be replaced with SQS/Pub-Sub without logic changes

### Multi-Model Support

Buffer architecture supports future multi-model embedding strategies:

- **Parallel Models**: Generate embeddings for multiple models simultaneously
- **A/B Testing**: Compare model performance on same content
- **Model Versioning**: Seamless upgrade and rollback capabilities

### Advanced Processing

The state machine can be extended for additional processing stages:

- **Quality Validation**: AI-powered content quality checks
- **Metadata Extraction**: Structured data extraction from documents
- **Content Classification**: Automatic tagging and categorization

---

## Conclusion

This RFC provides a comprehensive technical foundation for implementing the BaseWorker architecture. The design prioritizes reliability, observability, and future extensibility while simplifying the current specialized worker complexity. The buffer-driven approach ensures idempotent operations and crash recovery, while the webhook integration enables efficient external service coordination.

**Implementation Timeline:** 4 weeks for complete migration
**Success Criteria:** >98% processing reliability with simplified operational overhead
**Review Required:** Senior engineering, DevOps, and security teams