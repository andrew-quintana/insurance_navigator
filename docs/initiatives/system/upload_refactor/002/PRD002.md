# PRD002: Worker Refactor - Single BaseWorker Architecture

## Context & Background

This PRD defines the product requirements for the 002 Worker Refactor iteration, building upon the learnings from the 001 implementation. The goal is to replace the specialized worker architecture with a unified BaseWorker that orchestrates the entire processing pipeline through buffer-driven persistence and external service integration.

**Previous Iteration Context:** The 001 implementation provided valuable insights into the complexity of managing multiple specialized workers, the importance of idempotent operations, and the challenges of maintaining state consistency across distributed processing stages.

## Problem Statement

**Current Architecture Challenges:**
- Multiple specialized workers increase complexity and coupling between components
- State management across workers creates potential for inconsistency and race conditions
- Limited visibility into processing stages and difficult error recovery
- Complex inheritance hierarchies make testing and maintenance difficult
- API and worker coupling complicates future migration to managed platforms

**User Impact:**
- Insurance professionals experience unpredictable processing behavior during failures
- Recovery from processing errors requires manual intervention and debugging
- Limited progress visibility creates uncertainty about processing status
- Complex deployment and scaling procedures limit operational efficiency

## Success Metrics

**Primary KPIs:**
- Processing pipeline reliability: >98% success rate for end-to-end processing
- Recovery time from failures: <5 minutes for automatic retry scenarios
- Processing predictability: <10% variance in processing times for similar documents
- Operational complexity reduction: 50% fewer deployment components

**Secondary Metrics:**
- Worker scaling efficiency: Linear throughput increase with additional workers
- Buffer storage efficiency: <20% overhead from staging tables
- External service integration reliability: >99% successful API calls with retries
- Development velocity: 30% faster feature implementation due to simplified architecture

## User Stories

### Primary Users: Insurance Professionals

**As an insurance professional,**
- I want document processing to continue reliably even when individual services experience issues
- I want clear visibility into which stage of processing my document is in
- I want failed documents to automatically retry with exponential backoff
- I want duplicate documents to be detected and handled automatically without reprocessing

### Primary Users: System Administrators

**As a system administrator,**
- I want a single worker service that I can scale horizontally based on queue depth
- I want comprehensive logging that allows me to debug processing issues quickly
- I want buffer tables that provide clear audit trails of processing stages
- I want webhook endpoints that are secure and handle external service callbacks reliably

### Secondary Users: Developers

**As a developer,**
- I want a unified worker architecture that is easy to test and extend
- I want clear separation between API endpoints and worker processing logic
- I want idempotent operations that make testing and development predictable
- I want comprehensive error handling that provides actionable debugging information

## Functional Requirements

### FR1: Unified Worker Architecture
- Implement single BaseWorker class that orchestrates all processing stages
- Support state machine progression through upload_jobs.status field
- Provide micro-batch processing for embedding generation with immediate persistence
- Enable horizontal scaling through stateless worker design

### FR2: Buffer-Driven Pipeline
- Persist all stage handoffs through dedicated buffer tables (document_chunk_buffer, document_vector_buffer)
- Implement idempotent operations with deterministic keys (UUIDv5 with canonical strings)
- Support crash recovery through buffer state reconstruction
- Provide atomic status transitions tied to durable buffer writes

### FR3: Webhook Integration
- Accept LlamaParse callbacks through secure webhook endpoint with HMAC verification
- Write parsed artifacts to blob storage using backend service credentials
- Update job status atomically with artifact persistence
- Support replay protection and duplicate callback handling

### FR4: External Service Management
- Integrate with LlamaParse API for PDF to markdown conversion
- Implement micro-batched OpenAI embedding calls with rate limiting
- Provide circuit breaker patterns for external service failures
- Support exponential backoff retry logic with maximum retry limits

### FR5: Progress Tracking & Observability
- Maintain real-time progress counters in upload_jobs.progress field
- Provide detailed error logging with correlation IDs for debugging
- Support job status queries with stage-level progress information
- Enable manual retry capability from any failed stage

## Non-Functional Requirements

### NFR1: Reliability & Resilience
- Achieve >98% end-to-end processing success rate
- Support automatic recovery from transient failures within 5 minutes
- Maintain data consistency during worker crashes or restarts
- Provide deterministic processing results for identical input documents

### NFR2: Performance & Scalability
- Process documents in micro-batches to optimize external API usage
- Support horizontal worker scaling without coordination overhead
- Maintain memory efficiency by avoiding in-memory accumulation of large data sets
- Achieve sub-second response times for job status queries

### NFR3: Maintainability & Extensibility
- Simplify codebase by reducing specialized worker classes
- Provide clear separation between API and worker components
- Support future migration to managed queue/worker platforms
- Enable easy addition of new processing stages or external services

### NFR4: Security & Compliance
- Secure webhook endpoints with HMAC signature verification
- Use backend service credentials for all storage operations
- Prevent direct client access to buffer tables or internal processing state
- Maintain audit trails of all processing operations

### NFR5: Operational Excellence
- Provide comprehensive metrics for monitoring processing pipeline health
- Support graceful shutdown and restart of worker processes
- Enable easy debugging through structured logging and error reporting
- Facilitate cost tracking for external service usage

## Acceptance Criteria

### AC1: Worker Architecture
- ✅ Single BaseWorker class handles all processing stages through state machine
- ✅ Worker processes can be scaled horizontally without coordination
- ✅ Processing stages are clearly defined with atomic transitions
- ✅ Worker restarts resume processing from last committed buffer state

### AC2: Buffer Operations
- ✅ All chunks written to document_chunk_buffer with idempotent upserts
- ✅ Embeddings written to document_vector_buffer in micro-batches
- ✅ Status advances only after successful buffer writes
- ✅ Duplicate processing attempts result in no-op operations

### AC3: Webhook Functionality
- ✅ LlamaParse webhook accepts callbacks and verifies HMAC signatures
- ✅ Parsed artifacts are written to blob storage using backend credentials
- ✅ Job status is updated atomically with artifact persistence
- ✅ Webhook handles duplicate callbacks gracefully

### AC4: External Service Integration
- ✅ LlamaParse integration supports async processing with webhook callbacks
- ✅ OpenAI embedding calls are batched efficiently with rate limiting
- ✅ Circuit breaker patterns prevent cascade failures from external services
- ✅ Retry logic handles transient failures with exponential backoff

### AC5: Progress & Monitoring
- ✅ Job status endpoint returns accurate progress with stage-level details
- ✅ Processing errors are logged with sufficient context for debugging
- ✅ Failed jobs can be manually restarted from any stage
- ✅ Buffer tables provide complete audit trail of processing operations

## Technical Assumptions & Dependencies

### Assumptions
- Supabase Postgres can handle increased buffer table write volume efficiently
- LlamaParse webhook reliability meets >99% delivery rate
- OpenAI API maintains consistent rate limits and response times
- Buffer table cleanup processes will maintain reasonable storage overhead

### Dependencies
- Supabase Postgres with vector extension for embedding storage
- Supabase Storage for blob artifact persistence
- LlamaParse API for PDF to markdown conversion with webhook support
- OpenAI API for text embedding generation
- Render platform for worker hosting and horizontal scaling

### External Constraints
- OpenAI API rate limits must be respected to avoid service disruption
- LlamaParse webhook callbacks must be handled within timeout windows
- Supabase storage limits may constrain large document processing
- Worker memory limits require streaming approaches for large documents

## Risk Assessment

**High Risk:**
- Buffer table growth could impact database performance if cleanup processes fail
- Webhook security vulnerabilities could expose internal processing state
- External service dependencies create single points of failure

**Medium Risk:**
- Increased database write volume from buffer operations
- Memory pressure from micro-batch processing of large documents
- Complexity of state machine transitions during error scenarios

**Low Risk:**
- Performance overhead from additional buffer table operations
- Operational complexity from unified worker architecture
- Migration effort from existing specialized worker components

## Out of Scope

- Multi-model embedding support (single model for this iteration)
- Real-time processing capabilities (async batch processing continues)
- Advanced chunking strategies beyond current markdown-simple approach
- Complex workflow orchestration beyond linear stage progression
- Integration with external queue services (maintain Postgres-based queue)

## Migration Strategy

### Phase 1: Buffer Table Implementation (Week 1)
- Create document_chunk_buffer and document_vector_buffer tables
- Implement idempotent upsert operations for buffer writes
- Update upload_jobs schema with enhanced status values and progress tracking

### Phase 2: BaseWorker Development (Week 2)
- Implement unified BaseWorker class with state machine logic
- Develop micro-batch processing for embedding generation
- Create comprehensive error handling and retry mechanisms

### Phase 3: Webhook Integration (Week 3)
- Implement secure webhook endpoint for LlamaParse callbacks
- Integrate blob storage operations with backend service credentials
- Test end-to-end processing with external service integration

### Phase 4: Migration & Validation (Week 4)
- Migrate existing processing jobs to new architecture
- Validate performance and reliability improvements
- Deprecate specialized worker components and update deployment

## Next Steps

This PRD serves as the foundation for:
1. **RFC002.md** - Technical architecture and implementation decisions
2. **TODO002.md** - Detailed development tasks and implementation guidance
3. Engineering team sprint planning and resource allocation

**Stakeholder Review Required:** Product team, engineering leads, DevOps team
**Technical Review Required:** Database team, security team, infrastructure team
**Decision Authority:** Engineering Director and Product Director