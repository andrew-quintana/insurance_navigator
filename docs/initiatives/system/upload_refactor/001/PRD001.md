# PRD001: Insurance Document Ingestion Pipeline Refactor

## Context & Background
This PRD builds upon the technical requirements outlined in `CONTEXT.md` to define the product requirements for refactoring Insurance Navigator's MVP document ingestion pipeline. The current system needs to be rebuilt to handle insurance PDF documents through a robust upload → parse → chunk → embed → finalize workflow.

## Problem Statement

**Current Pain Points:**
- Insurance PDF processing lacks a structured, scalable pipeline
- No proper job tracking or error handling for document processing stages
- Missing idempotent processing capabilities leading to duplicate work
- No proper observability into document processing progress
- Storage patterns are inconsistent and not optimized for user isolation

**User Impact:**
- Insurance agents and customers experience unpredictable document processing times
- Failed uploads require complete restart with no resume capability
- No visibility into processing status creates user frustration
- Document processing failures are difficult to diagnose and recover from

## Success Metrics

**Primary KPIs:**
- Document processing success rate: >95%
- Average processing time per document: <5 minutes for documents up to 25MB
- User-perceived uptime: >99.5%
- Failed job recovery time: <1 hour

**Secondary Metrics:**
- Processing cost per document: <$0.50
- Storage efficiency: deduplicated uploads save >20% storage costs
- Error resolution time: <15 minutes for common failures
- User satisfaction with upload experience: >4.5/5

## User Stories

### Primary Users: Insurance Agents
**As an insurance agent,**
- I want to upload policy documents and see real-time processing status
- I want processing to continue even if I close my browser
- I want to be notified when documents are ready for search
- I want to understand why a document failed to process

### Primary Users: Insurance Customers  
**As an insurance customer,**
- I want to upload my policy documents and have them processed reliably
- I want to see clear progress indicators during upload and processing
- I want to know if my document upload was successful or failed
- I want duplicate documents to be detected automatically

### Secondary Users: System Administrators
**As a system administrator,**
- I want comprehensive logging for debugging processing failures
- I want to monitor system health and processing queues
- I want to identify and resolve bottlenecks in the pipeline
- I want to track processing costs and resource utilization

## Functional Requirements

### FR1: Upload Management
- Accept PDF files up to 25MB and 200 pages
- Generate deterministic document IDs using UUIDv5
- Detect duplicate uploads via SHA-256 hash comparison
- Provide signed URLs for secure file uploads to private storage
- Validate file integrity and format before processing

### FR2: Pipeline Orchestration
- Implement sequential stage processing: queued → job_validated → parsing → parsed → parse_validated → chunking → chunks_buffered → chunked → embedding → embeddings_buffered → embedded
- Support idempotent resume from any stage
- Maintain authoritative job state in `upload_jobs` table
- Handle concurrent processing with proper locking mechanisms
- Provide progress tracking with stage-level completion percentages

### FR3: Document Processing
- Parse PDFs to normalized markdown with consistent formatting
- Generate deterministic chunk IDs and embedding keys
- Support configurable chunking strategies (starting with markdown-simple@1)
- Create vector embeddings using text-embedding-3-small model
- Store embeddings co-located with chunks for efficient retrieval

### FR4: Storage & Security
- Use private Supabase storage buckets with user-isolated paths
- Implement row-level security for all user data access
- Generate short-lived signed URLs for frontend storage access
- Store all file paths using consistent URI patterns
- Maintain service-role access for backend processing

### FR5: Error Handling & Resilience
- Implement exponential backoff retry logic with maximum retry limits
- Move failed jobs to dead letter queue after max retries
- Provide detailed error logging with correlation IDs
- Support manual job restart from any stage
- Maintain data consistency during failures

## Non-Functional Requirements

### NFR1: Performance
- Process documents within 5 minutes for files up to 25MB
- Support concurrent processing of up to 2 jobs per user
- Handle embedding batches of up to 256 vectors efficiently
- Maintain sub-second response times for job status queries

### NFR2: Scalability
- Support horizontal scaling of worker processes
- Handle daily upload limits of 30 documents per user
- Efficiently process documents with up to 200 pages
- Support future multi-model embedding strategies

### NFR3: Reliability
- Achieve 95% processing success rate
- Implement proper database transaction handling
- Support graceful degradation during external service outages
- Maintain data integrity across all processing stages

### NFR4: Security & Compliance
- Encrypt all data in transit and at rest
- Implement proper access controls using RLS policies
- Never log sensitive content (text, embeddings, full paths)
- Support future HIPAA compliance requirements

### NFR5: Observability
- Provide comprehensive event logging for all processing stages
- Track key metrics: latency, retry counts, queue depth
- Support correlation tracking across distributed processing
- Enable real-time monitoring of system health

## Acceptance Criteria

### AC1: Upload Flow
- ✅ User can upload PDF file and receive immediate job ID
- ✅ System detects duplicate files and returns existing document
- ✅ Invalid files are rejected with clear error messages
- ✅ Upload process works reliably across different file sizes

### AC2: Processing Pipeline
- ✅ Each stage completes successfully or fails with actionable errors
- ✅ Failed jobs can be manually restarted from any stage
- ✅ Processing continues correctly after worker restarts
- ✅ All processing is idempotent and deterministic

### AC3: Status Tracking
- ✅ Users can query job status and receive accurate progress information
- ✅ Processing stages are clearly communicated to users
- ✅ Error states provide sufficient detail for user action
- ✅ Status updates occur in real-time or near real-time

### AC4: Data Quality
- ✅ Parsed documents maintain semantic structure and readability
- ✅ Chunks are generated consistently with proper overlap
- ✅ Embeddings are generated correctly and stored efficiently
- ✅ Vector similarity search returns relevant results

### AC5: System Integration
- ✅ All database migrations run successfully
- ✅ Storage buckets and permissions are configured correctly
- ✅ API endpoints follow established patterns and security requirements
- ✅ Worker processes can be deployed and scaled independently

## Assumptions & Dependencies

### Assumptions
- Supabase storage and database will handle expected load
- LlamaIndex parsing service will maintain >95% uptime
- OpenAI embedding API will remain available and cost-effective
- Document content is primarily text-based (not image-heavy)

### Dependencies
- Supabase Postgres with pgvector extension
- Supabase Storage for file hosting
- LlamaIndex parsing service for PDF to markdown conversion
- OpenAI API for text embeddings
- Render hosting platform for workers and API

### External Constraints
- Must work within Render's serverless constraints
- Cannot exceed OpenAI API rate limits
- Must comply with Supabase storage limits
- Processing must complete within reasonable timeframes for user experience

## Risk Assessment

**High Risk:**
- External service downtime (LlamaIndex, OpenAI) could block all processing
- Large document processing could exceed memory limits in serverless environment

**Medium Risk:**
- Database performance degradation with large embedding tables
- Cost escalation with high-volume usage

**Low Risk:**
- Storage costs exceeding projections
- User adoption exceeding current rate limits

## Out of Scope

- Multi-format document support (only PDF for MVP)
- Real-time processing (async batch processing acceptable)
- Advanced chunking strategies beyond markdown-simple
- Multi-model embedding support (single model for MVP)
- Advanced search and retrieval interfaces
- PHI isolation or on-premise deployment options

## Next Steps

This PRD serves as the foundation for:
1. **RFC001.md** - Technical design and architecture decisions
2. **TODO001.md** - Detailed implementation task breakdown
3. Development team implementation planning and sprint organization

**Stakeholder Review Required:** Product team, engineering leads, security team
**Technical Review Required:** Database architecture team, infrastructure team
**Decision Authority:** Product Director and Engineering Director