# RFC001: Insurance Document Ingestion Pipeline Technical Design

## Context & Overview

This RFC builds upon **PRD001.md** to define the technical architecture for Accessa's insurance document ingestion pipeline refactor. The design implements the sequential processing pipeline (upload → parse → chunk → embed → finalize) with robust error handling, idempotent processing, and comprehensive observability.

**Legacy System Retirement:** This implementation replaces existing systems in `supabase/functions/`, `db/services/`, and previous database schemas with a unified, job-queue-based architecture that provides better reliability, idempotency, and observability.

**Key PRD Requirements Addressed:**
- 95% processing success rate for PDF documents up to 25MB/200 pages
- <5 minute processing time with proper progress tracking
- Idempotent resume capability from any stage
- Proper user isolation and security controls
- Comprehensive error handling and retry logic

## Architecture Overview

### High-Level System Design

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Frontend      │    │   FastAPI       │    │   Workers       │
│   (Next.js)     │◄──►│   (Render)      │◄──►│   (Render)      │
│                 │    │                 │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         │                       │                       │
         ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Supabase      │    │   Supabase      │    │   External      │
│   Storage       │    │   Postgres      │    │   Services      │
│   (Private)     │    │   (+pgvector)   │    │   (LlamaIndex,  │
│                 │    │                 │    │    OpenAI)      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### Component Responsibilities

**Frontend (Next.js on Vercel):**
- Initiate uploads via API calls
- Poll job status for progress updates
- Access parsed documents via signed URLs
- Display processing status and error states

**API Server (FastAPI on Render):**
- Handle upload requests and file validation
- Generate signed URLs for storage access
- Provide job status endpoints
- Manage user authentication and rate limiting

**Workers (Render Background Services):**
- Poll job queue using `FOR UPDATE SKIP LOCKED`
- Execute stage-specific processing logic
- Handle retries and error states
- Communicate with external services (LlamaIndex, OpenAI)

**Database (Supabase Postgres):**
- Store job state and document metadata
- Provide queue coordination between workers
- Maintain comprehensive event logs
- Store vector embeddings with HNSW indexing

## Data Flow Architecture

### Processing Pipeline Stages

```
queued → job_validated → parsing → parsed → parse_validated → chunking → chunks_buffered → chunked → embedding → embeddings_buffered → embedded
```

**Stage Transitions:**
1. **queued**: Job created and waiting for processing
2. **job_validated**: File uploaded, dedupe check complete, ready for parsing
3. **parsing**: PDF converted to normalized markdown via LlamaIndex
4. **parsed**: Parse complete, content stored, ready for validation
5. **parse_validated**: Parse validation complete, ready for chunking
6. **chunking**: Markdown split into semantic chunks with deterministic IDs
7. **chunks_buffered**: Chunks placed in buffer, ready for database commit
8. **chunked**: Chunks committed to database, ready for embedding
9. **embedding**: Chunks converted to vectors via OpenAI text-embedding-3-small
10. **embeddings_buffered**: Embeddings placed in buffer, ready for database commit
11. **embedded**: Embeddings committed to database, document ready for search

### Job State Management

**Job States:**
- `queued`: Ready for worker processing
- `working`: Currently being processed by a worker
- `retryable`: Failed but eligible for retry
- `done`: Successfully completed
- `deadletter`: Failed permanently after max retries

**State Transitions:**
```
queued → working → done
   ↑        ↓
   └─── retryable ──→ deadletter
```

### Idempotency Design

**Deterministic ID Generation:**
- Namespace UUID: `6c8a1e6e-1f0b-4aa8-9f0a-1a7c2e6f2b42`
- Document ID: UUIDv5(namespace, user_id + file_sha256)
- Chunk ID: UUIDv5(namespace, document_id + chunker + version + ord)
- Parse ID: UUIDv5(namespace, document_id + parser + version)

**Idempotency Checks:**
- Upload: Check `(user_id, file_sha256)` uniqueness
- Parse: Verify `parsed_path` and `parsed_sha256` exist
- Chunk: Count existing chunks for `(document_id, chunker_name, chunker_version)`
- Embed: Verify embeddings populated with correct model/version

## Database Schema Design

### Core Tables

**documents** - Document metadata and storage paths
```sql
CREATE TABLE documents (
    document_id uuid PRIMARY KEY,
    user_id uuid NOT NULL,
    filename text NOT NULL,
    mime text NOT NULL,
    bytes_len bigint NOT NULL,
    file_sha256 text NOT NULL,
    parsed_sha256 text,
    raw_path text NOT NULL,
    parsed_path text,
    processing_status text,
    created_at timestamptz DEFAULT now(),
    updated_at timestamptz DEFAULT now()
);

CREATE UNIQUE INDEX uq_user_filehash ON documents (user_id, file_sha256);
CREATE INDEX ix_documents_user ON documents (user_id);
```

**upload_jobs** - Job queue and state management
```sql
CREATE TABLE upload_jobs (
    job_id uuid PRIMARY KEY,
    document_id uuid NOT NULL REFERENCES documents(document_id),
    stage text NOT NULL CHECK (stage IN ('queued','job_validated','parsing','parsed','parse_validated','chunking','chunks_buffered','chunked','embedding','embeddings_buffered','embedded')),
    state text NOT NULL CHECK (state IN ('queued','working','retryable','done','deadletter')),
    retry_count int DEFAULT 0,
    idempotency_key text,
    payload jsonb,
    last_error jsonb,
    claimed_by text,
    claimed_at timestamptz,
    started_at timestamptz,
    finished_at timestamptz,
    created_at timestamptz DEFAULT now(),
    updated_at timestamptz DEFAULT now()
);

CREATE UNIQUE INDEX uq_job_doc_stage_active ON upload_jobs (document_id, stage) 
WHERE state IN ('queued','working','retryable');
CREATE INDEX ix_jobs_state ON upload_jobs (state, created_at);
```

**document_chunks** - Chunks with co-located embeddings
```sql
CREATE TABLE document_chunks (
    chunk_id uuid PRIMARY KEY,
    document_id uuid NOT NULL REFERENCES documents(document_id),
    chunker_name text NOT NULL,
    chunker_version text NOT NULL,
    chunk_ord int NOT NULL,
    text text NOT NULL,
    chunk_sha text NOT NULL,
    embed_model text NOT NULL,
    embed_version text NOT NULL,
    vector_dim int NOT NULL CHECK (vector_dim = 1536),
    embedding vector(1536) NOT NULL,
    embed_updated_at timestamptz DEFAULT now(),
    created_at timestamptz DEFAULT now(),
    updated_at timestamptz DEFAULT now(),
    UNIQUE (document_id, chunker_name, chunker_version, chunk_ord)
);

CREATE INDEX idx_hnsw_chunks_te3s_v1 ON document_chunks USING hnsw (embedding) 
WHERE embed_model='text-embedding-3-small' AND embed_version='1';
ALTER TABLE document_chunks SET (fillfactor=70);
```

**document_vector_buffer** - Write-ahead buffer for embeddings
```sql
CREATE TABLE document_vector_buffer (
    buffer_id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    chunk_id uuid NOT NULL REFERENCES document_chunks(chunk_id),
    embed_model text NOT NULL,
    embed_version text NOT NULL,
    vector_dim int NOT NULL CHECK (vector_dim = 1536),
    embedding vector(1536) NOT NULL,
    created_at timestamptz DEFAULT now()
);
```

**events** - Comprehensive event logging
```sql
CREATE TABLE events (
    event_id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id uuid NOT NULL REFERENCES upload_jobs(job_id),
    document_id uuid NOT NULL REFERENCES documents(document_id),
    ts timestamptz DEFAULT now(),
    type text NOT NULL CHECK (type IN ('stage_started','stage_done','retry','error','finalized')),
    severity text NOT NULL CHECK (severity IN ('info','warn','error')),
    code text NOT NULL,
    payload jsonb,
    correlation_id uuid
);

CREATE INDEX idx_events_job_ts ON events (job_id, ts DESC);
CREATE INDEX idx_events_doc_ts ON events (document_id, ts DESC);
```

## Technical Decisions & Rationale

### Decision 1: Postgres-Based Job Queue

**Chosen Approach:** Use Postgres with `FOR UPDATE SKIP LOCKED` for job queue
**Alternatives Considered:** Redis, AWS SQS, dedicated message queue
**Rationale:**
- Simplifies infrastructure (single database)
- ACID guarantees for job state transitions
- Built-in persistence and observability
- Proven pattern for moderate scale workloads

### Decision 2: Co-located Embedding Storage

**Chosen Approach:** Store embeddings in same table as chunks
**Alternatives Considered:** Separate embeddings table, external vector database
**Rationale:**
- Reduces join complexity for similarity search
- Simplifies data consistency guarantees
- pgvector provides adequate performance for MVP scale
- Easier backup and recovery procedures

### Decision 3: Buffer-Based Embedding Updates

**Chosen Approach:** Write embeddings to buffer table, then batch copy to final table
**Alternatives Considered:** Direct updates, separate embedding versioning
**Rationale:**
- Prevents partial embedding updates during failures
- Enables atomic switchover for model changes
- Reduces lock contention on main table
- Supports future multi-model strategies

### Decision 4: Deterministic ID Generation

**Chosen Approach:** UUIDv5 with canonicalized input strings
**Alternatives Considered:** Database sequences, random UUIDs
**Rationale:**
- Enables perfect idempotency for retries
- Prevents duplicate processing of identical content
- Supports distributed processing without coordination
- Facilitates debugging and correlation

## Storage Architecture

### Bucket Organization
```
storage://raw/{user_id}/{document_id}.pdf
storage://parsed/{user_id}/{document_id}.md
```

**Security Model:**
- Private buckets with no public access
- Frontend accesses via short-lived signed URLs (5 min TTL)
- Backend services use service-role access
- User isolation via path-based organization

**Access Patterns:**
- Upload: Frontend → Signed URL → Supabase Storage
- Parse: Worker → Service SDK → Supabase Storage
- Read: Frontend → API → Signed URL → Supabase Storage

## Worker Processing Algorithm

### Job Dequeue Logic
```sql
WITH cte AS (
    SELECT job_id
    FROM upload_jobs
    WHERE state='queued'
    ORDER BY created_at
    FOR UPDATE SKIP LOCKED
    LIMIT 1
)
UPDATE upload_jobs u
SET state='working', claimed_by=:worker_id, claimed_at=now(),
    started_at=COALESCE(started_at, now())
FROM cte
WHERE u.job_id = cte.job_id
RETURNING u.*;
```

### Stage Processing Logic

**Upload Validation:**
1. Check for existing document with same `(user_id, file_sha256)`
2. If found, mark job done and return existing document_id
3. If new, validate file and advance to parsing stage

**Parsing:**
1. Check if `parsed_path` exists and `parsed_sha256` is set
2. If complete, advance to chunking stage
3. If not, submit to LlamaIndex API and poll for completion
4. Normalize parsed markdown and compute `parsed_sha256`
5. Store in `storage://parsed/{user_id}/{document_id}.md`

**Chunking:**
1. Check if chunks exist for `(document_id, chunker_name, chunker_version)`
2. If count matches expected, advance to embedding stage
3. If not, load parsed content and re-chunk
4. Insert chunks into `document_chunks` (without embeddings)

**Embedding:**
1. Check if all chunks have embeddings with correct model/version
2. If complete, advance to finalizing stage
3. If not, batch generate embeddings via OpenAI API
4. Write to `document_vector_buffer`
5. Acquire advisory lock on `document_id`
6. Copy buffer embeddings to final table
7. Delete buffer rows

**Finalizing:**
1. Mark job as `done`
2. Update `documents.processing_status`
3. Log completion event

### Error Handling & Retry Logic

**Retry Strategy:**
- Exponential backoff: `2^retry_count * 3 seconds`
- Maximum retries: 3
- Dead letter queue for permanent failures

**Error Classification:**
- Transient: Network timeouts, rate limits, temporary service unavailable
- Permanent: Invalid file format, parsing errors, quota exceeded
- Recoverable: Partial processing, external service errors

## Security Implementation

### Row-Level Security Policies
```sql
-- Enable RLS on all tables
ALTER TABLE documents ENABLE ROW LEVEL SECURITY;
ALTER TABLE document_chunks ENABLE ROW LEVEL SECURITY;
ALTER TABLE upload_jobs ENABLE ROW LEVEL SECURITY;
ALTER TABLE events ENABLE ROW LEVEL SECURITY;

-- User can only see their own documents
CREATE POLICY doc_select_self ON documents
FOR SELECT USING (user_id = auth.uid());

-- User can only see chunks from their documents
CREATE POLICY chunk_select_self ON document_chunks
FOR SELECT USING (
    EXISTS (SELECT 1 FROM documents d
            WHERE d.document_id = document_chunks.document_id
              AND d.user_id = auth.uid())
);
```

### Key Management
- Service-role keys stored in Render environment variables
- API keys for external services (LlamaIndex, OpenAI) in secure environment
- No sensitive data logged in events or application logs

## API Contracts

### Upload Endpoint
```
POST /upload
Content-Type: application/json

{
    "filename": "policy.pdf",
    "bytes_len": 2048576,
    "mime": "application/pdf",
    "sha256": "abc123...",
    "ocr": false
}

Response:
{
    "job_id": "uuid",
    "document_id": "uuid", 
    "signed_url": "https://...",
    "upload_expires_at": "2025-08-14T10:30:00Z"
}
```

### Job Status Endpoint
```
GET /job/{job_id}

Response:
{
    "job_id": "uuid",
    "stage": "embedding",
    "state": "working",
    "retry_count": 0,
    "progress": {
        "stage_pct": 72.5,
        "total_pct": 80.0
    },
    "cost_cents": 0,
    "document_id": "uuid",
    "last_error": null,
    "updated_at": "2025-08-14T10:25:00Z"
}
```

## Performance Considerations

### Database Optimization
- Partial HNSW index on embeddings with model/version filter
- Table fillfactor=70 for document_chunks to reduce page splits
- Proper indexing on job queue polling queries
- Advisory locking for embedding updates to prevent conflicts

### Scaling Strategy
- Horizontal worker scaling via additional Render services
- Database connection pooling and prepared statements
- Batch processing for embeddings (up to 256 vectors per batch)
- Async processing with proper timeout handling

### Memory Management
- Stream large file processing to avoid memory limits
- Chunked embedding generation for large documents
- Proper cleanup of temporary processing artifacts

## Observability & Monitoring

### Event Taxonomy
```
Event Types: stage_started, stage_done, retry, error, finalized
Severities: info, warn, error
Event Codes: UPLOAD_ACCEPTED, PARSE_REQUESTED, CHUNK_BUFFERED, 
            EMBED_COMMITTED, RETRY_SCHEDULED, DLQ_MOVED
```

### Key Metrics
- Per-stage processing latency (p50, p95, p99)
- Job retry counts and failure rates
- Queue depth and worker utilization
- External service response times and error rates
- Storage costs and embedding generation costs

### Logging Strategy
- Structured logging with correlation IDs
- Never log sensitive content (document text, embeddings)
- Comprehensive error context for debugging
- Performance tracking for optimization

## Legacy System Retirement Strategy

### Current Systems Inventory

**Active Systems to Replace:**
- **Supabase Edge Functions** (`/supabase/functions/`): upload-handler, doc-parser, chunker, embedder, processing-webhook
- **Python Database Services** (`/db/services/`): document_service.py, storage_service.py, embedding_service.py, auth_service.py
- **Current Database Schema**: documents.documents, documents.document_chunks with inconsistent storage patterns
- **Storage Organization**: Mixed patterns in `files` bucket without user isolation

**Dependencies to Map:**
- Frontend DocumentUploadServerless.tsx integration points
- Agent imports of db.services modules
- Current API endpoints using legacy services
- External service integration patterns (LlamaIndex, OpenAI)

### Retirement Implementation Phases

### Phase 1: Assessment & Parallel Setup (Weeks 1-2)
- **Legacy Dependency Mapping**: Audit all imports and usage of `db.services` modules
- **New Infrastructure Setup**: Create RFC001 database schema alongside existing
- **Safe Archive Creation**: Backup current systems before modification
- **Integration Point Documentation**: Map all current frontend and agent dependencies

### Phase 2: Parallel Implementation (Weeks 3-4)
- **New API Development**: Build FastAPI endpoints per RFC001 without breaking current system
- **Worker Framework**: Implement job queue and processing workers
- **Storage Migration Prep**: Create new bucket organization patterns
- **Gradual Integration**: Begin migrating non-critical endpoints to new system

### Phase 3: Core Migration (Weeks 5-6)
- **Data Migration**: Transfer existing documents to new schema with deterministic IDs
- **Frontend Updates**: Update DocumentUploadServerless.tsx to use new API patterns
- **Agent Updates**: Replace db.services imports with new Supabase integration
- **Parallel Operation**: Run both systems simultaneously for validation

### Phase 4: Cutover & Cleanup (Week 7)
- **Final Migration**: Complete data transfer and validation
- **Legacy Deprecation**: Mark old endpoints as deprecated with proper notices
- **System Cleanup**: Remove unused Python services and old schema elements
- **Documentation Updates**: Update all references to new system architecture

### Migration Safety Measures

**Rollback Plan:**
- Maintain legacy services until new system is fully validated
- Keep complete data backups during migration process
- Implement feature flags for gradual cutover
- Preserve legacy API endpoints until all clients migrated

**Risk Mitigation:**
- **HIPAA Compliance**: Ensure encryption and security features are maintained
- **Data Integrity**: Validate all document and embedding data during migration
- **Performance Validation**: Confirm new system meets or exceeds current performance
- **Feature Parity**: Ensure all current functionality is preserved or enhanced

## Implementation Phases

### Phase 1: Legacy Assessment & New Infrastructure (Weeks 1-2)
- Legacy system dependency mapping and documentation
- Database schema creation and migrations for new system
- Basic API endpoints for upload and status (parallel to existing)
- Storage bucket reconfiguration and migration planning

### Phase 2: Core Processing Pipeline (Weeks 3-4)
- Upload validation and deduplication implementation
- LlamaIndex integration migration from edge functions
- Chunking implementation with markdown-simple
- Basic error handling and retry logic
- Begin frontend integration updates

### Phase 3: Embedding & Agent Integration (Weeks 5-6)
- OpenAI integration for embeddings with buffer pattern
- Vector storage and indexing optimization
- Agent service migration from db.services to new API
- Progress tracking and status reporting
- Parallel system validation and testing

### Phase 4: Production Cutover & Cleanup (Week 7)
- Final data migration and legacy system retirement
- Security hardening and RLS policies validation
- Performance optimization and monitoring setup
- Legacy system cleanup and documentation updates

## Risk Assessment & Mitigation

### High-Risk Areas
**External Service Dependencies:**
- Risk: LlamaIndex or OpenAI downtime blocks processing
- Mitigation: Implement circuit breakers, fallback strategies, and alerting

**Memory Constraints in Serverless:**
- Risk: Large document processing exceeds Render memory limits
- Mitigation: Stream processing, chunked operations, file size validation

### Medium-Risk Areas
**Database Performance:**
- Risk: Large embedding tables impact query performance
- Mitigation: Proper indexing, table partitioning, connection pooling

**Cost Escalation:**
- Risk: High-volume usage increases external API costs
- Mitigation: Rate limiting, cost monitoring, usage analytics

## Testing Strategy

### Unit Testing
- UUID generation and canonicalization functions
- Markdown normalization and SHA256 computation
- Job state transition logic
- Error handling and retry mechanisms

### Integration Testing
- End-to-end pipeline from upload to embedding
- External service integration (LlamaIndex, OpenAI)
- Database transaction handling and rollbacks
- Storage operations and signed URL generation

### Performance Testing
- Large file processing under memory constraints
- Concurrent job processing and queue coordination
- Database performance under load
- Vector similarity search performance

### Security Testing
- RLS policy enforcement
- Signed URL security and expiration
- Input validation and sanitization
- Authentication and authorization flows

## Alternative Approaches Considered

### Message Queue vs Database Queue
**Considered:** Redis Streams, AWS SQS, RabbitMQ
**Rejected:** Added infrastructure complexity, additional failure points, less operational visibility

### Separate Vector Database
**Considered:** Pinecone, Weaviate, Qdrant
**Rejected:** Additional service dependency, data consistency challenges, increased latency

### Microservices Architecture
**Considered:** Separate services for parse/chunk/embed stages
**Rejected:** Over-engineering for MVP scale, deployment complexity, network latency

### Real-time Processing
**Considered:** WebSocket-based real-time updates
**Rejected:** Complexity doesn't match user value, polling sufficient for MVP

## Next Steps

This RFC serves as the foundation for:
1. **TODO001.md** - Detailed implementation tasks and developer guidance
2. Database migration scripts and schema setup
3. API endpoint implementation and testing
4. Worker service development and deployment

**Technical Review Required:** Senior engineers, database team, security team
**Implementation Timeline:** 7 weeks from approval
**Success Criteria:** All PRD acceptance criteria met with 95% test coverage

---

## Legacy System Reference

### Systems Being Retired

**Supabase Edge Functions** (`/supabase/functions/`):
- `upload-handler/` - Basic file upload handling
- `doc-parser/` - LlamaIndex PDF parsing integration
- `chunker/` - Document chunking with markdown processing
- `embedder/` - OpenAI embedding generation
- `processing-webhook/` - LlamaIndex callback handling

**Python Database Services** (`/db/services/`):
- `document_service.py` - HIPAA-compliant document management
- `storage_service.py` - Encrypted file storage operations
- `embedding_service.py` - OpenAI embedding wrapper
- `auth_service.py`, `user_service.py`, `conversation_service.py`

**Database Schema Elements** (Current):
- Inconsistent storage path organization
- Basic `documents` and `document_chunks` without job tracking
- Missing idempotency and retry mechanisms
- No comprehensive event logging

### Migration Benefits

**Reliability Improvements:**
- Idempotent processing with deterministic IDs
- Robust retry logic with exponential backoff
- Comprehensive error handling and dead letter queue
- Job state tracking and resume capability

**Performance Enhancements:**
- Co-located embeddings for faster similarity search
- Optimized database indexing and query patterns
- Batch processing for embedding generation
- Advisory locking for conflict prevention

**Operational Benefits:**
- Unified architecture replacing fragmented systems
- Comprehensive observability and event logging
- Simplified deployment and scaling model
- Better security with consistent RLS policies

## Context Reference

This RFC implements the complete technical specification outlined in the project CONTEXT.md document, ensuring all authoritative decisions from the 2025-08-13 discussion are properly incorporated into the final system design while providing a clear path for retiring legacy systems in `supabase/`, `db/`, and related components.