# PRD001.md â€” Short-Term Chat Memory MVP

## Project Context

**Project:** Insurance Navigator - AI Agent Platform  
**Target Users:** AI agents within the platform requiring contextual continuity across chat interactions  
**Current State:** Agents operate without persistent memory of prior conversations, requiring users to re-explain context and previous decisions within the same goal-oriented chat thread

## Problem Statement

AI agents in the Insurance Navigator platform lack continuity across user interactions within a single chat session. Users must repeatedly re-explain context, prior decisions, and clarifications, creating friction and reducing efficiency. This results in:

- **Context Loss:** Agents cannot reference previously confirmed user facts or preferences
- **Repetitive Interactions:** Users waste time re-explaining the same information
- **Inconsistent Agent Behavior:** Decisions made earlier in the conversation are not preserved
- **Poor User Experience:** Lack of conversational flow reduces perceived intelligence of the system

## Success Metrics

### Primary KPIs
- **Context Retention Rate:** 95% of user-confirmed facts and LLM-inferred assumptions preserved across chat interactions
- **Conversation Efficiency:** 30% reduction in repeated clarifications within a single chat thread  
- **Memory Update Reliability:** 99.5% success rate for memory updates after input-processing workflow completion

### Secondary Metrics
- **Agent Response Quality:** Improved relevance scores when agents reference prior context
- **User Satisfaction:** Measured through conversational flow surveys
- **System Performance:** Memory updates complete within 2 seconds of workflow completion

## User Stories

### Primary Users: AI Agents
- **As an AI agent**, I want access to a unified summary of prior chat context so that I can provide contextually relevant responses without asking users to repeat information
- **As an AI agent**, I want to distinguish between user-confirmed facts and my own inferences so that I can appropriately weight information in my responses
- **As an AI agent**, I want to understand the overarching goal and progress of the current chat so that I can maintain continuity in my assistance

### Secondary Users: System Operators
- **As a system operator**, I want memory updates to be reliable and traceable so that I can debug conversation quality issues
- **As a system operator**, I want memory size limits to prevent system overload so that performance remains consistent

## Functional Requirements

### Core Memory Management
- **FR001:** Store one canonical summary per chat in Supabase `chat_metadata` table
- **FR002:** Support three distinct memory fields:
  - `user_confirmed`: Facts explicitly confirmed by the user
  - `llm_inferred`: Model-derived assumptions not directly confirmed  
  - `general_summary`: Overarching summary of chat goal and progress
- **FR003:** Trigger memory updates via manual API calls only (no automatic integration triggers)
- **FR004:** Generate summaries using dedicated MCP summarizer agent powered by Claude Haiku

### Update Process
- **FR005:** Split memory operations into two sequential phases:
  1. MCP agent summarizes using prior context + new user/agent exchange
  2. Sequential processing step uploads result to `chat_metadata`
- **FR006:** Track update status in `chat_context_queue` table with fields:
  - `chat_id`
  - `new_context_snippet` 
  - `status`: `pending_summarization`, `ready_to_write`, `complete`
- **FR007:** Implement write-ahead logging pattern for reliability

### Size and Scope Management  
- **FR008:** Cap summary at maximum token threshold (TBD - see Open Questions)
- **FR009:** Prompt user to start new chat when context exceeds threshold
- **FR010:** Restrict memory scope to single chat thread (no cross-chat access)

## Non-Functional Requirements

### Performance
- **NFR001:** Memory updates complete within 2 seconds of workflow completion
- **NFR002:** Memory retrieval adds <100ms latency to agent response times
- **NFR003:** Support concurrent memory updates across multiple chats

### Reliability
- **NFR004:** 99.5% memory update success rate under normal operating conditions
- **NFR005:** Graceful degradation when summarization fails (agents use stale context)
- **NFR006:** Automatic retry mechanism for failed updates

### Scalability
- **NFR007:** Handle memory updates for up to 10,000 concurrent active chats
- **NFR008:** Database storage scales with chat volume without performance degradation

### Security & Privacy
- **NFR009:** Memory storage complies with existing data privacy policies
- **NFR010:** No sensitive information persists beyond chat session completion
- **NFR011:** Memory access restricted to agents within the same chat thread

## Acceptance Criteria

### Memory Storage
- [ ] `chat_metadata` table successfully stores three distinct memory fields per chat
- [ ] Memory updates trigger automatically after input-processing workflow completion
- [ ] `chat_context_queue` accurately tracks update status through all phases
- [ ] Summary size limits are enforced and trigger appropriate user prompts

### Integration Points
- [ ] Manual API endpoint successfully initiates memory updates
- [ ] MCP Summarizer Agent operates as standalone component following base agent pattern
- [ ] Sequential processing step correctly processes queue entries and uploads summaries
- [ ] Memory data is stored and retrievable via API (agent integration out of scope)

### Edge Case Handling
- [ ] System handles partial write failures using write-ahead log pattern
- [ ] Agents gracefully operate with stale context during update processing
- [ ] LLM crashes/timeouts leave queue entries in retrievable state for retry
- [ ] Excessive context length triggers new chat prompts as expected

### Quality Assurance
- [ ] Memory summaries accurately reflect user-confirmed vs. LLM-inferred information
- [ ] General summaries capture chat goals and progress effectively
- [ ] Context continuity improves measurably across agent interactions

## Assumptions & Dependencies

### Technical Dependencies
- **Supabase Database:** Requires `chat_metadata` and `chat_context_queue` tables
- **MCP Infrastructure:** Depends on existing MCP implementation for standalone summarizer agent
- **Sequential Processing:** Requires sequential processing capability for database operations
- **API Infrastructure:** Requires REST API endpoint capability

### Business Assumptions
- **Single Chat Scope:** Users will accept memory limitations to single chat threads for MVP
- **Token Threshold:** A reasonable token limit exists that balances context richness with system performance
- **Manual Operation:** System will be operated manually via API calls for MVP
- **Future Integration:** Agent integration and workflow automation will be handled in separate initiatives

### External Factors
- **Claude Haiku Availability:** Summarization depends on Anthropic API reliability
- **Database Performance:** Supabase can handle the additional read/write load
- **API Reliability:** System depends on stable API infrastructure

## Open Questions & Next Steps

### Critical Decisions Required
1. **Token Threshold:** What is the optimal token limit for triggering new chat recommendations?
2. **Delta Summarization:** Should `chat_context_queue` include prior summary reference for incremental updates?
3. **Agent-Triggered Updates:** Do we need to support manual memory updates in addition to passive ones?

### Next Steps
1. **Technical Design Phase:** Create RFC001.md detailing architecture and implementation approach
2. **Database Schema:** Finalize table structures for `chat_metadata` and `chat_context_queue`
3. **API Design:** Design manual trigger API endpoints and documentation
4. **Performance Testing:** Establish baseline metrics for memory update timing and reliability

### Out of Scope for This Implementation
- **Automatic Integration Triggers:** Input processing workflow, supervisor handoff integration
- **Agent Memory Access Integration:** Agents calling memory API automatically
- **Cross-System Orchestration:** Integration with existing MCP orchestration patterns
- **Workflow-Based Memory Updates:** Will be handled in separate input workflow initiative

---

**Document Version:** PRD001  
**Creation Date:** 2025-08-07  
**Next Document:** RFC001.md (Technical Design)  
**Approval Required:** Product and Engineering stakeholders