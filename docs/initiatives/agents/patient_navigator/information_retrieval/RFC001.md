# RFC001: Information Retrieval Agent - Technical Design

## Context

This RFC defines the technical architecture and implementation approach for the Information Retrieval Agent based on the requirements established in **PRD001.md**. The agent will bridge user queries with insurance expertise through intelligent terminology translation, RAG-based document retrieval, and self-consistency validation.

### PRD Reference
- **Document**: `PRD001.md` in this directory
- **Key Requirements**: Structured input processing (FR1), terminology translation (FR2), expert-reframed RAG integration (FR3), self-consistency response generation (FR4), structured output delivery (FR5)
- **Success Metrics**: >90% query translation accuracy, >0.7 RAG similarity threshold, >0.8 response consistency, <2s response time

## Technical Overview

### High-Level Architecture

The Information Retrieval Agent follows a **ReAct (Reasoning and Acting) pattern** with structured step-by-step processing:

```
┌─────────────────────┐    ┌──────────────────────────────────────────────┐    ┌─────────────────────┐
│  Supervisor         │    │ Information Retrieval ReAct Agent           │    │  RAG System         │
│  Workflow           │───▶│ Step 1: Parse Structured Input               │───▶│  (Existing)         │
│  (Upstream)         │    │ Step 2: Query Reframing                      │    │                     │
│                     │    │ Step 3: RAG Integration                      │    │                     │
│                     │    │ Step 4-N: Self-Consistency Loop             │    │                     │
│                     │    │ Final: Structured Output                     │    │                     │
└─────────────────────┘    └──────────────────────────────────────────────┘    └─────────────────────┘
                                                     │                                     │
                                                     ▼                                     ▼
                                          ┌──────────────────────┐                ┌─────────────────────┐
                                          │ ReAct State          │                │ Supabase Database   │
                                          │ Management           │                │ (pgvector)          │
                                          └──────────────────────┘                └─────────────────────┘
```

### Core Architecture Principles

1. **ReAct Pattern**: Structured reasoning and acting with explicit step-by-step processing
2. **Expert-Only Approach**: Generate embeddings only from expert-reframed queries
3. **Iterative Self-Consistency**: Loop-based multi-variant generation and validation
4. **Direct RAG Integration**: Use existing RAG system without modification

## System Architecture

### Directory Structure (Domain-Driven)

```
agents/
├── patient_navigator/                 # Patient navigation domain
│   ├── __init__.py                    # Domain initialization
│   ├── information_retrieval/         # NEW: Information retrieval agent
│   │   ├── __init__.py
│   │   ├── agent.py                   # Main IR agent (~300 lines)
│   │   ├── models.py                  # Pydantic I/O models
│   │   ├── prompts/
│   │   │   ├── system_prompt.md       # Expert reframing prompt
│   │   │   ├── human_message.md       # Input template
│   │   │   └── examples.json          # Few-shot examples based on scan_classic_hmo_parsed.pdf
│   │   └── tests/
│   │       ├── test_agent.py          # Agent integration tests using scan_classic_hmo_parsed.pdf
│   │       ├── test_terminology.py    # Translation unit tests with Classic HMO examples
│   │       └── test_consistency.py    # Consistency checker tests with HMO document content
│   ├── workflow_prescription/         # RELOCATE: Existing agent
│   │   └── [existing files]           # Move from current location
│   └── shared/                        # Domain-specific utilities
│       ├── __init__.py
│       ├── terminology.py             # Insurance term mapping
│       └── consistency.py             # Self-consistency implementation
└── tooling/                           # Cross-domain utilities (unchanged)
    └── rag/                           # Existing RAG system
        └── core.py                    # RAGTool, RetrievalConfig
```

### ReAct Agent Data Flow

```
Input: Supervisor Workflow Output
├─ Workflow prescription context
├─ Document requirements  
├─ User context & original query
└─ Availability status
         │
         ▼
┌─────────────────────────────────────────────────────────────────┐
│ Information Retrieval ReAct Agent Processing Loop              │
│                                                                 │
│ Step 1: Parse Structured Input                                 │
│   ├─ Extract user query from supervisor context               │
│   ├─ Validate workflow requirements                           │
│   └─ Set initial agent state                                  │
│                                                               │
│ Step 2: Query Reframing                                       │
│   ├─ Apply insurance terminology translation                  │
│   ├─ Generate expert-level query rephrasing                   │
│   └─ Validate reframing quality                              │
│                                                               │
│ Step 3: RAG Integration                                        │
│   ├─ Generate embeddings from expert query only               │
│   ├─ Retrieve document chunks via existing RAG system         │
│   ├─ Apply similarity thresholds and filtering                │
│   └─ Assess retrieval quality                                 │
│                                                               │
│ Step 4-N: Self-Consistency Loop (Repeat 3-5 times)           │
│   ├─ Generate response variant from retrieved chunks          │
│   ├─ Evaluate variant quality and completeness                │
│   ├─ Calculate consistency with previous variants             │
│   ├─ Decide: Continue loop or proceed to synthesis?           │
│   └─ Update confidence scoring                                │
│                                                               │
│ Final Step: Structured Output                                  │
│   ├─ Synthesize final response from variants                  │
│   ├─ Calculate final confidence score                         │
│   ├─ Format JSON with source attribution                      │
│   └─ Return to supervisor workflow                            │
└─────────────────────────────────────────────────────────────────┘
```

## Technical Decisions

### TD1: Domain-Driven Organization
**Decision**: Organize agents by business domain (`patient_navigator/`) rather than technical function.

**Rationale**:
- **Cohesion**: Related agents share domain knowledge and utilities
- **Scalability**: Easy to add new patient navigation agents
- **Maintenance**: Domain experts can work within focused directories
- **Reusability**: Shared utilities reduce code duplication

**Implementation**: Create `agents/patient_navigator/` hierarchy with shared utilities in `shared/` subdirectory.

### TD2: BaseAgent Inheritance Pattern
**Decision**: Inherit from existing BaseAgent class following established patterns.

**Rationale**:
- **Consistency**: Maintains ecosystem compatibility
- **Testing**: Leverages existing mock capabilities
- **Error Handling**: Inherits proven error handling patterns
- **Integration**: Seamless integration with existing agent infrastructure

**Implementation**: `class InformationRetrievalAgent(BaseAgent)` with standard initialization and process methods.

### TD3: Direct RAG System Integration
**Decision**: Use existing `agents/tooling/rag/core.py` without modifications.

**Rationale**:
- **Stability**: Proven system with user-scoped access control
- **Performance**: Optimized for <200ms queries with token budget management
- **Security**: Database-level security already implemented
- **Maintenance**: No additional system to maintain

**Implementation**: Instantiate `RAGTool` with user context and `RetrievalConfig.default()`.

### TD4: Expert-Only Embedding Strategy
**Decision**: Generate embeddings only from expert-reframed queries, not original user input.

**Rationale**:
- **Retrieval Quality**: Insurance documents use technical terminology
- **Consistency**: Expert language matches document language
- **Performance**: Single embedding generation approach
- **Simplicity**: Clear, linear data flow within agent

**Implementation**: Single LLM call for expert translation followed by embedding generation.

### TD5: ReAct Pattern Implementation
**Decision**: Implement Information Retrieval Agent using ReAct (Reasoning and Acting) pattern with explicit step-by-step processing.

**Rationale**:
- **Transparency**: Clear visibility into agent reasoning process
- **Debuggability**: Each step can be individually monitored and validated
- **Flexibility**: Can adjust step logic without rewriting entire agent
- **Quality Control**: Explicit validation points for each processing stage
- **Iterative Improvement**: Loop-based self-consistency with early termination

**Implementation**: Structured agent with defined steps and state management between iterations.

### TD6: Loop-Based Self-Consistency Generation
**Decision**: Implement self-consistency as an iterative loop (3-5 iterations) with quality assessment.

**Rationale**:
- **Quality Control**: Can assess variant quality before deciding to continue
- **Efficiency**: Early termination if high consistency achieved
- **Transparency**: Clear progression through consistency iterations
- **Flexibility**: Configurable iteration count based on quality thresholds

**Implementation**: Loop with continuation decision based on consistency scores and iteration limits.

## Alternative Approaches Considered

### ALT1: Single Integrated Agent (Previous Approach)
**Approach**: Handle all processing internally with single LLM call for expert translation and integrated self-consistency generation.
**Rejected Because**:
- Limited transparency into agent reasoning process
- Harder to debug specific processing stages
- Less flexibility to optimize individual steps
- Missing clear validation points for quality control
- Difficult to monitor and improve specific components

### ALT2: Separate Terminology Translation Service
**Approach**: Create dedicated microservice for insurance terminology translation.
**Rejected Because**:
- Adds network latency to <2s requirement
- Increases deployment complexity
- Violates single agent responsibility principle
- Additional infrastructure overhead

### ALT3: Pre-computed Translation Mappings
**Approach**: Build static keyword mapping database for query translation.
**Rejected Because**:
- Cannot handle context-dependent terminology
- Limited flexibility for complex queries
- Missing nuanced interpretation capabilities
- LLM approach provides better quality

### ALT4: External Self-Consistency Service
**Approach**: Separate service for response variant generation and consistency checking.
**Rejected Because**:
- Breaks agent cohesion and ownership
- Adds complexity to error handling
- Increases response time with additional service calls
- Agent should own complete response quality process

### ALT5: Multi-Step Agent Chain
**Approach**: Break down into separate agents for translation, retrieval, and consistency.
**Rejected Because**:
- Overcomplicates single-purpose functionality
- Increases error points and debugging complexity
- Violates established agent patterns
- Single agent can handle all steps efficiently

## Risk Assessment & Mitigation

### R1: RAG System Dependency (High Risk)
**Risk**: Existing RAG system instability affects Information Retrieval Agent.
**Mitigation**: 
- Implement fallback responses when RAG unavailable
- Add circuit breaker pattern for RAG calls
- Monitor RAG system health and error rates
- Graceful degradation with cached responses

### R2: Expert Translation Quality (High Risk)
**Risk**: Single LLM call may not consistently produce high-quality expert translations.
**Mitigation**:
- Comprehensive prompt engineering with examples
- Validation testing with insurance domain experts
- Fallback to keyword-based mapping if translation confidence low
- Continuous monitoring of translation quality metrics

### R3: Performance Requirements (High Risk)
**Risk**: <2s response time challenging with LLM calls and RAG retrieval.
**Mitigation**:
- Optimize LLM prompts for faster response times
- Implement intelligent caching for repeated queries
- Parallel processing where possible
- Response time monitoring with alerting

### R4: Self-Consistency Reliability (Medium Risk)
**Risk**: Internal consistency scoring may not correlate with actual accuracy.
**Mitigation**:
- Empirical validation of consistency vs accuracy correlation
- Multiple consistency calculation approaches
- Human evaluation of high/low confidence responses
- Confidence threshold tuning based on feedback

### R5: Single Point of Failure (Medium Risk)
**Risk**: Agent handles all processing, creating potential bottleneck.
**Mitigation**:
- Horizontal scaling capabilities
- Comprehensive error handling and recovery
- Performance monitoring and alerting
- Circuit breaker patterns for external dependencies

## Testing Strategy

### Unit Testing
- **Expert Translation**: Validate LLM call output quality and format using insurance terminology examples from scan_classic_hmo_parsed.pdf
- **RAG Integration**: Test embedding generation and chunk retrieval with scan_classic_hmo_parsed.pdf document content
- **Self-Consistency**: Verify variant generation and scoring algorithms using examples from scan_classic_hmo_parsed.pdf
- **Output Formatting**: Validate JSON structure and confidence ranges with test response data

### Integration Testing
- **End-to-End Flow**: Complete agent processing from supervisor input to output using test queries from scan_classic_hmo_parsed.pdf
- **RAG System Integration**: Real document corpus retrieval testing with scan_classic_hmo_parsed.pdf content
- **Supervisor Compatibility**: Upstream workflow integration validation with insurance-specific use cases
- **Error Handling**: Graceful degradation scenario testing with document unavailability

### Performance Testing
- **Response Time**: Validate <2s requirement under normal load
- **Concurrent Users**: Multi-user scenario testing
- **Resource Efficiency**: Memory and CPU utilization monitoring
- **Consistency Quality**: Correlation between confidence and accuracy

### Quality Assurance
- **Expert Review**: Insurance domain expert validation of translations using examples from scan_classic_hmo_parsed.pdf
- **User Acceptance**: Response quality and completeness assessment with scan_classic_hmo_parsed.pdf content as test cases
- **Edge Cases**: Complex and ambiguous query handling with insurance-specific scenarios
- **Confidence Calibration**: Verify confidence scores match perceived quality using test examples from scan_classic_hmo_parsed.pdf

## Performance Considerations

### Response Time Optimization
- **Prompt Optimization**: Streamlined prompts for faster LLM responses
- **Caching Strategy**: Cache expert translations and embeddings
- **Parallel Processing**: Concurrent embedding and variant generation where possible
- **Database Optimization**: Leverage existing pgvector indexing

### Scalability Planning
- **Stateless Design**: Agent instances scale horizontally
- **Resource Management**: Efficient memory and connection handling
- **Load Distribution**: Multiple agent instances for concurrent requests
- **Monitoring Integration**: Performance metrics and alerting

### Quality Metrics
- **Translation Accuracy**: Expert query quality assessment
- **Retrieval Relevance**: Document chunk relevance scoring
- **Consistency Correlation**: Confidence vs actual accuracy tracking
- **User Satisfaction**: Response completeness and helpfulness metrics

## Next Steps

This RFC establishes the technical foundation for implementing the Information Retrieval Agent. The next step is to create **TODO001.md** which will break down the implementation into specific, actionable development tasks based on this technical design.

Key areas for TODO001 focus:
- Setup tasks for domain reorganization and directory structure
- Core implementation tasks for integrated agent processing
- Testing tasks for unit, integration, and performance validation
- Documentation and deployment preparation tasks
- Success milestone definitions and validation criteria