# RFC001.md - Input Processing Workflow Technical Design

## Context and PRD Reference

This RFC defines the technical architecture for the **Input Processing Workflow** as outlined in [PRD001.md](./PRD001.md). The system processes multilingual voice and text input, translating and sanitizing it for downstream multi-agent workflows.

### Key Requirements from PRD
- **Latency Target**: <5 seconds end-to-end processing
- **Core Pipeline**: Voice/Text Input → Translation → Sanitization → Handoff
- **Primary Integration**: ElevenLabs v3 API for translation, Flash v2.5 for fallback
- **Scale**: MVP supporting 10 concurrent users via CLI interface
- **Cost Constraint**: Free-tier API usage only
- **Language Setting**: User-configured language (hardcoded for MVP), no automatic detection

## Technical Overview

The Input Processing Workflow implements a **sequential pipeline architecture** with parallel fallback chains. Each component operates as an independent service with standardized input/output contracts, enabling modular testing and graceful error handling.

### Core Architecture Pattern
```
User Input → [Input Handler] → [Translation Router] → [Sanitization Agent] → [Output Formatter] → Downstream Agents
                    ↓                     ↓                      ↓                    ↓
               [Error Handler]      [Fallback Chain]    [Context Validator]   [Format Validator]
```

## System Architecture

### Component Design

#### 1. Input Handler (`agents/patient_navigator/input_processing/handler.py`)
**Responsibility**: Capture and normalize voice/text input
```python
from typing import Protocol, Union, Optional
from dataclasses import dataclass
import asyncio

@dataclass
class QualityScore:
    score: float
    confidence: float
    issues: list[str]

class InputHandler(Protocol):
    async def capture_voice_input(self) -> bytes:
        """Capture audio from microphone and return raw audio bytes"""
        ...
    
    async def capture_text_input(self) -> str:
        """Capture text input from CLI"""
        ...
        
    def validate_input_quality(self, input_data: Union[bytes, str]) -> QualityScore:
        """Validate quality of input data"""
        ...
```

**Implementation Details**:
- **Voice Capture**: PyAudio for microphone access via CLI
- **Text Capture**: Standard input() interface with UTF-8 support
- **Quality Validation**: Audio level detection using scipy, text length validation
- **Timeout Handling**: 30-second voice timeout with asyncio, immediate text processing

#### 2. Translation Router (`agents/patient_navigator/input_processing/router.py`)
**Responsibility**: Route to optimal translation service based on configured language and cost
```python
from typing import Protocol, Dict, List
from dataclasses import dataclass
from enum import Enum

@dataclass
class TranslationResult:
    text: str
    confidence: float
    provider: str
    cost_estimate: float

class TranslationProvider(Protocol):
    async def translate(self, text: str, source_lang: str, target_lang: str) -> str:
        """Translate text from source to target language"""
        ...
    
    def get_cost_estimate(self, text: str) -> float:
        """Estimate cost for translation"""
        ...
        
    def get_supported_languages(self) -> List[str]:
        """Get list of supported language codes"""
        ...

class TranslationRouter:
    def __init__(self):
        self.source_language = "es"  # Default to Spanish for MVP
        self.providers: Dict[str, TranslationProvider] = {}
    
    async def route(self, text: str, source_language: str) -> TranslationResult:
        """Route translation request to optimal provider"""
        ...
        
    def set_source_language(self, language: str) -> None:
        """Set the source language configuration"""
        ...
        
    def get_source_language(self) -> str:
        """Get current source language configuration"""
        ...
```

**Service Priority Logic**:
1. **ElevenLabs v3** for complex/uncommon languages (>40 supported)
2. **Flash v2.5** for high-volume common languages (cost optimization)
3. **Fallback Chain**: Browser Translation API → Google Translate (if available) → Manual escalation

**Cost Optimization**:
- Cache translations for repeated phrases (session-level, memory-only using functools.lru_cache)
- Smart routing based on text complexity analysis
- Rate limiting to stay within free-tier bounds

#### 3. Sanitization Agent (`agents/patient_navigator/input_processing/sanitizer.py`)
**Responsibility**: Clean and structure translated output
```python
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class UserContext:
    user_id: str
    conversation_history: List[str]
    language_preference: str
    domain_context: str = "insurance"

@dataclass 
class SanitizedOutput:
    cleaned_text: str
    structured_prompt: str
    confidence: float
    modifications: List[str]
    metadata: Dict[str, any]

class SanitizationAgent:
    def __init__(self):
        self.domain_keywords = self._load_insurance_keywords()
        
    async def sanitize(self, input_text: str, context: UserContext) -> SanitizedOutput:
        """Clean and structure translated text for downstream agents"""
        ...
        
    def _resolve_coreferences(self, text: str, context: UserContext) -> str:
        """Replace pronouns with explicit references"""
        ...
        
    def _clarify_intent(self, text: str) -> str:
        """Expand ambiguous terms using domain context"""
        ...
        
    def _structure_output(self, text: str) -> str:
        """Convert to structured prompt format"""
        ...
```

**Sanitization Pipeline**:
1. **Coreference Resolution**: Replace pronouns with explicit references
2. **Intent Clarification**: Expand ambiguous terms using insurance domain context
3. **Formalization**: Convert informal speech to structured prompts
4. **Validation**: Ensure output compatibility with downstream agents

#### 4. Integration Layer (`agents/patient_navigator/input_processing/integration.py`)
**Responsibility**: Format output for existing multi-agent workflow
```python
from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class AgentPrompt:
    prompt_text: str
    context: Dict[str, Any]
    metadata: Dict[str, Any]
    confidence: float
    source_language: str
    processing_steps: List[str]

class WorkflowHandoff:
    def __init__(self, downstream_agent_config: Dict[str, Any]):
        self.downstream_config = downstream_agent_config
        
    async def format_for_downstream(self, sanitized_output: SanitizedOutput, user_context: UserContext) -> AgentPrompt:
        """Format sanitized output for existing patient navigator workflow"""
        ...
        
    def validate_compatibility(self, prompt: AgentPrompt) -> bool:
        """Validate that prompt is compatible with downstream agents"""
        ...
        
    def _add_workflow_metadata(self, prompt: AgentPrompt) -> AgentPrompt:
        """Add metadata required by existing workflow system"""
        ...
```

#### 5. CLI Interface (`agents/patient_navigator/input_processing/cli_interface.py`)
**Responsibility**: Provide command-line interface for testing and MVP deployment
```python
import asyncio
import argparse
from typing import Optional
from .handler import InputHandler
from .router import TranslationRouter
from .sanitizer import SanitizationAgent
from .integration import WorkflowHandoff

class InputProcessingCLI:
    def __init__(self):
        self.input_handler = InputHandler()
        self.translation_router = TranslationRouter()
        self.sanitizer = SanitizationAgent()
        self.integration = WorkflowHandoff({})
        
    async def process_voice_input(self) -> str:
        """Process voice input through full pipeline"""
        ...
        
    async def process_text_input(self, text: str) -> str:
        """Process text input through full pipeline"""
        ...
        
    async def run_interactive_mode(self):
        """Run interactive CLI mode for testing"""
        ...

# Integration with existing FastAPI main.py
def add_input_processing_endpoints(app: FastAPI):
    """Add input processing endpoints to existing FastAPI app"""
    
    @app.post("/api/v1/input/process")
    async def process_input(
        request: Dict[str, Any],
        current_user: Dict[str, Any] = Depends(get_current_user)
    ):
        """Process multilingual input through translation and sanitization pipeline"""
        cli = InputProcessingCLI()
        
        if request.get("input_type") == "voice":
            result = await cli.process_voice_input()
        else:
            result = await cli.process_text_input(request["text"])
            
        return {"processed_text": result, "status": "completed"}
```

## Technical Decisions

### 1. Language Configuration: User-Set vs Auto-Detection
**Decision**: User-configured source language (hardcoded for MVP)
**Rationale**: 
- Eliminates language detection latency and accuracy issues
- Simpler architecture and fewer failure points
- User control over translation quality
- Faster processing pipeline

**Rejected Alternative**: Automatic language detection
- **Why Rejected**: Added complexity, latency overhead, potential accuracy issues, removed per PRD update

### 2. Architecture Pattern: Sequential Pipeline vs Event-Driven
**Decision**: Sequential Pipeline with Parallel Fallbacks
**Rationale**: 
- Simpler debugging and error tracking for MVP
- Predictable latency characteristics 
- Each component can be independently tested and optimized
- Fallback chains operate in parallel to main pipeline for performance

**Rejected Alternative**: Event-driven microservices
- **Why Rejected**: Added complexity for orchestration, debugging difficulty, potential for race conditions in MVP timeframe

### 3. Translation Service Integration: Direct API vs SDK
**Decision**: Direct REST API calls with custom retry logic
**Rationale**:
- Full control over request/response handling for latency optimization
- Custom fallback chain implementation
- Simplified dependency management
- Better error message customization

**Rejected Alternative**: Official SDKs
- **Why Rejected**: Additional abstraction layers, potential version conflicts, less control over performance optimization

### 4. Data Storage: In-Memory vs Persistent Cache
**Decision**: Session-level in-memory caching only
**Rationale**:
- Privacy compliance (no persistent audio/text storage)
- Simplified deployment (no database dependencies)
- Sufficient for MVP scale (10 concurrent users)

**Rejected Alternative**: Redis/persistent caching
- **Why Rejected**: Infrastructure complexity, privacy concerns, overkill for MVP scale

### 5. Error Handling: Fail-Fast vs Graceful Degradation
**Decision**: Graceful degradation with fallback chains
**Rationale**:
- Better user experience for unreliable external services
- Allows partial functionality when primary services fail
- Aligns with 99.5% availability requirement

## Implementation Plan

### Phase 1: Core Pipeline (Week 1)
**Deliverables**:
- Basic voice/text input capture via CLI
- User language configuration system (hardcoded for MVP)
- ElevenLabs integration for primary translation path
- Simple sanitization (basic cleanup, no advanced NLP)
- Output formatting for downstream handoff

**Validation Criteria**:
- End-to-end processing for 5 test languages with configured source language
- Latency <7 seconds (buffer for optimization)
- Success rate >80% for clear input

**Technical Assumptions to Validate**:
- ElevenLabs API response times <2 seconds
- CLI microphone access works across development environments
- Downstream workflow accepts formatted English prompts
- Basic sanitization improves agent performance by >20%
- Hardcoded language configuration provides sufficient translation accuracy

### Phase 2: Fallback & Optimization (Week 2)
**Deliverables**:
- Flash v2.5 integration for cost optimization
- Fallback chain implementation
- Performance optimization (caching, parallel processing)
- Advanced sanitization (coreference resolution, intent clarification)

**Validation Criteria**:
- Latency <5 seconds consistently
- Fallback success rate >85%
- Cost per interaction <$0.05
- Advanced sanitization improves user satisfaction scores

**Technical Assumptions to Validate**:
- Flash v2.5 provides sufficient translation quality for common languages
- Parallel fallback processing doesn't exceed memory limits
- Advanced sanitization maintains original user intent >90% of time
- User language configuration covers 95% of expected use cases

### Phase 3: Error Handling & Edge Cases (Week 3)
**Deliverables**:
- Comprehensive error handling and user feedback
- Edge case handling (mixed languages, long input, silent audio)
- Performance monitoring and alerting
- Documentation and deployment guides

## Risk Assessment

### High-Impact Technical Risks

#### 1. ElevenLabs API Reliability (Probability: Medium, Impact: High)
**Risk**: Primary translation service experiences downtime or rate limiting
**Mitigation**: 
- Implement circuit breaker pattern with automatic fallback
- Monitor API health and switch to backup services preemptively
- Cache successful translations to reduce API dependency
**Contingency**: Manual escalation workflow for critical translation failures

#### 2. Latency Performance (Probability: Medium, Impact: Medium)
**Risk**: End-to-end processing exceeds 5-second target
**Mitigation**:
- Parallel processing where possible (language detection + service health checks)
- Implement request queuing and batching for multiple inputs
- Optimize API calls with connection pooling and keep-alive
**Contingency**: Relaxed latency requirements for complex edge cases

#### 3. Cost Overrun (Probability: Low, Impact: Medium)
**Risk**: Free-tier API limits exceeded during testing
**Mitigation**:
- Implement usage monitoring and automatic throttling
- Smart routing to lower-cost services for simple translations
- Request batching to optimize API call efficiency
**Contingency**: Manual approval workflow for exceeding cost thresholds

### Low-Impact Technical Risks
- **Mixed-language input parsing**: Implement segmentation analysis
- **Audio quality variations**: Preprocessing filters and quality validation
- **Sanitization over-correction**: Confidence scoring and user validation loops
- **Incorrect language configuration**: User guidance and validation prompts

## Testing Strategy

### Unit Testing Approach
- **Input Handler**: Mock microphone/stdin interfaces, test timeout handling
- **Translation Router**: Mock API responses, test fallback chain activation, language configuration validation
- **Sanitization Agent**: Before/after comparison datasets for common insurance queries

### Integration Testing
- **End-to-end pipeline**: Real API calls with test accounts for latency validation
- **Error simulation**: Deliberate API failures to test fallback behavior  
- **Load testing**: 10 concurrent users with realistic usage patterns
- **Compatibility testing**: Output validation against downstream agent expectations

### Performance Testing
- **Latency benchmarking**: 95th percentile response times under various loads
- **Memory profiling**: Ensure no memory leaks during extended sessions
- **Cost validation**: Track API usage against free-tier limits
- **Network resilience**: Test behavior under slow/intermittent connectivity

### Testing Artifacts per Phase
After each implementation phase, generate `@TODO001_phaseX_test_update.md` documenting:
- Tests executed with evidence (screenshots, logs, performance metrics)
- Assumptions validated/disproven with impact analysis  
- New technical debt discovered and follow-up plans
- Performance benchmarks and optimization opportunities

## Performance Considerations

### Latency Optimization
- **Parallel Processing**: Run language detection and service health checks simultaneously
- **Connection Pooling**: Maintain persistent connections to external APIs
- **Request Batching**: Group multiple sanitization operations where possible
- **Progressive Loading**: Start downstream processing as soon as translation completes

### Memory Management
- **Stream Processing**: Handle large audio inputs without full buffering
- **Cache Limits**: Implement LRU eviction for translation cache (max 1000 entries)
- **Garbage Collection**: Explicit cleanup of audio buffers and temporary objects

### Scalability Preparation
- **Stateless Design**: All components support horizontal scaling
- **Load Balancing**: Ready for reverse proxy distribution when needed
- **Resource Monitoring**: Built-in metrics for CPU, memory, and API usage

## Security Considerations

### Data Protection
- **No Persistent Storage**: Audio and text processed in-memory only
- **Encryption in Transit**: All API calls use TLS 1.3
- **Data Sanitization**: Remove sensitive information before logging
- **Session Isolation**: No cross-session data leakage

### API Security
- **Key Management**: Environment variable storage for API keys
- **Rate Limiting**: Prevent abuse of external services
- **Input Validation**: Sanitize all user input to prevent injection attacks

## Alternative Approaches Considered

### 1. Client-Side Processing
**Approach**: Run translation/sanitization entirely in browser
**Pros**: No server costs, better privacy, reduced latency
**Cons**: Limited translation quality, browser compatibility issues, no fallback control
**Verdict**: Rejected due to quality and reliability concerns

### 2. Webhook-Based Architecture  
**Approach**: Async processing with webhook callbacks
**Pros**: Better scalability, fault isolation
**Cons**: Increased complexity, callback handling overhead, user experience impact
**Verdict**: Rejected for MVP due to complexity vs. benefit tradeoff

### 3. Pre-built Translation Platforms
**Approach**: Use Google Cloud Translation or AWS Translate
**Pros**: Enterprise reliability, comprehensive language support
**Cons**: Cost implications, less customization, vendor lock-in
**Verdict**: Considered for Phase 2 fallback chain integration

## Next Steps

Following RFC approval and technical review, the next document in sequence will be **TODO001.md** covering:

- Detailed implementation task breakdown with time estimates
- Development environment setup and dependency management
- Testing procedures and validation checkpoints
- Deployment configuration for Vercel/Render environments
- Monitoring and debugging setup for production readiness

The TODO document will reference this RFC's architecture decisions to ensure implementation consistency and traceability from requirements through technical design to development tasks.