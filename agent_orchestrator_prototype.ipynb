{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Agent Orchestrator Prototype Notebook\n",
        "\n",
        "**Purpose**: Fast prototyping and iterative development of agent workflows\n",
        "\n",
        "This notebook allows you to:\n",
        "- Test individual agents in isolation\n",
        "- Walk through complete workflows step-by-step \n",
        "- Measure performance and analyze results\n",
        "- Prototype new workflow architectures\n",
        "- Debug agent interactions\n",
        "\n",
        "Based on the principles from:\n",
        "- [How to refactor a Jupyter notebook](https://medium.com/data-science/how-to-refactor-a-jupyter-notebook-ed531b6a17)\n",
        "- [Prototyping with Python](https://www.fuzzingbook.org/beta/html/PrototypingWithPython.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List, Optional\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.abspath('.')\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"üìÅ Project root: {project_root}\")\n",
        "print(f\"üêç Python path updated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Import Project Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import orchestrator and agents\n",
        "try:\n",
        "    from graph.agent_orchestrator import AgentOrchestrator, get_orchestrator, reset_orchestrator\n",
        "    from agents import (\n",
        "        PromptSecurityAgent,\n",
        "        PatientNavigatorAgent, \n",
        "        TaskRequirementsAgent,\n",
        "        ServiceAccessStrategyAgent,\n",
        "        ChatCommunicatorAgent,\n",
        "        RegulatoryAgent\n",
        "    )\n",
        "    from utils.config_manager import ConfigManager\n",
        "    print(\"‚úÖ All imports successful\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"Please ensure you're running from the project root directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration and Global Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global configuration\n",
        "BYPASS_SECURITY = True  # Set to False for production testing\n",
        "TEST_USER_ID = \"prototype_user_001\"\n",
        "DEFAULT_CONVERSATION_ID = \"prototype_conv_001\"\n",
        "\n",
        "# Performance tracking\n",
        "performance_data = []\n",
        "test_results = []\n",
        "\n",
        "# Test scenarios\n",
        "TEST_SCENARIOS = {\n",
        "    \"simple_qa\": \"What does Medicare cover?\",\n",
        "    \"doctor_search\": \"I need to find a cardiologist in Seattle with Blue Cross insurance\",\n",
        "    \"incomplete_info\": \"I need a doctor\",\n",
        "    \"emergency\": \"I'm having chest pain and need immediate care\",\n",
        "    \"complex_request\": \"I need an X-ray for my back pain, I have Medicare Part B, live in Portland Oregon\"\n",
        "}\n",
        "\n",
        "print(f\"üîß Configuration loaded\")\n",
        "print(f\"üõ°Ô∏è Security bypass: {BYPASS_SECURITY}\")\n",
        "print(f\"üë§ Test user ID: {TEST_USER_ID}\")\n",
        "print(f\"üí¨ Test scenarios: {len(TEST_SCENARIOS)} loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Utility Functions for Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_timestamp(timestamp=None):\n",
        "    \"\"\"Format timestamp for display\"\"\"\n",
        "    if timestamp is None:\n",
        "        timestamp = datetime.now()\n",
        "    return timestamp.strftime(\"%H:%M:%S\")\n",
        "\n",
        "def print_section_header(title, emoji=\"üî¨\"):\n",
        "    \"\"\"Print a formatted section header\"\"\"\n",
        "    print(f\"\\n{emoji} {title}\")\n",
        "    print(\"=\" * (len(title) + 3))\n",
        "\n",
        "def record_performance(test_name, start_time, end_time, success, metadata=None):\n",
        "    \"\"\"Record performance data for analysis\"\"\"\n",
        "    duration = end_time - start_time\n",
        "    entry = {\n",
        "        'test_name': test_name,\n",
        "        'timestamp': datetime.now(),\n",
        "        'duration_seconds': duration,\n",
        "        'success': success,\n",
        "        'metadata': metadata or {}\n",
        "    }\n",
        "    performance_data.append(entry)\n",
        "    return entry\n",
        "\n",
        "def display_agent_result(agent_name, result, execution_time=None):\n",
        "    \"\"\"Display agent result in a formatted way\"\"\"\n",
        "    print(f\"\\nü§ñ {agent_name.upper()}\")\n",
        "    print(\"-\" * (len(agent_name) + 3))\n",
        "    \n",
        "    if execution_time:\n",
        "        print(f\"‚è±Ô∏è Execution time: {execution_time:.3f}s\")\n",
        "    \n",
        "    if isinstance(result, dict):\n",
        "        for key, value in result.items():\n",
        "            if key not in ['raw_response', 'full_output']:  # Skip verbose fields\n",
        "                print(f\"üìä {key}: {value}\")\n",
        "    else:\n",
        "        print(f\"üìù Result: {result}\")\n",
        "\n",
        "def analyze_workflow_performance(workflow_data):\n",
        "    \"\"\"Analyze workflow execution performance\"\"\"\n",
        "    if not workflow_data:\n",
        "        return \"No performance data available\"\n",
        "    \n",
        "    df = pd.DataFrame(workflow_data)\n",
        "    \n",
        "    analysis = {\n",
        "        'total_tests': len(df),\n",
        "        'success_rate': df['success'].mean() * 100,\n",
        "        'avg_duration': df['duration_seconds'].mean(),\n",
        "        'max_duration': df['duration_seconds'].max(),\n",
        "        'min_duration': df['duration_seconds'].min()\n",
        "    }\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "print(\"üõ†Ô∏è Utility functions loaded\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Initialize Orchestrator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset and initialize orchestrator\n",
        "print_section_header(\"Initializing Agent Orchestrator\", \"üöÄ\")\n",
        "\n",
        "try:\n",
        "    # Reset any existing instance\n",
        "    reset_orchestrator()\n",
        "    print(\"üîÑ Orchestrator reset\")\n",
        "    \n",
        "    # Create new instance\n",
        "    orchestrator = get_orchestrator(bypass_security=BYPASS_SECURITY)\n",
        "    print(\"‚úÖ Orchestrator initialized\")\n",
        "    print(f\"üõ°Ô∏è Security bypass: {orchestrator.bypass_security}\")\n",
        "    \n",
        "    # Test basic functionality\n",
        "    test_message = \"Hello, test\"\n",
        "    workflow_type = orchestrator._determine_workflow_type(test_message)\n",
        "    print(f\"üß™ Test workflow determination: '{test_message}' ‚Üí {workflow_type}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Orchestrator initialization failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Individual Agent Testing\n",
        "\n",
        "Test each agent in isolation to understand their behavior and performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test individual agents\n",
        "async def test_individual_agents(test_message=\"I need to find a cardiologist\"):\n",
        "    \"\"\"Test each agent individually\"\"\"\n",
        "    print_section_header(\"Individual Agent Testing\", \"üß™\")\n",
        "    \n",
        "    # Test data\n",
        "    test_user_id = TEST_USER_ID\n",
        "    test_conversation_id = f\"test_{int(time.time())}\"\n",
        "    \n",
        "    # 1. Test Prompt Security Agent\n",
        "    print(\"\\n1Ô∏è‚É£ Testing Prompt Security Agent\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        security_result = await orchestrator.prompt_security_agent.check_prompt_security(\n",
        "            test_message, test_user_id\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        display_agent_result(\"Prompt Security\", {\n",
        "            \"is_safe\": security_result.is_safe,\n",
        "            \"risk_score\": getattr(security_result, 'risk_score', 'N/A'),\n",
        "            \"issues\": getattr(security_result, 'issues', [])\n",
        "        }, end_time - start_time)\n",
        "        record_performance(\"prompt_security\", start_time, end_time, True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        record_performance(\"prompt_security\", start_time, time.time(), False)\n",
        "    \n",
        "    # 2. Test Patient Navigator Agent\n",
        "    print(\"\\n2Ô∏è‚É£ Testing Patient Navigator Agent\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        navigator_result = await orchestrator.patient_navigator_agent.analyze_request(\n",
        "            test_message, test_conversation_id\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        display_agent_result(\"Patient Navigator\", {\n",
        "            \"intent_type\": navigator_result.intent_type,\n",
        "            \"confidence_score\": navigator_result.confidence_score,\n",
        "            \"analysis_summary\": str(navigator_result.analysis_details)[:200] + \"...\"\n",
        "        }, end_time - start_time)\n",
        "        record_performance(\"patient_navigator\", start_time, end_time, True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        record_performance(\"patient_navigator\", start_time, time.time(), False)\n",
        "    \n",
        "    # 3. Test Task Requirements Agent\n",
        "    print(\"\\n3Ô∏è‚É£ Testing Task Requirements Agent\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # Mock navigator result for task requirements\n",
        "        mock_navigator_result = {\n",
        "            \"meta_intent\": {\"request_type\": \"find_provider\", \"location\": \"Seattle\"},\n",
        "            \"service_intent\": {\"specialty\": \"cardiology\"},\n",
        "            \"metadata\": {\"raw_user_text\": test_message}\n",
        "        }\n",
        "        task_result = await orchestrator.task_requirements_agent.analyze_requirements_structured(\n",
        "            mock_navigator_result, test_message\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        display_agent_result(\"Task Requirements\", {\n",
        "            \"requirements_count\": task_result.requirements_count,\n",
        "            \"documents_needed\": task_result.documents_needed,\n",
        "            \"status\": getattr(task_result, 'status', 'N/A')\n",
        "        }, end_time - start_time)\n",
        "        record_performance(\"task_requirements\", start_time, end_time, True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        record_performance(\"task_requirements\", start_time, time.time(), False)\n",
        "    \n",
        "    print(\"\\n‚úÖ Individual agent testing complete\")\n",
        "\n",
        "# Run the test\n",
        "await test_individual_agents()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Complete Workflow Testing\n",
        "\n",
        "Test the full orchestrator workflow with different scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test complete workflows\n",
        "async def test_complete_workflow(scenario_name, message):\n",
        "    \"\"\"Test the complete orchestrator workflow\"\"\"\n",
        "    print_section_header(f\"Testing Workflow: {scenario_name}\", \"üîÑ\")\n",
        "    print(f\"üìù Message: {message}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Process message through orchestrator\n",
        "        result = await orchestrator.process_message(\n",
        "            message=message,\n",
        "            user_id=TEST_USER_ID,\n",
        "            conversation_id=f\"test_{scenario_name}_{int(time.time())}\"\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"\\n‚è±Ô∏è Total execution time: {execution_time:.3f}s\")\n",
        "        print(f\"üîÑ Workflow type: {result.get('workflow_type', 'N/A')}\")\n",
        "        print(f\"üí¨ Response length: {len(result.get('text', ''))} characters\")\n",
        "        \n",
        "        # Display response (truncated)\n",
        "        response_text = result.get('text', '')\n",
        "        if len(response_text) > 300:\n",
        "            print(f\"üìÑ Response preview: {response_text[:300]}...\")\n",
        "        else:\n",
        "            print(f\"üìÑ Response: {response_text}\")\n",
        "        \n",
        "        # Display metadata summary\n",
        "        metadata = result.get('metadata', {})\n",
        "        print(f\"\\nüìä Metadata summary:\")\n",
        "        for key, value in metadata.items():\n",
        "            if isinstance(value, dict):\n",
        "                print(f\"   {key}: {len(value)} items\")\n",
        "            else:\n",
        "                print(f\"   {key}: {value}\")\n",
        "        \n",
        "        # Record performance\n",
        "        success = 'error' not in result\n",
        "        record_performance(\n",
        "            f\"workflow_{scenario_name}\", \n",
        "            start_time, \n",
        "            end_time, \n",
        "            success,\n",
        "            {\n",
        "                'workflow_type': result.get('workflow_type'),\n",
        "                'response_length': len(result.get('text', '')),\n",
        "                'metadata_keys': list(metadata.keys())\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Workflow completed successfully: {success}\")\n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        end_time = time.time()\n",
        "        print(f\"‚ùå Workflow failed: {e}\")\n",
        "        record_performance(f\"workflow_{scenario_name}\", start_time, end_time, False)\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Test all scenarios\n",
        "async def test_all_scenarios():\n",
        "    \"\"\"Test all predefined scenarios\"\"\"\n",
        "    print_section_header(\"Testing All Scenarios\", \"üéØ\")\n",
        "    \n",
        "    results = {}\n",
        "    for scenario_name, message in TEST_SCENARIOS.items():\n",
        "        result = await test_complete_workflow(scenario_name, message)\n",
        "        results[scenario_name] = result\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run all scenario tests\n",
        "scenario_results = await test_all_scenarios()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Performance Analysis and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze performance data\n",
        "def visualize_performance():\n",
        "    \"\"\"Create visualizations of performance data\"\"\"\n",
        "    print_section_header(\"Performance Analysis\", \"üìä\")\n",
        "    \n",
        "    if not performance_data:\n",
        "        print(\"‚ùå No performance data available\")\n",
        "        return\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(performance_data)\n",
        "    \n",
        "    # Overall statistics\n",
        "    print(\"üìà Overall Performance Statistics:\")\n",
        "    analysis = analyze_workflow_performance(performance_data)\n",
        "    for key, value in analysis.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Agent Orchestrator Performance Analysis', fontsize=16)\n",
        "    \n",
        "    # 1. Success rate by test\n",
        "    success_by_test = df.groupby('test_name')['success'].mean()\n",
        "    axes[0, 0].bar(success_by_test.index, success_by_test.values)\n",
        "    axes[0, 0].set_title('Success Rate by Test')\n",
        "    axes[0, 0].set_ylabel('Success Rate')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. Execution time by test\n",
        "    time_by_test = df.groupby('test_name')['duration_seconds'].mean()\n",
        "    axes[0, 1].bar(time_by_test.index, time_by_test.values)\n",
        "    axes[0, 1].set_title('Average Execution Time by Test')\n",
        "    axes[0, 1].set_ylabel('Time (seconds)')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. Timeline of execution times\n",
        "    df['timestamp_formatted'] = pd.to_datetime(df['timestamp'])\n",
        "    axes[1, 0].plot(df['timestamp_formatted'], df['duration_seconds'], 'o-')\n",
        "    axes[1, 0].set_title('Execution Time Timeline')\n",
        "    axes[1, 0].set_ylabel('Time (seconds)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 4. Success/failure distribution\n",
        "    success_counts = df['success'].value_counts()\n",
        "    axes[1, 1].pie(success_counts.values, labels=['Success', 'Failure'], autopct='%1.1f%%')\n",
        "    axes[1, 1].set_title('Success/Failure Distribution')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Display detailed performance table\n",
        "    print(\"\\nüìã Detailed Performance Data:\")\n",
        "    display_df = df[['test_name', 'duration_seconds', 'success', 'timestamp']].copy()\n",
        "    display_df['timestamp'] = display_df['timestamp'].dt.strftime('%H:%M:%S')\n",
        "    display_df = display_df.round({'duration_seconds': 3})\n",
        "    display(display_df)\n",
        "\n",
        "# Run performance analysis\n",
        "visualize_performance()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step-by-Step Workflow Debugging\n",
        "\n",
        "Walk through a workflow step-by-step to understand each agent's contribution.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
