{
  "permissions": {
    "allow": [
      "Bash(psql:*)",
      "Bash(for:*)",
      "Bash(do sed -i '' 's/@docs\\/initiatives\\//docs\\/initiatives\\//g' \"$file\")",
      "Bash(done)",
      "Bash(do sed -i '' 's/@TODO003_/TODO003_/g' \"$file\")",
      "Bash(mkdir:*)",
      "Bash(chmod:*)",
      "Bash(python:*)",
      "Bash(scripts/scaffold_initiative.sh:*)",
      "Bash(rm:*)",
      "Bash(docker-compose up:*)",
      "Bash(docker exec:*)",
      "Bash(find:*)",
      "Bash(__NEW_LINE__ mv TODO001_phase5_decisions.md TODO001_phase6_decisions.md)",
      "Bash(__NEW_LINE__ mv TODO001_phase5_handoff.md TODO001_phase6_handoff.md)",
      "Bash(__NEW_LINE__ mv TODO001_phase5_notes.md TODO001_phase6_notes.md)",
      "Bash(__NEW_LINE__ mv TODO001_phase5_testing_summary.md TODO001_phase6_testing_summary.md)",
      "Bash(docker-compose:*)",
      "Bash(docker compose:*)",
      "Bash(curl:*)",
      "Bash(PYTHONPATH=/Users/aq_home/1Projects/accessa/insurance_navigator:/Users/aq_home/1Projects/accessa/insurance_navigator/backend DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres python -c \"\nimport asyncio\nimport sys\nimport os\nsys.path.insert(0, ''/Users/aq_home/1Projects/accessa/insurance_navigator'')\nsys.path.insert(0, ''/Users/aq_home/1Projects/accessa/insurance_navigator/backend'')\n\nfrom scripts.test_complete_api_workflow import test_worker_job_processing_large_file, create_test_document, TEST_USER_ID, LARGE_TEST_FILE\nimport asyncpg\nimport uuid\n\nasync def run_test():\n    pool = await asyncpg.create_pool(''postgresql://postgres:postgres@localhost:5432/postgres'')\n    try:\n        # Clean up first\n        async with pool.acquire() as conn:\n            document_ids = await conn.fetch(''SELECT document_id FROM upload_pipeline.documents WHERE user_id = $1;'', TEST_USER_ID)\n            if document_ids:\n                await conn.execute(''DELETE FROM upload_pipeline.upload_jobs WHERE document_id = ANY($1);'', [d[''document_id''] for d in document_ids])\n            await conn.execute(''DELETE FROM upload_pipeline.documents WHERE user_id = $1;'', TEST_USER_ID)\n        \n        # Create test document\n        document_id, job_id = await create_test_document(pool, LARGE_TEST_FILE, TEST_USER_ID)\n        print(f''Created test document: {document_id}, job: {job_id}'')\n        \n        # Run the actual worker test\n        await test_worker_job_processing_large_file(pool)\n        print(''âœ… Large file test PASSED!'')\n        \n    finally:\n        await pool.close()\n\nasyncio.run(run_test())\n\")",
      "Bash(PYTHONPATH=/Users/aq_home/1Projects/accessa/insurance_navigator:/Users/aq_home/1Projects/accessa/insurance_navigator/backend python -c \"\nimport asyncio\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment\nload_dotenv(''.env.development'')\nsys.path.insert(0, ''.'')\nsys.path.insert(0, ''backend'')\n\nfrom backend.workers.base_worker import BaseWorker\nfrom shared.config import WorkerConfig\n\nasync def run_worker():\n    print(''ðŸ”§ Configuring worker...'')\n    config = WorkerConfig(\n        database_url=''postgresql://postgres:postgres@postgres:5432/postgres'',\n        supabase_url=''http://127.0.0.1:54321'',\n        supabase_anon_key=os.getenv(''ANON_KEY''),\n        supabase_service_role_key=os.getenv(''SERVICE_ROLE_KEY''),\n        llamaparse_api_url=''http://localhost:8001'',  # Use mock service\n        llamaparse_api_key=''test-key'',\n        openai_api_url=''http://localhost:8002'',      # Use mock service  \n        openai_api_key=''test-key'',\n        openai_model=''text-embedding-3-small''\n    )\n    \n    print(''ðŸš€ Starting BaseWorker...'')\n    worker = BaseWorker(config)\n    \n    print(''ðŸ”„ Processing jobs...'')\n    await worker.process_jobs()\n    \n    print(''âœ… Worker processing complete!'')\n\nasyncio.run(run_worker())\n\")",
      "Bash(timeout:*)",
      "Bash(PYTHONPATH=/Users/aq_home/1Projects/accessa/insurance_navigator:/Users/aq_home/1Projects/accessa/insurance_navigator/backend DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres python -c \"\nimport asyncio\nimport sys\nimport os\nsys.path.insert(0, ''.'')\nsys.path.insert(0, ''backend'')\nimport asyncpg\n\nasync def check_schema():\n    pool = await asyncpg.create_pool(''postgresql://postgres:postgres@localhost:5432/postgres'')\n    try:\n        async with pool.acquire() as conn:\n            print(''=== Upload Jobs Table Schema ==='')\n            result = await conn.fetch(\"\"SELECT column_name, data_type FROM information_schema.columns WHERE table_schema = ''upload_pipeline'' AND table_name = ''upload_jobs'' ORDER BY ordinal_position;\"\")\n            for row in result:\n                print(f''{row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n            print(''\\n=== Current Job States and Stages ==='')\n            jobs = await conn.fetch(''SELECT DISTINCT stage, state FROM upload_pipeline.upload_jobs ORDER BY stage, state;'')\n            for job in jobs:\n                print(f''Stage: {job[\"\"stage\"\"]}, State: {job[\"\"state\"\"]}'')\n                \n            print(''\\n=== Sample Jobs ==='')\n            sample_jobs = await conn.fetch(''SELECT job_id, stage, state, created_at FROM upload_pipeline.upload_jobs ORDER BY created_at DESC LIMIT 5;'')\n            for job in sample_jobs:\n                print(f''Job: {job[\"\"job_id\"\"]} | Stage: {job[\"\"stage\"\"]} | State: {job[\"\"state\"\"]} | Created: {job[\"\"created_at\"\"]}'')\n    finally:\n        await pool.close()\n\nasyncio.run(check_schema())\n\")",
      "Bash(PYTHONPATH=/Users/aq_home/1Projects/accessa/insurance_navigator:/Users/aq_home/1Projects/accessa/insurance_navigator/backend python -c \"\nimport asyncio\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment\nload_dotenv(''.env.development'')\nsys.path.insert(0, ''.'')\nsys.path.insert(0, ''backend'')\n\nfrom config.database import get_async_database_url\nimport asyncpg\n\nasync def check_schema():\n    db_url = get_async_database_url()\n    print(f''Database URL: {db_url}'')\n    \n    pool = await asyncpg.create_pool(db_url)\n    try:\n        async with pool.acquire() as conn:\n            print(''=== Upload Jobs Table Schema ==='')\n            result = await conn.fetch(\"\"SELECT column_name, data_type FROM information_schema.columns WHERE table_schema = ''upload_pipeline'' AND table_name = ''upload_jobs'' ORDER BY ordinal_position;\"\")\n            for row in result:\n                print(f''{row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n            print(''\\n=== Current Job States and Stages ==='')\n            jobs = await conn.fetch(''SELECT DISTINCT stage, state FROM upload_pipeline.upload_jobs ORDER BY stage, state;'')\n            for job in jobs:\n                print(f''Stage: {job[\"\"stage\"\"]}, State: {job[\"\"state\"\"]}'')\n                \n            print(''\\n=== Sample Jobs with Document Info ==='')\n            sample_jobs = await conn.fetch(''''''\n                SELECT uj.job_id, uj.stage, uj.state, d.filename, uj.created_at \n                FROM upload_pipeline.upload_jobs uj\n                JOIN upload_pipeline.documents d ON uj.document_id = d.document_id\n                ORDER BY uj.created_at DESC LIMIT 10;\n            '''''')\n            for job in sample_jobs:\n                print(f''Job: {job[\"\"job_id\"\"]} | {job[\"\"filename\"\"]} | Stage: {job[\"\"stage\"\"]} | State: {job[\"\"state\"\"]} | Created: {job[\"\"created_at\"\"]}'')\n    finally:\n        await pool.close()\n\nasyncio.run(check_schema())\n\")",
      "Bash(PYTHONPATH=/Users/aq_home/1Projects/accessa/insurance_navigator:/Users/aq_home/1Projects/accessa/insurance_navigator/backend python -c \"\nimport asyncio\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment\nload_dotenv(''.env.development'')\nsys.path.insert(0, ''.'')\nsys.path.insert(0, ''backend'')\nimport asyncpg\n\nasync def check_schema():\n    # Use direct connection string\n    db_url = ''postgresql://postgres:postgres@127.0.0.1:54322/postgres''\n    print(f''Database URL: {db_url}'')\n    \n    pool = await asyncpg.create_pool(db_url)\n    try:\n        async with pool.acquire() as conn:\n            print(''=== Upload Jobs Table Schema ==='')\n            result = await conn.fetch(\"\"SELECT column_name, data_type FROM information_schema.columns WHERE table_schema = ''upload_pipeline'' AND table_name = ''upload_jobs'' ORDER BY ordinal_position;\"\")\n            for row in result:\n                print(f''{row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n            print(''\\n=== Current Job States and Stages ==='')\n            jobs = await conn.fetch(''SELECT DISTINCT stage, state FROM upload_pipeline.upload_jobs ORDER BY stage, state;'')\n            for job in jobs:\n                print(f''Stage: {job[\"\"stage\"\"]}, State: {job[\"\"state\"\"]}'')\n                \n            print(''\\n=== Sample Jobs with Document Info ==='')\n            sample_jobs = await conn.fetch(''''''\n                SELECT uj.job_id, uj.stage, uj.state, d.filename, uj.created_at \n                FROM upload_pipeline.upload_jobs uj\n                JOIN upload_pipeline.documents d ON uj.document_id = d.document_id\n                ORDER BY uj.created_at DESC LIMIT 5;\n            '''''')\n            for job in sample_jobs:\n                print(f''Job: {job[\"\"job_id\"\"]} | {job[\"\"filename\"\"]} | Stage: {job[\"\"stage\"\"]} | State: {job[\"\"state\"\"]}'')\n    finally:\n        await pool.close()\n\nasyncio.run(check_schema())\n\")",
      "Bash(PYTHONPATH:*)",
      "Bash(git checkout:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(./scripts/run-e2e-integration-test.sh:*)"
    ],
    "deny": []
  }
}