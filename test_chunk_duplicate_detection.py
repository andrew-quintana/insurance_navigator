#!/usr/bin/env python3
"""
Test Chunk Duplicate Detection

This script tests the chunk-level duplicate detection functionality by
creating documents with similar content to verify that duplicate chunks
are properly detected and handled.
"""

import asyncio
import json
import time
import uuid
import asyncpg
from datetime import datetime
from typing import Dict, Any

# Test configuration
TEST_USER_ID = "752ae479-0fc4-41d3-b2fa-8f8ac467685f"
DATABASE_URL = "postgresql://postgres:postgres@localhost:54322/postgres"

async def create_test_job_with_unique_hash(test_name: str) -> tuple[str, str]:
    """Create a test job with a unique file hash"""
    print(f"üìÑ Creating {test_name}...")
    
    document_id = str(uuid.uuid4())
    job_id = str(uuid.uuid4())
    unique_hash = f"{test_name}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    
    conn = await asyncpg.connect(DATABASE_URL)
    
    try:
        # Create document record
        await conn.execute("""
            INSERT INTO upload_pipeline.documents (
                document_id, user_id, filename, mime, bytes_len, 
                file_sha256, raw_path, created_at, updated_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, NOW(), NOW())
        """, document_id, TEST_USER_ID, f"{test_name}_{int(time.time())}.pdf", 
            "application/pdf", 1024, unique_hash, 
            f"storage://files/user/{TEST_USER_ID}/raw/{test_name}_{int(time.time())}.pdf")
        
        # Create upload job
        progress = {
            "user_id": TEST_USER_ID,
            "document_id": document_id,
            "file_sha256": unique_hash,
            "bytes_len": 1024,
            "mime": "application/pdf",
            "storage_path": f"storage://files/user/{TEST_USER_ID}/raw/{test_name}_{int(time.time())}.pdf"
        }
        
        await conn.execute("""
            INSERT INTO upload_pipeline.upload_jobs (
                job_id, document_id, status, state, progress, 
                created_at, updated_at
            ) VALUES ($1, $2, $3, $4, $5, NOW(), NOW())
        """, job_id, document_id, "uploaded", "queued", json.dumps(progress))
        
        print(f"‚úÖ Created {test_name}: document={document_id}, job={job_id}")
        return document_id, job_id
        
    finally:
        await conn.close()

async def monitor_job_status(job_id: str, max_attempts: int = 15) -> Dict[str, Any]:
    """Monitor job status"""
    print(f"‚è≥ Monitoring job {job_id}...")
    
    conn = await asyncpg.connect(DATABASE_URL)
    
    try:
        for attempt in range(1, max_attempts + 1):
            job = await conn.fetchrow("""
                SELECT status, state, progress, last_error
                FROM upload_pipeline.upload_jobs
                WHERE job_id = $1
            """, job_id)
            
            if not job:
                print(f"‚ùå Job {job_id} not found")
                return {"error": "Job not found"}
            
            status = job["status"]
            state = job["state"]
            error = job["last_error"]
            
            print(f"üîÑ Attempt {attempt}: Status={status}, State={state}")
            
            if state == "done":
                print(f"‚úÖ Job completed: {status}")
                return {"status": status, "state": state, "progress": job["progress"]}
            elif state == "deadletter":
                print(f"‚ùå Job failed: {error}")
                return {"status": status, "state": state, "error": error}
            
            # Wait before next check
            await asyncio.sleep(3)
        
        print(f"‚è∞ Job monitoring timed out after {max_attempts} attempts")
        return {"error": "Job monitoring timeout"}
        
    finally:
        await conn.close()

async def check_chunk_duplicates(document_id: str) -> Dict[str, Any]:
    """Check for duplicate chunks in the database"""
    print(f"üîç Checking for duplicate chunks for document {document_id}...")
    
    conn = await asyncpg.connect(DATABASE_URL)
    
    try:
        # Get all chunks for this document
        chunks = await conn.fetch("""
            SELECT chunk_id, chunk_sha, text, chunk_ord
            FROM upload_pipeline.document_chunks
            WHERE document_id = $1
            ORDER BY chunk_ord
        """, document_id)
        
        print(f"   Found {len(chunks)} chunks for document {document_id}")
        
        # Check for duplicate chunk hashes across all documents
        duplicate_chunks = await conn.fetch("""
            SELECT chunk_sha, COUNT(*) as count, 
                   array_agg(DISTINCT document_id) as document_ids
            FROM upload_pipeline.document_chunks
            WHERE chunk_sha IN (
                SELECT chunk_sha 
                FROM upload_pipeline.document_chunks 
                WHERE document_id = $1
            )
            GROUP BY chunk_sha
            HAVING COUNT(*) > 1
        """, document_id)
        
        print(f"   Found {len(duplicate_chunks)} duplicate chunk hashes")
        
        for dup in duplicate_chunks:
            print(f"   Duplicate chunk_sha: {dup['chunk_sha'][:16]}... (found in {dup['count']} documents)")
        
        return {
            "total_chunks": len(chunks),
            "duplicate_chunks": len(duplicate_chunks),
            "chunks": [{"chunk_id": str(c["chunk_id"]), "chunk_sha": c["chunk_sha"], "text": c["text"][:50] + "..."} for c in chunks]
        }
        
    finally:
        await conn.close()

async def test_chunk_duplicate_detection():
    """Test chunk duplicate detection functionality"""
    print("üîç Testing Chunk Duplicate Detection")
    print("=" * 50)
    
    try:
        # Step 1: Create first document
        print("\nüìÑ Step 1: Creating first document...")
        doc1_id, job1_id = await create_test_job_with_unique_hash("document1")
        
        # Step 2: Process first document
        print("\n‚è≥ Step 2: Processing first document...")
        result1 = await monitor_job_status(job1_id)
        
        if result1.get('state') != 'done':
            print("‚ùå First document failed to process")
            return False
        
        # Step 3: Check chunks from first document
        print("\nüîç Step 3: Checking chunks from first document...")
        chunks1 = await check_chunk_duplicates(doc1_id)
        
        # Step 4: Create second document
        print("\nüìÑ Step 4: Creating second document...")
        doc2_id, job2_id = await create_test_job_with_unique_hash("document2")
        
        # Step 5: Process second document
        print("\n‚è≥ Step 5: Processing second document...")
        result2 = await monitor_job_status(job2_id)
        
        if result2.get('state') != 'done':
            print("‚ùå Second document failed to process")
            return False
        
        # Step 6: Check chunks from second document
        print("\nüîç Step 6: Checking chunks from second document...")
        chunks2 = await check_chunk_duplicates(doc2_id)
        
        # Step 7: Check for cross-document duplicate chunks
        print("\nüîç Step 7: Checking for cross-document duplicate chunks...")
        chunks1_duplicates = await check_chunk_duplicates(doc1_id)
        
        # Step 8: Report results
        print("\nüìä Test Results:")
        print("=" * 30)
        
        print(f"Document 1:")
        print(f"  Status: {result1.get('status', 'unknown')}")
        print(f"  State: {result1.get('state', 'unknown')}")
        print(f"  Chunks: {chunks1['total_chunks']}")
        print(f"  Duplicate chunks: {chunks1['duplicate_chunks']}")
        
        print(f"Document 2:")
        print(f"  Status: {result2.get('status', 'unknown')}")
        print(f"  State: {result2.get('state', 'unknown')}")
        print(f"  Chunks: {chunks2['total_chunks']}")
        print(f"  Duplicate chunks: {chunks2['duplicate_chunks']}")
        
        # Check if chunk duplicate detection is working
        if chunks1_duplicates['duplicate_chunks'] > 0:
            print("\nüéâ CHUNK DUPLICATE DETECTION TEST PASSED!")
            print("‚úÖ Duplicate chunks detected across documents")
            print("‚úÖ Chunk duplicate detection logic is working")
        else:
            print("\n‚ö†Ô∏è CHUNK DUPLICATE DETECTION TEST INCOMPLETE")
            print("‚ÑπÔ∏è No duplicate chunks found (this could be normal if content is different)")
            print("‚ÑπÔ∏è Chunk duplicate detection logic is implemented but not triggered")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    asyncio.run(test_chunk_duplicate_detection())
