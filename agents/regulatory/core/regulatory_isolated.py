#!/usr/bin/env python3
"""
Isolated Regulatory Research Assistant Agent

This version is designed to work around dependency conflicts by:
1. Avoiding problematic import chains
2. Using direct imports only
3. Providing real internet search and database caching
4. Including Supabase document storage
"""

import asyncio
import logging
import os
import json
import hashlib
import time
import re
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from urllib.parse import urlparse
import aiohttp
import asyncpg

# Direct imports to avoid conflicts
from duckduckgo_search import DDGS
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class IsolatedRegulatoryAgent:
    """
    Isolated regulatory agent that avoids dependency conflicts.
    Provides real functionality without going through problematic import chains.
    """
    
    def __init__(
        self,
        db_config: Optional[Dict[str, Any]] = None,
        supabase_url: Optional[str] = None,
        supabase_key: Optional[str] = None,
        anthropic_api_key: Optional[str] = None
    ):
        """Initialize the isolated regulatory agent."""
        self.db_config = db_config or self._get_db_config()
        self.supabase_url = supabase_url or os.getenv('SUPABASE_URL')
        self.supabase_key = supabase_key or os.getenv('SUPABASE_ANON_KEY')
        self.anthropic_api_key = anthropic_api_key or os.getenv('ANTHROPIC_API_KEY')
        
        # Trusted domains for regulatory search
        self.trusted_domains = {
            'cms.gov', 'medicare.gov', 'medicaid.gov', 'ssa.gov',
            'dhcs.ca.gov', 'health.ny.gov', 'mass.gov',
            'ecfr.gov', 'regulations.gov', 'federalregister.gov'
        }
        
        # Blocked URL patterns
        self.blocked_patterns = [
            'archive', 'draft', 'test', 'staging', 'dev',
            'old', 'legacy', 'deprecated', 'temp'
        ]
        
        logger.info("Isolated regulatory agent initialized")
    
    def _get_db_config(self) -> Dict[str, Any]:
        """Get database configuration from environment."""
        return {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'user': os.getenv('DB_USER', 'postgres'),
            'password': os.getenv('DB_PASSWORD', ''),
            'database': os.getenv('DB_NAME', 'insurance_navigator')
        }
    
    def is_trusted_url(self, url: str) -> bool:
        """Check if URL is from a trusted domain."""
        try:
            domain = urlparse(url).netloc.lower()
            domain = domain.replace('www.', '')
            
            for trusted in self.trusted_domains:
                if domain == trusted or domain.endswith('.' + trusted):
                    return True
            return False
        except Exception:
            return False
    
    def should_block_url(self, url: str) -> bool:
        """Check if URL should be blocked based on patterns."""
        url_lower = url.lower()
        return any(pattern in url_lower for pattern in self.blocked_patterns)
    
    def create_search_query(self, query: str, jurisdiction: str = None, program: str = None) -> str:
        """Create optimized search query with site restrictions."""
        search_terms = [query]
        
        if program:
            search_terms.append(program)
        
        if jurisdiction == 'federal':
            search_terms.append('site:cms.gov OR site:medicare.gov OR site:medicaid.gov')
        elif jurisdiction == 'CA':
            search_terms.append('site:dhcs.ca.gov')
        else:
            search_terms.append('site:gov')
        
        return ' '.join(search_terms)
    
    async def search_regulatory_documents(
        self,
        query: str,
        jurisdiction: str = None,
        program: str = None,
        max_results: int = 5
    ) -> Dict[str, Any]:
        """
        Search for regulatory documents using DuckDuckGo.
        """
        try:
            start_time = datetime.now()
            
            search_query = self.create_search_query(query, jurisdiction, program)
            logger.info(f"Searching for: {search_query}")
            
            results = []
            
            # Use DDGS in a more isolated way
            try:
                with DDGS() as ddgs:
                    search_results = ddgs.text(
                        search_query,
                        max_results=max_results * 2,
                        region='us-en'
                    )
                    
                    for result in search_results:
                        url = result.get('href', '')
                        title = result.get('title', '')
                        snippet = result.get('body', '')
                        
                        if self.is_trusted_url(url) and not self.should_block_url(url):
                            doc_type = self._estimate_document_type(title, snippet, url)
                            priority = self._calculate_priority_score(title, snippet, url, query)
                            
                            doc_result = {
                                'title': title,
                                'url': url,
                                'snippet': snippet,
                                'domain': urlparse(url).netloc,
                                'document_type': doc_type,
                                'priority_score': priority,
                                'jurisdiction': jurisdiction or 'federal',
                                'programs': [program] if program else [],
                                'search_query': query,
                                'search_timestamp': datetime.now().isoformat()
                            }
                            
                            results.append(doc_result)
                            
                            if len(results) >= max_results:
                                break
            
            except Exception as search_error:
                logger.warning(f"DuckDuckGo search failed: {search_error}")
                # Fallback to mock results for testing
                results = self._get_fallback_results(query, jurisdiction, program, max_results)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return {
                'query': query,
                'total_results': len(results),
                'results': results,
                'processing_time_seconds': processing_time,
                'search_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}")
            return {
                'query': query,
                'total_results': 0,
                'results': [],
                'error': str(e)
            }
    
    def _get_fallback_results(self, query: str, jurisdiction: str, program: str, max_results: int) -> List[Dict[str, Any]]:
        """Provide fallback results when search fails."""
        fallback_results = [
            {
                'title': f'Medicare Coverage Guidelines for {query}',
                'url': 'https://www.cms.gov/medicare/coverage/fallback',
                'snippet': f'Coverage guidance related to {query}',
                'domain': 'cms.gov',
                'document_type': 'guidance',
                'priority_score': 0.8,
                'jurisdiction': jurisdiction or 'federal',
                'programs': [program] if program else ['Medicare'],
                'search_query': query,
                'search_timestamp': datetime.now().isoformat(),
                'fallback': True
            }
        ]
        return fallback_results[:max_results]
    
    def _estimate_document_type(self, title: str, snippet: str, url: str) -> str:
        """Estimate document type based on content analysis."""
        content = f"{title} {snippet} {url}".lower()
        
        if any(word in content for word in ['regulation', 'cfr', 'code of federal']):
            return 'regulation'
        elif any(word in content for word in ['policy', 'manual', 'handbook']):
            return 'policy'
        elif any(word in content for word in ['guidance', 'guide', 'instruction']):
            return 'guidance'
        elif any(word in content for word in ['form', 'application', 'worksheet']):
            return 'form'
        elif any(word in content for word in ['notice', 'announcement', 'alert']):
            return 'notice'
        else:
            return 'document'
    
    def _calculate_priority_score(self, title: str, snippet: str, url: str, query: str) -> float:
        """Calculate priority score for search result."""
        score = 0.0
        content = f"{title} {snippet}".lower()
        query_terms = query.lower().split()
        
        for term in query_terms:
            if term in title.lower():
                score += 0.3
            if term in snippet.lower():
                score += 0.2
        
        domain = urlparse(url).netloc.lower()
        if 'cms.gov' in domain:
            score += 0.3
        elif 'medicare.gov' in domain or 'medicaid.gov' in domain:
            score += 0.25
        elif '.gov' in domain:
            score += 0.2
        
        if any(word in content for word in ['regulation', 'policy']):
            score += 0.2
        elif any(word in content for word in ['guidance', 'manual']):
            score += 0.15
        
        return min(score, 1.0)
    
    async def extract_document_content(self, url: str) -> Dict[str, Any]:
        """Extract content from a regulatory document URL."""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=30) as response:
                    if response.status == 200:
                        content_type = response.headers.get('content-type', '').lower()
                        
                        if 'text/html' in content_type:
                            html_content = await response.text()
                            return self._extract_html_content(html_content, url)
                        else:
                            return {
                                'content_type': 'non_html',
                                'url': url,
                                'content': f"Document available at: {url}",
                                'extraction_method': 'link_reference'
                            }
                    else:
                        return {
                            'error': f"HTTP {response.status}",
                            'url': url,
                            'content': f"Unable to access document at: {url}"
                        }
        except Exception as e:
            logger.error(f"Content extraction failed for {url}: {str(e)}")
            return {
                'error': str(e),
                'url': url,
                'content': f"Error accessing document at: {url}"
            }
    
    def _extract_html_content(self, html: str, url: str) -> Dict[str, Any]:
        """Extract structured content from HTML."""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            for script in soup(["script", "style"]):
                script.extract()
            
            content_selectors = [
                'main', '[role="main"]', '.content', '#content',
                '.main-content', '.page-content', 'article'
            ]
            
            main_content = None
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    main_content = element
                    break
            
            if not main_content:
                main_content = soup.find('body') or soup
            
            text_content = main_content.get_text(separator='\n', strip=True)
            sections = self._extract_sections(main_content)
            
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else ''
            
            return {
                'content_type': 'html',
                'url': url,
                'title': title_text,
                'content': text_content[:10000],
                'sections': sections,
                'extraction_method': 'html_parsing',
                'content_length': len(text_content)
            }
            
        except Exception as e:
            logger.error(f"HTML extraction failed: {str(e)}")
            return {
                'error': str(e),
                'url': url,
                'content': f"Error parsing HTML content from: {url}"
            }
    
    def _extract_sections(self, soup) -> List[Dict[str, str]]:
        """Extract structured sections from HTML."""
        sections = []
        
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):
            section_title = heading.get_text(strip=True)
            
            content_parts = []
            current_level = int(heading.name[1])
            
            for sibling in heading.find_next_siblings():
                if sibling.name and sibling.name.startswith('h'):
                    sibling_level = int(sibling.name[1])
                    if sibling_level <= current_level:
                        break
                
                if sibling.get_text(strip=True):
                    content_parts.append(sibling.get_text(strip=True))
            
            if section_title and content_parts:
                sections.append({
                    'title': section_title,
                    'content': '\n'.join(content_parts)[:1000]
                })
        
        return sections[:10]
    
    async def cache_document(self, document: Dict[str, Any]) -> str:
        """Cache document in database and Supabase storage."""
        try:
            content_hash = hashlib.md5(
                f"{document['url']}{document.get('content', '')}".encode()
            ).hexdigest()
            
            document_id = f"doc_{content_hash[:16]}"
            
            # Store in PostgreSQL
            await self._store_in_database(document_id, document)
            
            # Store in Supabase if configured
            if self.supabase_url and self.supabase_key:
                await self._store_in_supabase(document_id, document)
            
            logger.info(f"Document cached with ID: {document_id}")
            return document_id
            
        except Exception as e:
            logger.error(f"Caching failed: {str(e)}")
            return f"cache_error_{int(time.time())}"
    
    async def _store_in_database(self, document_id: str, document: Dict[str, Any]):
        """Store document in PostgreSQL database."""
        try:
            conn = await asyncpg.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                database=self.db_config['database']
            )
            
            await conn.execute("""
                INSERT INTO regulatory_documents 
                (document_id, title, url, content, document_type, jurisdiction, programs, metadata, created_at, updated_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                ON CONFLICT (document_id) 
                DO UPDATE SET 
                    title = EXCLUDED.title,
                    content = EXCLUDED.content,
                    metadata = EXCLUDED.metadata,
                    updated_at = EXCLUDED.updated_at
            """, 
                document_id,
                document.get('title', ''),
                document.get('url', ''),
                document.get('content', ''),
                document.get('document_type', 'document'),
                document.get('jurisdiction', 'federal'),
                json.dumps(document.get('programs', [])),
                json.dumps(document),
                datetime.now(),
                datetime.now()
            )
            
            await conn.close()
            logger.info(f"Document stored in database: {document_id}")
            
        except Exception as e:
            logger.error(f"Database storage failed: {str(e)}")
            # Don't raise - continue with other storage methods
    
    async def _store_in_supabase(self, document_id: str, document: Dict[str, Any]):
        """Store document in Supabase storage bucket."""
        try:
            file_content = {
                'document_id': document_id,
                'metadata': document,
                'cached_at': datetime.now().isoformat()
            }
            
            file_data = json.dumps(file_content, indent=2).encode('utf-8')
            
            headers = {
                'Authorization': f'Bearer {self.supabase_key}',
                'Content-Type': 'application/json'
            }
            
            upload_url = f"{self.supabase_url}/storage/v1/object/documents/{document_id}.json"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(upload_url, data=file_data, headers=headers) as response:
                    if response.status in [200, 201]:
                        logger.info(f"Document stored in Supabase: {document_id}")
                    else:
                        logger.warning(f"Supabase storage failed: {response.status}")
                        
        except Exception as e:
            logger.error(f"Supabase storage failed: {str(e)}")
    
    async def analyze_strategy_with_claude(
        self,
        strategy: str,
        context: Optional[Dict[str, Any]] = None,
        sources: List[Dict[str, Any]] = None
    ) -> str:
        """
        Use Claude to analyze strategy with found sources.
        """
        if not self.anthropic_api_key:
            return self._generate_simple_analysis(strategy, context, sources or [])
        
        try:
            # Prepare sources summary
            sources_text = ""
            if sources:
                sources_text = "\n".join([
                    f"- {source.get('title', 'Unknown')}: {source.get('content', source.get('snippet', ''))[:200]}..."
                    for source in sources[:5]
                ])
            
            # Create prompt for Claude
            prompt = f"""
You are a regulatory compliance expert. Analyze this strategy against current healthcare regulations.

Strategy: {strategy}

Context: {json.dumps(context) if context else 'None provided'}

Available Regulatory Sources:
{sources_text if sources_text else 'No specific sources found'}

Please provide:
1. Regulatory compliance assessment
2. Required approvals or certifications
3. Potential barriers or risks
4. Implementation recommendations
5. Timeline considerations

Format your response with clear sections and actionable guidance.
"""

            # Use Claude API (simplified)
            headers = {
                'x-api-key': self.anthropic_api_key,
                'Content-Type': 'application/json',
                'anthropic-version': '2023-06-01'
            }
            
            payload = {
                'model': 'claude-3-sonnet-20240229',
                'max_tokens': 2000,
                'messages': [
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ]
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    'https://api.anthropic.com/v1/messages',
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result['content'][0]['text']
                    else:
                        logger.warning(f"Claude API failed: {response.status}")
                        return self._generate_simple_analysis(strategy, context, sources or [])
        
        except Exception as e:
            logger.error(f"Claude analysis failed: {str(e)}")
            return self._generate_simple_analysis(strategy, context, sources or [])
    
    def _generate_simple_analysis(
        self,
        strategy: str,
        context: Optional[Dict[str, Any]],
        sources: List[Dict[str, Any]]
    ) -> str:
        """Generate simple rule-based analysis."""
        parts = []
        
        parts.append("# Regulatory Strategy Analysis")
        parts.append(f"**Strategy:** {strategy}")
        
        if context:
            parts.append(f"**Context:** {context}")
        
        parts.append(f"\n## ðŸ“‹ Information Sources ({len(sources)} found)")
        for i, source in enumerate(sources, 1):
            parts.append(f"{i}. **{source.get('title', 'Untitled')}**")
            parts.append(f"   - URL: {source.get('url', 'Unknown')}")
            parts.append(f"   - Type: {source.get('document_type', 'document')}")
            parts.append(f"   - Priority: {source.get('priority_score', 0):.2f}")
        
        parts.append(f"\n## âš–ï¸ Regulatory Assessment")
        
        strategy_lower = strategy.lower()
        
        if 'medicare' in strategy_lower:
            parts.append("âœ… **Medicare regulations apply**")
            parts.append("- Must comply with CMS Medicare coverage determination processes")
            parts.append("- Required to follow Medicare provider enrollment requirements")
        
        if 'medicaid' in strategy_lower:
            parts.append("âœ… **Medicaid regulations apply**")
            parts.append("- Subject to federal Medicaid requirements")
            parts.append("- State-specific Medicaid rules may vary")
        
        if 'coverage' in strategy_lower:
            parts.append("âš ï¸ **Coverage determination required**")
            parts.append("- Must establish medical necessity")
            parts.append("- Prior authorization may be required")
        
        if sources:
            parts.append(f"\n## ðŸ“„ Key Document Insights")
            for source in sources[:3]:
                if source.get('content'):
                    parts.append(f"- **{source.get('title', 'Document')}**: {source.get('content', '')[:200]}...")
        
        parts.append(f"\n## ðŸŽ¯ Recommendations")
        parts.append("1. Review all referenced regulatory documents thoroughly")
        parts.append("2. Consult with regulatory compliance team")
        parts.append("3. Consider jurisdiction-specific requirements")
        parts.append("4. Ensure documentation meets all cited standards")
        
        parts.append(f"\n**Analysis completed:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return '\n'.join(parts)
    
    async def analyze_strategy(
        self,
        strategy: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Full strategy analysis with search, caching, and AI analysis.
        """
        try:
            start_time = datetime.now()
            
            # Extract search terms
            search_terms = self._extract_search_terms(strategy, context)
            
            # Search for relevant documents
            all_results = []
            for term in search_terms[:3]:
                results = await self.search_regulatory_documents(
                    query=term,
                    jurisdiction=context.get('jurisdiction') if context else None,
                    program=context.get('program') if context else None,
                    max_results=3
                )
                all_results.extend(results.get('results', []))
            
            # Extract content and cache documents
            enriched_results = []
            cached_docs = []
            
            for result in all_results[:5]:
                content = await self.extract_document_content(result['url'])
                enriched_result = {**result, **content}
                enriched_results.append(enriched_result)
                
                # Cache the document
                doc_id = await self.cache_document(enriched_result)
                cached_docs.append(doc_id)
            
            # Generate analysis using Claude or simple analysis
            analysis = await self.analyze_strategy_with_claude(strategy, context, enriched_results)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return {
                'strategy': strategy,
                'context': context or {},
                'analysis': analysis,
                'sources_found': len(enriched_results),
                'documents_cached': len(cached_docs),
                'cached_document_ids': cached_docs,
                'processing_time_seconds': processing_time,
                'timestamp': datetime.now().isoformat(),
                'agent_version': 'isolated_v1.0',
                'sources': enriched_results
            }
            
        except Exception as e:
            logger.error(f"Strategy analysis failed: {str(e)}")
            return {
                'strategy': strategy,
                'analysis': f"Analysis failed: {str(e)}",
                'error': str(e),
                'agent_version': 'isolated_v1.0'
            }
    
    def _extract_search_terms(self, strategy: str, context: Optional[Dict[str, Any]]) -> List[str]:
        """Extract key search terms from strategy and context."""
        terms = []
        
        regulatory_keywords = [
            'medicare', 'medicaid', 'coverage', 'policy', 'regulation',
            'eligibility', 'enrollment', 'benefits', 'provider',
            'reimbursement', 'compliance', 'requirements'
        ]
        
        strategy_words = re.findall(r'\b\w+\b', strategy.lower())
        key_terms = [word for word in strategy_words 
                    if word in regulatory_keywords or len(word) > 5]
        
        if context and context.get('program'):
            terms.append(f"{context['program']} {' '.join(key_terms[:3])}")
        
        terms.append(' '.join(key_terms[:4]))
        
        return terms
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Get agent capabilities."""
        return {
            'agent_type': 'isolated_regulatory',
            'version': 'isolated_v1.0',
            'features': [
                'Internet regulatory document search',
                'Trusted domain filtering',
                'Document content extraction',
                'PostgreSQL database caching',
                'Supabase document storage',
                'Claude AI strategy analysis',
                'Multi-jurisdiction support'
            ],
            'supported_programs': ['Medicare', 'Medicaid', 'General'],
            'supported_jurisdictions': ['federal', 'CA', 'NY', 'MA'],
            'trusted_domains': list(self.trusted_domains),
            'dependencies': ['duckduckgo-search', 'aiohttp', 'asyncpg', 'beautifulsoup4'],
            'ai_integration': bool(self.anthropic_api_key),
            'database_integration': bool(self.db_config.get('password')),
            'supabase_integration': bool(self.supabase_url and self.supabase_key)
        }


# Factory function
def create_isolated_regulatory_agent(**kwargs) -> IsolatedRegulatoryAgent:
    """Create an isolated regulatory agent."""
    return IsolatedRegulatoryAgent(**kwargs)


# Convenience function
async def analyze_strategy_isolated(
    strategy: str,
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Quick strategy analysis using isolated agent."""
    agent = create_isolated_regulatory_agent()
    return await agent.analyze_strategy(strategy, context) 