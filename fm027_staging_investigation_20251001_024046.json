{
  "investigation_id": "842bc243-0895-468a-b4cf-827f100caaa8",
  "investigation_name": "FM-027 Staging Investigation",
  "start_time": "2025-10-01T02:40:46.816310",
  "end_time": "2025-10-01T02:40:46.816591",
  "environment": {
    "supabase_url": "***REMOVED***",
    "investigation_type": "staging_analysis"
  },
  "analysis_results": [
    {
      "phase": "database_state_analysis",
      "start_time": "2025-10-01T02:40:46.816310",
      "findings": [
        {
          "metric": "job_status_distribution",
          "description": "Distribution of job statuses in last 24 hours",
          "query": "\n                SELECT status, COUNT(*) as count\n                FROM upload_pipeline.upload_jobs\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status\n                ORDER BY count DESC\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "failed_jobs_analysis",
          "description": "Analysis of failed jobs and their errors",
          "query": "\n                SELECT \n                    status,\n                    last_error,\n                    COUNT(*) as count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_processing_time\n                FROM upload_pipeline.upload_jobs\n                WHERE status LIKE 'failed_%'\n                AND created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status, last_error\n                ORDER BY count DESC\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "timing_analysis",
          "description": "Analysis of job processing timing",
          "query": "\n                SELECT \n                    status,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_duration,\n                    MIN(EXTRACT(EPOCH FROM (updated_at - created_at))) as min_duration,\n                    MAX(EXTRACT(EPOCH FROM (updated_at - created_at))) as max_duration,\n                    COUNT(*) as count\n                FROM upload_pipeline.upload_jobs\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status\n                ORDER BY avg_duration\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        }
      ],
      "end_time": "2025-10-01T02:40:46.816460"
    },
    {
      "phase": "job_status_pattern_analysis",
      "start_time": "2025-10-01T02:40:46.816479",
      "findings": [
        {
          "metric": "status_transitions",
          "description": "Analysis of job status transitions and timing",
          "query": "\n                WITH status_transitions AS (\n                    SELECT \n                        job_id,\n                        status,\n                        LAG(status) OVER (PARTITION BY job_id ORDER BY updated_at) as previous_status,\n                        updated_at,\n                        LAG(updated_at) OVER (PARTITION BY job_id ORDER BY updated_at) as previous_updated_at\n                    FROM upload_pipeline.upload_jobs\n                    WHERE created_at >= NOW() - INTERVAL '24 hours'\n                )\n                SELECT \n                    previous_status,\n                    status as current_status,\n                    COUNT(*) as transition_count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - previous_updated_at))) as avg_transition_time\n                FROM status_transitions\n                WHERE previous_status IS NOT NULL\n                GROUP BY previous_status, status\n                ORDER BY transition_count DESC\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "stuck_jobs",
          "description": "Analysis of jobs that appear to be stuck",
          "query": "\n                SELECT \n                    job_id,\n                    status,\n                    created_at,\n                    updated_at,\n                    EXTRACT(EPOCH FROM (NOW() - updated_at)) as seconds_since_update\n                FROM upload_pipeline.upload_jobs\n                WHERE status IN ('uploaded', 'parse_queued', 'parsed', 'chunking', 'embedding_queued')\n                AND updated_at < NOW() - INTERVAL '1 hour'\n                ORDER BY seconds_since_update DESC\n                LIMIT 20\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "race_condition_evidence",
          "description": "Analysis of jobs that failed due to file access issues",
          "query": "\n                SELECT \n                    job_id,\n                    status,\n                    created_at,\n                    updated_at,\n                    EXTRACT(EPOCH FROM (updated_at - created_at)) as processing_time\n                FROM upload_pipeline.upload_jobs\n                WHERE status = 'failed_parse'\n                AND last_error LIKE '%not accessible%'\n                AND created_at >= NOW() - INTERVAL '24 hours'\n                ORDER BY created_at DESC\n                LIMIT 20\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        }
      ],
      "end_time": "2025-10-01T02:40:46.816497"
    },
    {
      "phase": "file_access_pattern_analysis",
      "start_time": "2025-10-01T02:40:46.816513",
      "findings": [
        {
          "metric": "file_upload_timing",
          "description": "Analysis of file upload to processing timing",
          "query": "\n                SELECT \n                    d.document_id,\n                    d.raw_path,\n                    d.created_at as document_created,\n                    uj.created_at as job_created,\n                    uj.updated_at as job_updated,\n                    EXTRACT(EPOCH FROM (uj.updated_at - d.created_at)) as upload_to_processing_time\n                FROM upload_pipeline.documents d\n                JOIN upload_pipeline.upload_jobs uj ON d.document_id = uj.document_id\n                WHERE d.created_at >= NOW() - INTERVAL '24 hours'\n                AND uj.status = 'uploaded'\n                ORDER BY upload_to_processing_time\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "file_access_failures",
          "description": "Analysis of file access failures",
          "query": "\n                SELECT \n                    uj.job_id,\n                    d.raw_path,\n                    uj.status,\n                    uj.last_error,\n                    uj.created_at,\n                    uj.updated_at,\n                    EXTRACT(EPOCH FROM (uj.updated_at - uj.created_at)) as failure_time\n                FROM upload_pipeline.upload_jobs uj\n                JOIN upload_pipeline.documents d ON uj.document_id = d.document_id\n                WHERE uj.status = 'failed_parse'\n                AND (uj.last_error LIKE '%not accessible%' OR uj.last_error LIKE '%storage%')\n                AND uj.created_at >= NOW() - INTERVAL '24 hours'\n                ORDER BY uj.created_at DESC\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "storage_path_patterns",
          "description": "Analysis of storage path patterns and file sizes",
          "query": "\n                SELECT \n                    raw_path,\n                    COUNT(*) as count,\n                    AVG(bytes_len) as avg_file_size,\n                    MIN(created_at) as first_upload,\n                    MAX(created_at) as last_upload\n                FROM upload_pipeline.documents\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY raw_path\n                ORDER BY count DESC\n                LIMIT 20\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        }
      ],
      "end_time": "2025-10-01T02:40:46.816531"
    },
    {
      "phase": "race_condition_analysis",
      "start_time": "2025-10-01T02:40:46.816547",
      "findings": [
        {
          "metric": "concurrent_job_processing",
          "description": "Analysis of concurrent job processing and failures",
          "query": "\n                WITH job_timing AS (\n                    SELECT \n                        job_id,\n                        document_id,\n                        status,\n                        created_at,\n                        updated_at,\n                        EXTRACT(EPOCH FROM (updated_at - created_at)) as processing_time\n                    FROM upload_pipeline.upload_jobs\n                    WHERE created_at >= NOW() - INTERVAL '24 hours'\n                )\n                SELECT \n                    DATE_TRUNC('minute', created_at) as minute_bucket,\n                    COUNT(*) as jobs_created,\n                    COUNT(CASE WHEN status = 'failed_parse' THEN 1 END) as failed_jobs,\n                    AVG(processing_time) as avg_processing_time\n                FROM job_timing\n                GROUP BY minute_bucket\n                HAVING COUNT(*) > 1\n                ORDER BY minute_bucket DESC\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "timing_windows",
          "description": "Analysis of job processing timing windows",
          "query": "\n                SELECT \n                    status,\n                    COUNT(*) as count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_duration,\n                    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (updated_at - created_at))) as median_duration,\n                    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (updated_at - created_at))) as p95_duration\n                FROM upload_pipeline.upload_jobs\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status\n                ORDER BY avg_duration\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        },
        {
          "metric": "error_patterns",
          "description": "Analysis of error patterns and failure timing",
          "query": "\n                SELECT \n                    last_error,\n                    COUNT(*) as error_count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_failure_time\n                FROM upload_pipeline.upload_jobs\n                WHERE status = 'failed_parse'\n                AND last_error IS NOT NULL\n                AND created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY last_error\n                ORDER BY error_count DESC\n                LIMIT 10\n            ",
          "note": "Would be executed via mcp_supabase_staging_execute_sql"
        }
      ],
      "end_time": "2025-10-01T02:40:46.816563"
    }
  ],
  "summary": {
    "total_phases": 4,
    "completed_phases": 4,
    "failed_phases": 0,
    "key_queries": [
      {
        "phase": "database_state_analysis",
        "metric": "job_status_distribution",
        "description": "Distribution of job statuses in last 24 hours",
        "query": "\n                SELECT status, COUNT(*) as count\n                FROM upload_pipeline.upload_jobs\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status\n                ORDER BY count DESC\n            "
      },
      {
        "phase": "database_state_analysis",
        "metric": "failed_jobs_analysis",
        "description": "Analysis of failed jobs and their errors",
        "query": "\n                SELECT \n                    status,\n                    last_error,\n                    COUNT(*) as count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_processing_time\n                FROM upload_pipeline.upload_jobs\n                WHERE status LIKE 'failed_%'\n                AND created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status, last_error\n                ORDER BY count DESC\n            "
      },
      {
        "phase": "database_state_analysis",
        "metric": "timing_analysis",
        "description": "Analysis of job processing timing",
        "query": "\n                SELECT \n                    status,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_duration,\n                    MIN(EXTRACT(EPOCH FROM (updated_at - created_at))) as min_duration,\n                    MAX(EXTRACT(EPOCH FROM (updated_at - created_at))) as max_duration,\n                    COUNT(*) as count\n                FROM upload_pipeline.upload_jobs\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status\n                ORDER BY avg_duration\n            "
      },
      {
        "phase": "job_status_pattern_analysis",
        "metric": "status_transitions",
        "description": "Analysis of job status transitions and timing",
        "query": "\n                WITH status_transitions AS (\n                    SELECT \n                        job_id,\n                        status,\n                        LAG(status) OVER (PARTITION BY job_id ORDER BY updated_at) as previous_status,\n                        updated_at,\n                        LAG(updated_at) OVER (PARTITION BY job_id ORDER BY updated_at) as previous_updated_at\n                    FROM upload_pipeline.upload_jobs\n                    WHERE created_at >= NOW() - INTERVAL '24 hours'\n                )\n                SELECT \n                    previous_status,\n                    status as current_status,\n                    COUNT(*) as transition_count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - previous_updated_at))) as avg_transition_time\n                FROM status_transitions\n                WHERE previous_status IS NOT NULL\n                GROUP BY previous_status, status\n                ORDER BY transition_count DESC\n            "
      },
      {
        "phase": "job_status_pattern_analysis",
        "metric": "stuck_jobs",
        "description": "Analysis of jobs that appear to be stuck",
        "query": "\n                SELECT \n                    job_id,\n                    status,\n                    created_at,\n                    updated_at,\n                    EXTRACT(EPOCH FROM (NOW() - updated_at)) as seconds_since_update\n                FROM upload_pipeline.upload_jobs\n                WHERE status IN ('uploaded', 'parse_queued', 'parsed', 'chunking', 'embedding_queued')\n                AND updated_at < NOW() - INTERVAL '1 hour'\n                ORDER BY seconds_since_update DESC\n                LIMIT 20\n            "
      },
      {
        "phase": "job_status_pattern_analysis",
        "metric": "race_condition_evidence",
        "description": "Analysis of jobs that failed due to file access issues",
        "query": "\n                SELECT \n                    job_id,\n                    status,\n                    created_at,\n                    updated_at,\n                    EXTRACT(EPOCH FROM (updated_at - created_at)) as processing_time\n                FROM upload_pipeline.upload_jobs\n                WHERE status = 'failed_parse'\n                AND last_error LIKE '%not accessible%'\n                AND created_at >= NOW() - INTERVAL '24 hours'\n                ORDER BY created_at DESC\n                LIMIT 20\n            "
      },
      {
        "phase": "file_access_pattern_analysis",
        "metric": "file_upload_timing",
        "description": "Analysis of file upload to processing timing",
        "query": "\n                SELECT \n                    d.document_id,\n                    d.raw_path,\n                    d.created_at as document_created,\n                    uj.created_at as job_created,\n                    uj.updated_at as job_updated,\n                    EXTRACT(EPOCH FROM (uj.updated_at - d.created_at)) as upload_to_processing_time\n                FROM upload_pipeline.documents d\n                JOIN upload_pipeline.upload_jobs uj ON d.document_id = uj.document_id\n                WHERE d.created_at >= NOW() - INTERVAL '24 hours'\n                AND uj.status = 'uploaded'\n                ORDER BY upload_to_processing_time\n            "
      },
      {
        "phase": "file_access_pattern_analysis",
        "metric": "file_access_failures",
        "description": "Analysis of file access failures",
        "query": "\n                SELECT \n                    uj.job_id,\n                    d.raw_path,\n                    uj.status,\n                    uj.last_error,\n                    uj.created_at,\n                    uj.updated_at,\n                    EXTRACT(EPOCH FROM (uj.updated_at - uj.created_at)) as failure_time\n                FROM upload_pipeline.upload_jobs uj\n                JOIN upload_pipeline.documents d ON uj.document_id = d.document_id\n                WHERE uj.status = 'failed_parse'\n                AND (uj.last_error LIKE '%not accessible%' OR uj.last_error LIKE '%storage%')\n                AND uj.created_at >= NOW() - INTERVAL '24 hours'\n                ORDER BY uj.created_at DESC\n            "
      },
      {
        "phase": "file_access_pattern_analysis",
        "metric": "storage_path_patterns",
        "description": "Analysis of storage path patterns and file sizes",
        "query": "\n                SELECT \n                    raw_path,\n                    COUNT(*) as count,\n                    AVG(bytes_len) as avg_file_size,\n                    MIN(created_at) as first_upload,\n                    MAX(created_at) as last_upload\n                FROM upload_pipeline.documents\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY raw_path\n                ORDER BY count DESC\n                LIMIT 20\n            "
      },
      {
        "phase": "race_condition_analysis",
        "metric": "concurrent_job_processing",
        "description": "Analysis of concurrent job processing and failures",
        "query": "\n                WITH job_timing AS (\n                    SELECT \n                        job_id,\n                        document_id,\n                        status,\n                        created_at,\n                        updated_at,\n                        EXTRACT(EPOCH FROM (updated_at - created_at)) as processing_time\n                    FROM upload_pipeline.upload_jobs\n                    WHERE created_at >= NOW() - INTERVAL '24 hours'\n                )\n                SELECT \n                    DATE_TRUNC('minute', created_at) as minute_bucket,\n                    COUNT(*) as jobs_created,\n                    COUNT(CASE WHEN status = 'failed_parse' THEN 1 END) as failed_jobs,\n                    AVG(processing_time) as avg_processing_time\n                FROM job_timing\n                GROUP BY minute_bucket\n                HAVING COUNT(*) > 1\n                ORDER BY minute_bucket DESC\n            "
      },
      {
        "phase": "race_condition_analysis",
        "metric": "timing_windows",
        "description": "Analysis of job processing timing windows",
        "query": "\n                SELECT \n                    status,\n                    COUNT(*) as count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_duration,\n                    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (updated_at - created_at))) as median_duration,\n                    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (updated_at - created_at))) as p95_duration\n                FROM upload_pipeline.upload_jobs\n                WHERE created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY status\n                ORDER BY avg_duration\n            "
      },
      {
        "phase": "race_condition_analysis",
        "metric": "error_patterns",
        "description": "Analysis of error patterns and failure timing",
        "query": "\n                SELECT \n                    last_error,\n                    COUNT(*) as error_count,\n                    AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_failure_time\n                FROM upload_pipeline.upload_jobs\n                WHERE status = 'failed_parse'\n                AND last_error IS NOT NULL\n                AND created_at >= NOW() - INTERVAL '24 hours'\n                GROUP BY last_error\n                ORDER BY error_count DESC\n                LIMIT 10\n            "
      }
    ],
    "investigation_focus": [
      "Job status transition timing",
      "File access failure patterns",
      "Race condition evidence",
      "Database consistency issues"
    ]
  },
  "recommendations": [
    "Execute the provided SQL queries against staging database to gather actual data",
    "Analyze job status transition timing to identify bottlenecks",
    "Investigate file access failure patterns to understand race conditions",
    "Monitor concurrent job processing to identify timing issues",
    "Implement file existence checks before processing",
    "Add retry mechanisms for failed file access",
    "Implement job status update delays to ensure file availability",
    "Add comprehensive logging for timing analysis",
    "Consider implementing circuit breaker pattern for file access failures",
    "Add monitoring and alerting for race condition detection"
  ],
  "next_steps": [
    "Run the provided SQL queries against staging database using MCP tools",
    "Analyze the query results to identify specific timing issues",
    "Create targeted tests to reproduce identified race conditions",
    "Implement and test solutions based on findings",
    "Monitor production system for similar issues",
    "Document findings and solutions for future reference"
  ]
}